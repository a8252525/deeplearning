{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f88941c8410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=256, channels=3, file=None, img_size=64, latent_dim=100, lr=0.0002, n_classes=29, n_cpu=8, n_epochs=200, sample_interval=400)\n"
     ]
    }
   ],
   "source": [
    "# use argparse set all the parameter here\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
    "parser.add_argument(\n",
    "        '-f',\n",
    "        '--file',\n",
    "        help='Path for input file. First line should contain number of lines to search in'\n",
    "    )\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=256, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=29, help=\"number of classes for dataset\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
    "x = '--img_size 100'.split()\n",
    "opt = parser.parse_args(\" --img_size 64\".split())\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./asl_alphabet_train/asl_alphabet_train/A/A1493.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1891.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1901.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1954.jpg', './asl_alphabet_train/asl_alphabet_train/A/A2243.jpg', './asl_alphabet_train/asl_alphabet_train/A/A2517.jpg', './asl_alphabet_train/asl_alphabet_train/A/A2625.jpg', './asl_alphabet_train/asl_alphabet_train/A/A2810.jpg', './asl_alphabet_train/asl_alphabet_train/A/A2897.jpg', './asl_alphabet_train/asl_alphabet_train/A/A2946.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1.jpg', './asl_alphabet_train/asl_alphabet_train/A/A10.jpg', './asl_alphabet_train/asl_alphabet_train/A/A100.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1000.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1001.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1002.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1003.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1004.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1005.jpg', './asl_alphabet_train/asl_alphabet_train/A/A1006.jpg'] 87000\n"
     ]
    }
   ],
   "source": [
    "test_prefix_path = './asl_alphabet_test/asl_alphabet_test/'\n",
    "train_prefix_path = './asl_alphabet_train/asl_alphabet_train/'\n",
    "_ = os.listdir(test_prefix_path)\n",
    "\n",
    "test_path = [test_prefix_path + x for x in _]\n",
    "#print(test_path[:3])\n",
    "\n",
    "tmp = [train_prefix_path + x for x in os.listdir(train_prefix_path)]\n",
    "from glob import glob\n",
    "train_path = []\n",
    "for i in tmp:\n",
    "    # _ = os.listdir(i)\n",
    "    # _ == ['./asl_alphabet_train/asl_alphabet_train/A/A1.jpg etc.', ... ]\n",
    "    for j in glob(i + '/*.jpg'):\n",
    "        train_path.append(j)\n",
    "print(train_path[:20], len(train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'asl_alphabet_train', 'asl_alphabet_train', 'A', 'A1493.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(train_path[0].split('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class hand_dataset(object):\n",
    "    #input the path to get the data\n",
    "    def __init__(self, data_path, transforms):\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data_path[idx]\n",
    "        img = Image.open(img_path)\n",
    "        #get img\n",
    "        #and get label from path\n",
    "        label = img_path.split('/')[-2]\n",
    "        #If we have transform, do it.\n",
    "        #trans english label to number\n",
    "        if label == 'space':\n",
    "            label = 26\n",
    "        elif label == 'del':\n",
    "            label = 27\n",
    "        elif label == 'nothing':\n",
    "            label = 28\n",
    "        else :\n",
    "            label = ord(label)-65\n",
    "\n",
    "        if transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img,label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "trans = transforms.Compose([transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "hand_train = hand_dataset(train_path, trans)\n",
    "hand_test = hand_dataset(test_path, trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7725, -0.5294, -0.4980,  ..., -0.7333, -0.6706, -0.7020],\n",
       "         [-0.6000,  0.0118,  0.0510,  ..., -0.3020, -0.1765, -0.3569],\n",
       "         [-0.5294,  0.0510, -0.0118,  ..., -0.0745, -0.1059, -0.3490],\n",
       "         ...,\n",
       "         [-0.2941,  0.6941,  0.6863,  ...,  0.3647,  0.4039,  0.0980],\n",
       "         [-0.3098,  0.6706,  0.6627,  ...,  0.3647,  0.3804,  0.0824],\n",
       "         [-0.4745,  0.2706,  0.2627,  ...,  0.0431,  0.0510, -0.1765]],\n",
       "\n",
       "        [[-0.8510, -0.6078, -0.6000,  ..., -0.8196, -0.7333, -0.7490],\n",
       "         [-0.6471, -0.2157, -0.1686,  ..., -0.4431, -0.2471, -0.3882],\n",
       "         [-0.6314, -0.1765, -0.2314,  ..., -0.2314, -0.2157, -0.4118],\n",
       "         ...,\n",
       "         [-0.2706,  0.7412,  0.7490,  ...,  0.3098,  0.3569,  0.0275],\n",
       "         [-0.2706,  0.7490,  0.7647,  ...,  0.3176,  0.3647,  0.0196],\n",
       "         [-0.4431,  0.3412,  0.3569,  ..., -0.0118,  0.0196, -0.2314]],\n",
       "\n",
       "        [[ 0.6784,  0.3412,  0.3333,  ...,  0.1843,  0.2627,  0.5137],\n",
       "         [ 0.3098, -0.3961, -0.4039,  ..., -0.6000, -0.3725,  0.0275],\n",
       "         [ 0.3020, -0.4431, -0.4824,  ..., -0.3490, -0.2784,  0.0745],\n",
       "         ...,\n",
       "         [ 0.7647,  0.7176,  0.7333,  ...,  0.2784,  0.2941,  0.5451],\n",
       "         [ 0.7647,  0.7255,  0.7333,  ...,  0.2549,  0.2627,  0.5137],\n",
       "         [ 0.8039,  0.7333,  0.7333,  ...,  0.4902,  0.5059,  0.6235]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_train.__getitem__(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hand_train.__getitem__(0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_data = torch.utils.data.DataLoader(\n",
    " hand_test, batch_size=2, shuffle=True, num_workers=0)\n",
    "train_data = torch.utils.data.DataLoader(\n",
    " hand_train, batch_size=opt.batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.latent_dim)\n",
    "\n",
    "        self.init_size = opt.img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout2d(p=0.25, inplace=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (adv_layer): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (aux_layer): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=29, bias=True)\n",
       "    (1): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/340] [D loss: 2.030270, acc: 2%] [G loss: 2.026789]\n",
      "[Epoch 0/200] [Batch 20/340] [D loss: 2.029470, acc: 4%] [G loss: 1.990261]\n",
      "[Epoch 0/200] [Batch 40/340] [D loss: 2.029001, acc: 3%] [G loss: 2.048695]\n",
      "[Epoch 0/200] [Batch 60/340] [D loss: 2.026309, acc: 5%] [G loss: 2.019625]\n",
      "[Epoch 0/200] [Batch 80/340] [D loss: 2.021957, acc: 4%] [G loss: 2.055563]\n",
      "[Epoch 0/200] [Batch 100/340] [D loss: 2.018302, acc: 9%] [G loss: 2.001978]\n",
      "[Epoch 0/200] [Batch 120/340] [D loss: 2.021887, acc: 7%] [G loss: 2.021203]\n",
      "[Epoch 0/200] [Batch 140/340] [D loss: 2.018646, acc: 9%] [G loss: 2.033036]\n",
      "[Epoch 0/200] [Batch 160/340] [D loss: 2.000196, acc: 6%] [G loss: 2.052599]\n",
      "[Epoch 0/200] [Batch 180/340] [D loss: 1.989007, acc: 8%] [G loss: 2.064776]\n",
      "[Epoch 0/200] [Batch 200/340] [D loss: 1.991482, acc: 8%] [G loss: 2.036966]\n",
      "[Epoch 0/200] [Batch 220/340] [D loss: 1.992297, acc: 8%] [G loss: 2.044227]\n",
      "[Epoch 0/200] [Batch 240/340] [D loss: 1.981819, acc: 11%] [G loss: 2.086098]\n",
      "[Epoch 0/200] [Batch 260/340] [D loss: 1.950037, acc: 14%] [G loss: 2.068004]\n",
      "[Epoch 0/200] [Batch 280/340] [D loss: 1.926135, acc: 14%] [G loss: 2.100266]\n",
      "[Epoch 0/200] [Batch 300/340] [D loss: 1.942995, acc: 15%] [G loss: 2.097507]\n",
      "[Epoch 0/200] [Batch 320/340] [D loss: 1.943550, acc: 16%] [G loss: 2.100456]\n",
      "[Epoch 1/200] [Batch 0/340] [D loss: 1.928132, acc: 16%] [G loss: 2.102757]\n",
      "[Epoch 1/200] [Batch 20/340] [D loss: 1.928514, acc: 14%] [G loss: 2.073889]\n",
      "[Epoch 1/200] [Batch 40/340] [D loss: 1.936137, acc: 13%] [G loss: 2.142581]\n",
      "[Epoch 1/200] [Batch 60/340] [D loss: 1.935202, acc: 15%] [G loss: 2.141953]\n",
      "[Epoch 1/200] [Batch 80/340] [D loss: 1.941026, acc: 14%] [G loss: 2.103742]\n",
      "[Epoch 1/200] [Batch 100/340] [D loss: 1.917644, acc: 15%] [G loss: 2.100786]\n",
      "[Epoch 1/200] [Batch 120/340] [D loss: 1.914073, acc: 17%] [G loss: 2.116521]\n",
      "[Epoch 1/200] [Batch 140/340] [D loss: 1.914627, acc: 17%] [G loss: 2.165511]\n",
      "[Epoch 1/200] [Batch 160/340] [D loss: 1.922872, acc: 18%] [G loss: 2.104549]\n",
      "[Epoch 1/200] [Batch 180/340] [D loss: 1.923231, acc: 17%] [G loss: 2.155234]\n",
      "[Epoch 1/200] [Batch 200/340] [D loss: 1.929949, acc: 16%] [G loss: 2.146541]\n",
      "[Epoch 1/200] [Batch 220/340] [D loss: 1.913554, acc: 20%] [G loss: 2.093488]\n",
      "[Epoch 1/200] [Batch 240/340] [D loss: 1.916433, acc: 19%] [G loss: 2.134284]\n",
      "[Epoch 1/200] [Batch 260/340] [D loss: 1.918613, acc: 20%] [G loss: 2.109442]\n",
      "[Epoch 1/200] [Batch 280/340] [D loss: 1.905617, acc: 18%] [G loss: 2.194763]\n",
      "[Epoch 1/200] [Batch 300/340] [D loss: 1.928646, acc: 19%] [G loss: 2.135607]\n",
      "[Epoch 1/200] [Batch 320/340] [D loss: 1.910401, acc: 17%] [G loss: 2.105995]\n",
      "[Epoch 2/200] [Batch 0/340] [D loss: 1.900681, acc: 20%] [G loss: 2.116955]\n",
      "[Epoch 2/200] [Batch 20/340] [D loss: 1.893499, acc: 23%] [G loss: 2.145937]\n",
      "[Epoch 2/200] [Batch 40/340] [D loss: 1.902062, acc: 21%] [G loss: 2.165282]\n",
      "[Epoch 2/200] [Batch 60/340] [D loss: 1.905694, acc: 18%] [G loss: 2.145635]\n",
      "[Epoch 2/200] [Batch 80/340] [D loss: 1.904481, acc: 19%] [G loss: 2.157061]\n",
      "[Epoch 2/200] [Batch 100/340] [D loss: 1.899461, acc: 20%] [G loss: 2.098308]\n",
      "[Epoch 2/200] [Batch 120/340] [D loss: 1.899332, acc: 21%] [G loss: 2.115232]\n",
      "[Epoch 2/200] [Batch 140/340] [D loss: 1.895620, acc: 23%] [G loss: 2.218552]\n",
      "[Epoch 2/200] [Batch 160/340] [D loss: 1.889133, acc: 23%] [G loss: 2.130369]\n",
      "[Epoch 2/200] [Batch 180/340] [D loss: 1.891419, acc: 24%] [G loss: 2.135313]\n",
      "[Epoch 2/200] [Batch 200/340] [D loss: 1.885883, acc: 20%] [G loss: 2.149865]\n",
      "[Epoch 2/200] [Batch 220/340] [D loss: 1.886655, acc: 23%] [G loss: 2.086109]\n",
      "[Epoch 2/200] [Batch 240/340] [D loss: 1.904382, acc: 21%] [G loss: 2.124651]\n",
      "[Epoch 2/200] [Batch 260/340] [D loss: 1.892159, acc: 24%] [G loss: 2.097170]\n",
      "[Epoch 2/200] [Batch 280/340] [D loss: 1.884176, acc: 23%] [G loss: 2.179910]\n",
      "[Epoch 2/200] [Batch 300/340] [D loss: 1.901912, acc: 22%] [G loss: 2.165234]\n",
      "[Epoch 2/200] [Batch 320/340] [D loss: 1.906995, acc: 20%] [G loss: 2.188913]\n",
      "[Epoch 3/200] [Batch 0/340] [D loss: 1.905781, acc: 23%] [G loss: 2.071661]\n",
      "[Epoch 3/200] [Batch 20/340] [D loss: 1.904715, acc: 23%] [G loss: 2.160632]\n",
      "[Epoch 3/200] [Batch 40/340] [D loss: 1.881968, acc: 21%] [G loss: 2.179853]\n",
      "[Epoch 3/200] [Batch 60/340] [D loss: 1.879217, acc: 25%] [G loss: 2.142608]\n",
      "[Epoch 3/200] [Batch 80/340] [D loss: 1.893004, acc: 22%] [G loss: 2.165702]\n",
      "[Epoch 3/200] [Batch 100/340] [D loss: 1.884956, acc: 24%] [G loss: 2.142011]\n",
      "[Epoch 3/200] [Batch 120/340] [D loss: 1.871671, acc: 22%] [G loss: 2.132073]\n",
      "[Epoch 3/200] [Batch 140/340] [D loss: 1.882463, acc: 23%] [G loss: 2.172888]\n",
      "[Epoch 3/200] [Batch 160/340] [D loss: 1.877819, acc: 25%] [G loss: 2.162483]\n",
      "[Epoch 3/200] [Batch 180/340] [D loss: 1.870937, acc: 22%] [G loss: 2.167048]\n",
      "[Epoch 3/200] [Batch 200/340] [D loss: 1.899776, acc: 21%] [G loss: 2.190236]\n",
      "[Epoch 3/200] [Batch 220/340] [D loss: 1.879831, acc: 26%] [G loss: 2.159769]\n",
      "[Epoch 3/200] [Batch 240/340] [D loss: 1.870250, acc: 25%] [G loss: 2.146325]\n",
      "[Epoch 3/200] [Batch 260/340] [D loss: 1.871302, acc: 24%] [G loss: 2.145765]\n",
      "[Epoch 3/200] [Batch 280/340] [D loss: 1.896190, acc: 23%] [G loss: 2.131833]\n",
      "[Epoch 3/200] [Batch 300/340] [D loss: 1.875602, acc: 25%] [G loss: 2.115827]\n",
      "[Epoch 3/200] [Batch 320/340] [D loss: 1.862154, acc: 25%] [G loss: 2.114273]\n",
      "[Epoch 4/200] [Batch 0/340] [D loss: 1.889282, acc: 24%] [G loss: 2.077159]\n",
      "[Epoch 4/200] [Batch 20/340] [D loss: 1.865924, acc: 26%] [G loss: 2.148825]\n",
      "[Epoch 4/200] [Batch 40/340] [D loss: 1.895439, acc: 24%] [G loss: 2.164978]\n",
      "[Epoch 4/200] [Batch 60/340] [D loss: 1.843577, acc: 28%] [G loss: 2.090434]\n",
      "[Epoch 4/200] [Batch 80/340] [D loss: 1.835746, acc: 26%] [G loss: 2.181480]\n",
      "[Epoch 4/200] [Batch 100/340] [D loss: 1.853007, acc: 25%] [G loss: 2.106256]\n",
      "[Epoch 4/200] [Batch 120/340] [D loss: 1.848960, acc: 25%] [G loss: 2.115062]\n",
      "[Epoch 4/200] [Batch 140/340] [D loss: 1.851013, acc: 29%] [G loss: 2.131095]\n",
      "[Epoch 4/200] [Batch 160/340] [D loss: 1.858009, acc: 29%] [G loss: 2.131346]\n",
      "[Epoch 4/200] [Batch 180/340] [D loss: 1.852419, acc: 27%] [G loss: 2.060989]\n",
      "[Epoch 4/200] [Batch 200/340] [D loss: 1.867987, acc: 26%] [G loss: 2.164273]\n",
      "[Epoch 4/200] [Batch 220/340] [D loss: 1.865802, acc: 25%] [G loss: 2.124924]\n",
      "[Epoch 4/200] [Batch 240/340] [D loss: 1.866814, acc: 27%] [G loss: 2.106512]\n",
      "[Epoch 4/200] [Batch 260/340] [D loss: 1.851925, acc: 26%] [G loss: 2.121560]\n",
      "[Epoch 4/200] [Batch 280/340] [D loss: 1.876788, acc: 29%] [G loss: 2.135245]\n",
      "[Epoch 4/200] [Batch 300/340] [D loss: 1.865570, acc: 25%] [G loss: 2.103747]\n",
      "[Epoch 4/200] [Batch 320/340] [D loss: 1.840937, acc: 29%] [G loss: 2.148192]\n",
      "[Epoch 5/200] [Batch 0/340] [D loss: 1.855947, acc: 28%] [G loss: 2.090345]\n",
      "[Epoch 5/200] [Batch 20/340] [D loss: 1.856552, acc: 26%] [G loss: 2.164855]\n",
      "[Epoch 5/200] [Batch 40/340] [D loss: 1.854793, acc: 29%] [G loss: 2.130367]\n",
      "[Epoch 5/200] [Batch 60/340] [D loss: 1.839085, acc: 28%] [G loss: 2.091799]\n",
      "[Epoch 5/200] [Batch 80/340] [D loss: 1.854687, acc: 28%] [G loss: 2.210630]\n",
      "[Epoch 5/200] [Batch 100/340] [D loss: 1.854486, acc: 31%] [G loss: 2.128197]\n",
      "[Epoch 5/200] [Batch 120/340] [D loss: 1.823770, acc: 29%] [G loss: 2.143414]\n",
      "[Epoch 5/200] [Batch 140/340] [D loss: 1.842158, acc: 29%] [G loss: 2.111957]\n",
      "[Epoch 5/200] [Batch 160/340] [D loss: 1.843272, acc: 29%] [G loss: 2.165683]\n",
      "[Epoch 5/200] [Batch 180/340] [D loss: 1.827847, acc: 31%] [G loss: 2.114682]\n",
      "[Epoch 5/200] [Batch 200/340] [D loss: 1.853401, acc: 29%] [G loss: 2.140208]\n",
      "[Epoch 5/200] [Batch 220/340] [D loss: 1.863478, acc: 28%] [G loss: 2.082079]\n",
      "[Epoch 5/200] [Batch 240/340] [D loss: 1.853338, acc: 30%] [G loss: 2.187418]\n",
      "[Epoch 5/200] [Batch 260/340] [D loss: 1.844943, acc: 25%] [G loss: 2.109337]\n",
      "[Epoch 5/200] [Batch 280/340] [D loss: 1.824755, acc: 33%] [G loss: 2.114747]\n",
      "[Epoch 5/200] [Batch 300/340] [D loss: 1.828488, acc: 27%] [G loss: 2.213502]\n",
      "[Epoch 5/200] [Batch 320/340] [D loss: 1.846157, acc: 28%] [G loss: 2.287395]\n",
      "[Epoch 6/200] [Batch 0/340] [D loss: 1.823485, acc: 33%] [G loss: 2.096141]\n",
      "[Epoch 6/200] [Batch 20/340] [D loss: 1.832510, acc: 28%] [G loss: 2.117038]\n",
      "[Epoch 6/200] [Batch 40/340] [D loss: 1.844086, acc: 27%] [G loss: 2.190966]\n",
      "[Epoch 6/200] [Batch 60/340] [D loss: 1.808327, acc: 30%] [G loss: 2.187748]\n",
      "[Epoch 6/200] [Batch 80/340] [D loss: 1.833065, acc: 31%] [G loss: 2.108323]\n",
      "[Epoch 6/200] [Batch 100/340] [D loss: 1.825536, acc: 27%] [G loss: 2.211396]\n",
      "[Epoch 6/200] [Batch 120/340] [D loss: 1.829468, acc: 29%] [G loss: 2.223428]\n",
      "[Epoch 6/200] [Batch 140/340] [D loss: 1.842315, acc: 28%] [G loss: 2.252934]\n",
      "[Epoch 6/200] [Batch 160/340] [D loss: 1.843525, acc: 32%] [G loss: 2.194581]\n",
      "[Epoch 6/200] [Batch 180/340] [D loss: 1.832053, acc: 31%] [G loss: 2.299337]\n",
      "[Epoch 6/200] [Batch 200/340] [D loss: 1.820530, acc: 30%] [G loss: 2.160247]\n",
      "[Epoch 6/200] [Batch 220/340] [D loss: 1.805490, acc: 34%] [G loss: 2.103762]\n",
      "[Epoch 6/200] [Batch 240/340] [D loss: 1.824506, acc: 31%] [G loss: 2.121824]\n",
      "[Epoch 6/200] [Batch 260/340] [D loss: 1.832706, acc: 27%] [G loss: 2.258890]\n",
      "[Epoch 6/200] [Batch 280/340] [D loss: 1.826087, acc: 29%] [G loss: 2.316127]\n",
      "[Epoch 6/200] [Batch 300/340] [D loss: 1.805937, acc: 33%] [G loss: 2.222042]\n",
      "[Epoch 6/200] [Batch 320/340] [D loss: 1.808187, acc: 30%] [G loss: 2.164111]\n",
      "[Epoch 7/200] [Batch 0/340] [D loss: 1.819201, acc: 29%] [G loss: 2.235087]\n",
      "[Epoch 7/200] [Batch 20/340] [D loss: 1.798049, acc: 32%] [G loss: 2.138698]\n",
      "[Epoch 7/200] [Batch 40/340] [D loss: 1.837059, acc: 30%] [G loss: 2.192061]\n",
      "[Epoch 7/200] [Batch 60/340] [D loss: 1.793778, acc: 34%] [G loss: 2.235176]\n",
      "[Epoch 7/200] [Batch 80/340] [D loss: 1.808215, acc: 34%] [G loss: 2.233838]\n",
      "[Epoch 7/200] [Batch 100/340] [D loss: 1.797584, acc: 32%] [G loss: 2.170285]\n",
      "[Epoch 7/200] [Batch 120/340] [D loss: 1.827081, acc: 31%] [G loss: 2.132437]\n",
      "[Epoch 7/200] [Batch 140/340] [D loss: 1.830481, acc: 32%] [G loss: 2.266517]\n",
      "[Epoch 7/200] [Batch 160/340] [D loss: 1.794144, acc: 33%] [G loss: 2.171986]\n",
      "[Epoch 7/200] [Batch 180/340] [D loss: 1.817120, acc: 28%] [G loss: 2.196579]\n",
      "[Epoch 7/200] [Batch 200/340] [D loss: 1.828533, acc: 31%] [G loss: 2.094117]\n",
      "[Epoch 7/200] [Batch 220/340] [D loss: 1.831816, acc: 31%] [G loss: 2.145722]\n",
      "[Epoch 7/200] [Batch 240/340] [D loss: 1.826700, acc: 35%] [G loss: 2.228742]\n",
      "[Epoch 7/200] [Batch 260/340] [D loss: 1.834914, acc: 27%] [G loss: 2.109567]\n",
      "[Epoch 7/200] [Batch 280/340] [D loss: 1.834581, acc: 30%] [G loss: 2.155863]\n",
      "[Epoch 7/200] [Batch 300/340] [D loss: 1.835688, acc: 29%] [G loss: 2.207344]\n",
      "[Epoch 7/200] [Batch 320/340] [D loss: 1.806350, acc: 32%] [G loss: 2.112207]\n",
      "[Epoch 8/200] [Batch 0/340] [D loss: 1.829923, acc: 28%] [G loss: 2.112896]\n",
      "[Epoch 8/200] [Batch 20/340] [D loss: 1.828133, acc: 32%] [G loss: 2.178268]\n",
      "[Epoch 8/200] [Batch 40/340] [D loss: 1.812250, acc: 35%] [G loss: 2.181010]\n",
      "[Epoch 8/200] [Batch 60/340] [D loss: 1.822114, acc: 31%] [G loss: 2.232619]\n",
      "[Epoch 8/200] [Batch 80/340] [D loss: 1.828016, acc: 30%] [G loss: 2.092421]\n",
      "[Epoch 8/200] [Batch 100/340] [D loss: 1.788317, acc: 37%] [G loss: 2.223839]\n",
      "[Epoch 8/200] [Batch 120/340] [D loss: 1.797634, acc: 33%] [G loss: 2.190879]\n",
      "[Epoch 8/200] [Batch 140/340] [D loss: 1.854724, acc: 28%] [G loss: 2.265757]\n",
      "[Epoch 8/200] [Batch 160/340] [D loss: 1.791162, acc: 33%] [G loss: 2.261504]\n",
      "[Epoch 8/200] [Batch 180/340] [D loss: 1.823928, acc: 34%] [G loss: 2.208255]\n",
      "[Epoch 8/200] [Batch 200/340] [D loss: 1.788392, acc: 36%] [G loss: 2.138396]\n",
      "[Epoch 8/200] [Batch 220/340] [D loss: 1.775538, acc: 36%] [G loss: 2.269376]\n",
      "[Epoch 8/200] [Batch 240/340] [D loss: 1.808061, acc: 32%] [G loss: 2.243937]\n",
      "[Epoch 8/200] [Batch 260/340] [D loss: 1.832504, acc: 31%] [G loss: 2.259261]\n",
      "[Epoch 8/200] [Batch 280/340] [D loss: 1.829874, acc: 33%] [G loss: 2.082065]\n",
      "[Epoch 8/200] [Batch 300/340] [D loss: 1.794255, acc: 35%] [G loss: 2.191719]\n",
      "[Epoch 8/200] [Batch 320/340] [D loss: 1.840792, acc: 31%] [G loss: 2.205773]\n",
      "[Epoch 9/200] [Batch 0/340] [D loss: 1.794571, acc: 33%] [G loss: 2.354862]\n",
      "[Epoch 9/200] [Batch 20/340] [D loss: 1.819853, acc: 30%] [G loss: 2.069761]\n",
      "[Epoch 9/200] [Batch 40/340] [D loss: 1.824168, acc: 37%] [G loss: 2.089639]\n",
      "[Epoch 9/200] [Batch 60/340] [D loss: 1.844213, acc: 31%] [G loss: 2.240123]\n",
      "[Epoch 9/200] [Batch 80/340] [D loss: 1.802861, acc: 28%] [G loss: 2.204871]\n",
      "[Epoch 9/200] [Batch 100/340] [D loss: 1.815663, acc: 35%] [G loss: 2.291699]\n",
      "[Epoch 9/200] [Batch 120/340] [D loss: 1.793415, acc: 34%] [G loss: 2.172420]\n",
      "[Epoch 9/200] [Batch 140/340] [D loss: 1.811281, acc: 32%] [G loss: 2.245392]\n",
      "[Epoch 9/200] [Batch 160/340] [D loss: 1.820694, acc: 32%] [G loss: 2.287550]\n",
      "[Epoch 9/200] [Batch 180/340] [D loss: 1.844338, acc: 30%] [G loss: 2.242228]\n",
      "[Epoch 9/200] [Batch 200/340] [D loss: 1.801129, acc: 33%] [G loss: 2.210606]\n",
      "[Epoch 9/200] [Batch 220/340] [D loss: 1.809526, acc: 33%] [G loss: 2.160796]\n",
      "[Epoch 9/200] [Batch 240/340] [D loss: 1.825880, acc: 31%] [G loss: 2.190969]\n",
      "[Epoch 9/200] [Batch 260/340] [D loss: 1.800794, acc: 32%] [G loss: 2.176730]\n",
      "[Epoch 9/200] [Batch 280/340] [D loss: 1.789363, acc: 36%] [G loss: 2.108789]\n",
      "[Epoch 9/200] [Batch 300/340] [D loss: 1.838420, acc: 32%] [G loss: 2.349936]\n",
      "[Epoch 9/200] [Batch 320/340] [D loss: 1.826726, acc: 32%] [G loss: 2.339479]\n",
      "[Epoch 10/200] [Batch 0/340] [D loss: 1.785637, acc: 37%] [G loss: 2.097109]\n",
      "[Epoch 10/200] [Batch 20/340] [D loss: 1.794424, acc: 36%] [G loss: 2.225051]\n",
      "[Epoch 10/200] [Batch 40/340] [D loss: 1.804365, acc: 35%] [G loss: 2.151951]\n",
      "[Epoch 10/200] [Batch 60/340] [D loss: 1.814143, acc: 34%] [G loss: 2.288268]\n",
      "[Epoch 10/200] [Batch 80/340] [D loss: 1.802289, acc: 33%] [G loss: 2.115070]\n",
      "[Epoch 10/200] [Batch 100/340] [D loss: 1.810592, acc: 34%] [G loss: 2.062736]\n",
      "[Epoch 10/200] [Batch 120/340] [D loss: 1.790652, acc: 32%] [G loss: 2.327000]\n",
      "[Epoch 10/200] [Batch 140/340] [D loss: 1.782186, acc: 33%] [G loss: 2.359750]\n",
      "[Epoch 10/200] [Batch 160/340] [D loss: 1.805280, acc: 37%] [G loss: 2.178050]\n",
      "[Epoch 10/200] [Batch 180/340] [D loss: 1.809693, acc: 33%] [G loss: 2.168062]\n",
      "[Epoch 10/200] [Batch 200/340] [D loss: 1.749599, acc: 35%] [G loss: 2.342907]\n",
      "[Epoch 10/200] [Batch 220/340] [D loss: 1.813149, acc: 32%] [G loss: 2.195355]\n",
      "[Epoch 10/200] [Batch 240/340] [D loss: 1.782112, acc: 33%] [G loss: 2.327649]\n",
      "[Epoch 10/200] [Batch 260/340] [D loss: 1.796706, acc: 33%] [G loss: 2.162744]\n",
      "[Epoch 10/200] [Batch 280/340] [D loss: 1.803032, acc: 32%] [G loss: 2.173952]\n",
      "[Epoch 10/200] [Batch 300/340] [D loss: 1.794018, acc: 34%] [G loss: 2.394784]\n",
      "[Epoch 10/200] [Batch 320/340] [D loss: 1.792402, acc: 33%] [G loss: 2.067544]\n",
      "[Epoch 11/200] [Batch 0/340] [D loss: 1.780199, acc: 33%] [G loss: 2.419415]\n",
      "[Epoch 11/200] [Batch 20/340] [D loss: 1.794722, acc: 34%] [G loss: 2.136632]\n",
      "[Epoch 11/200] [Batch 40/340] [D loss: 1.806032, acc: 35%] [G loss: 2.269514]\n",
      "[Epoch 11/200] [Batch 60/340] [D loss: 1.781962, acc: 34%] [G loss: 2.308454]\n",
      "[Epoch 11/200] [Batch 80/340] [D loss: 1.839878, acc: 29%] [G loss: 2.300128]\n",
      "[Epoch 11/200] [Batch 100/340] [D loss: 1.795019, acc: 34%] [G loss: 2.261507]\n",
      "[Epoch 11/200] [Batch 120/340] [D loss: 1.793971, acc: 31%] [G loss: 2.224798]\n",
      "[Epoch 11/200] [Batch 140/340] [D loss: 1.796595, acc: 37%] [G loss: 2.082234]\n",
      "[Epoch 11/200] [Batch 160/340] [D loss: 1.802516, acc: 32%] [G loss: 2.190602]\n",
      "[Epoch 11/200] [Batch 180/340] [D loss: 1.790747, acc: 33%] [G loss: 2.355326]\n",
      "[Epoch 11/200] [Batch 200/340] [D loss: 1.807285, acc: 31%] [G loss: 2.137929]\n",
      "[Epoch 11/200] [Batch 220/340] [D loss: 1.831471, acc: 32%] [G loss: 2.019305]\n",
      "[Epoch 11/200] [Batch 240/340] [D loss: 1.811043, acc: 33%] [G loss: 2.349941]\n",
      "[Epoch 11/200] [Batch 260/340] [D loss: 1.806430, acc: 38%] [G loss: 2.385775]\n",
      "[Epoch 11/200] [Batch 280/340] [D loss: 1.796628, acc: 35%] [G loss: 2.127715]\n",
      "[Epoch 11/200] [Batch 300/340] [D loss: 1.783864, acc: 38%] [G loss: 2.414673]\n",
      "[Epoch 11/200] [Batch 320/340] [D loss: 1.768573, acc: 35%] [G loss: 2.244388]\n",
      "[Epoch 12/200] [Batch 0/340] [D loss: 1.804856, acc: 33%] [G loss: 2.330458]\n",
      "[Epoch 12/200] [Batch 20/340] [D loss: 1.791152, acc: 35%] [G loss: 2.169889]\n",
      "[Epoch 12/200] [Batch 40/340] [D loss: 1.779815, acc: 36%] [G loss: 2.228153]\n",
      "[Epoch 12/200] [Batch 60/340] [D loss: 1.817405, acc: 33%] [G loss: 2.114678]\n",
      "[Epoch 12/200] [Batch 80/340] [D loss: 1.789391, acc: 36%] [G loss: 2.118337]\n",
      "[Epoch 12/200] [Batch 100/340] [D loss: 1.781494, acc: 33%] [G loss: 2.216592]\n",
      "[Epoch 12/200] [Batch 120/340] [D loss: 1.792394, acc: 34%] [G loss: 2.254700]\n",
      "[Epoch 12/200] [Batch 140/340] [D loss: 1.771453, acc: 33%] [G loss: 2.275424]\n",
      "[Epoch 12/200] [Batch 160/340] [D loss: 1.791963, acc: 34%] [G loss: 2.367040]\n",
      "[Epoch 12/200] [Batch 180/340] [D loss: 1.810369, acc: 34%] [G loss: 2.439001]\n",
      "[Epoch 12/200] [Batch 200/340] [D loss: 1.796792, acc: 35%] [G loss: 2.151347]\n",
      "[Epoch 12/200] [Batch 220/340] [D loss: 1.784963, acc: 38%] [G loss: 2.125238]\n",
      "[Epoch 12/200] [Batch 240/340] [D loss: 1.785894, acc: 35%] [G loss: 2.197736]\n",
      "[Epoch 12/200] [Batch 260/340] [D loss: 1.813335, acc: 36%] [G loss: 2.138283]\n",
      "[Epoch 12/200] [Batch 280/340] [D loss: 1.771845, acc: 35%] [G loss: 2.227521]\n",
      "[Epoch 12/200] [Batch 300/340] [D loss: 1.807932, acc: 35%] [G loss: 2.083172]\n",
      "[Epoch 12/200] [Batch 320/340] [D loss: 1.794540, acc: 33%] [G loss: 2.262994]\n",
      "[Epoch 13/200] [Batch 0/340] [D loss: 1.788684, acc: 32%] [G loss: 2.234579]\n",
      "[Epoch 13/200] [Batch 20/340] [D loss: 1.768067, acc: 34%] [G loss: 2.318264]\n",
      "[Epoch 13/200] [Batch 40/340] [D loss: 1.794571, acc: 36%] [G loss: 2.502831]\n",
      "[Epoch 13/200] [Batch 60/340] [D loss: 1.773983, acc: 38%] [G loss: 2.186960]\n",
      "[Epoch 13/200] [Batch 80/340] [D loss: 1.816535, acc: 34%] [G loss: 2.266804]\n",
      "[Epoch 13/200] [Batch 100/340] [D loss: 1.798142, acc: 34%] [G loss: 2.165083]\n",
      "[Epoch 13/200] [Batch 120/340] [D loss: 1.838646, acc: 36%] [G loss: 2.202798]\n",
      "[Epoch 13/200] [Batch 140/340] [D loss: 1.788507, acc: 34%] [G loss: 2.287670]\n",
      "[Epoch 13/200] [Batch 160/340] [D loss: 1.774418, acc: 37%] [G loss: 2.174993]\n",
      "[Epoch 13/200] [Batch 180/340] [D loss: 1.826510, acc: 33%] [G loss: 2.238840]\n",
      "[Epoch 13/200] [Batch 200/340] [D loss: 1.784705, acc: 37%] [G loss: 2.179626]\n",
      "[Epoch 13/200] [Batch 220/340] [D loss: 1.782495, acc: 34%] [G loss: 2.176501]\n",
      "[Epoch 13/200] [Batch 240/340] [D loss: 1.824835, acc: 36%] [G loss: 2.213297]\n",
      "[Epoch 13/200] [Batch 260/340] [D loss: 1.750789, acc: 38%] [G loss: 2.489799]\n",
      "[Epoch 13/200] [Batch 280/340] [D loss: 1.742726, acc: 37%] [G loss: 2.304496]\n",
      "[Epoch 13/200] [Batch 300/340] [D loss: 1.759638, acc: 36%] [G loss: 2.362259]\n",
      "[Epoch 13/200] [Batch 320/340] [D loss: 1.776941, acc: 38%] [G loss: 2.307384]\n",
      "[Epoch 14/200] [Batch 0/340] [D loss: 1.783138, acc: 40%] [G loss: 2.230073]\n",
      "[Epoch 14/200] [Batch 20/340] [D loss: 1.778996, acc: 37%] [G loss: 2.181373]\n",
      "[Epoch 14/200] [Batch 40/340] [D loss: 1.793638, acc: 36%] [G loss: 2.257745]\n",
      "[Epoch 14/200] [Batch 60/340] [D loss: 1.768356, acc: 35%] [G loss: 2.183659]\n",
      "[Epoch 14/200] [Batch 80/340] [D loss: 1.780810, acc: 34%] [G loss: 2.210281]\n",
      "[Epoch 14/200] [Batch 100/340] [D loss: 1.755112, acc: 37%] [G loss: 2.310053]\n",
      "[Epoch 14/200] [Batch 120/340] [D loss: 1.780902, acc: 35%] [G loss: 2.217904]\n",
      "[Epoch 14/200] [Batch 140/340] [D loss: 1.779709, acc: 37%] [G loss: 2.211105]\n",
      "[Epoch 14/200] [Batch 160/340] [D loss: 1.799251, acc: 34%] [G loss: 2.334345]\n",
      "[Epoch 14/200] [Batch 180/340] [D loss: 1.763275, acc: 40%] [G loss: 2.155766]\n",
      "[Epoch 14/200] [Batch 200/340] [D loss: 1.806182, acc: 36%] [G loss: 2.288855]\n",
      "[Epoch 14/200] [Batch 220/340] [D loss: 1.776497, acc: 39%] [G loss: 2.168649]\n",
      "[Epoch 14/200] [Batch 240/340] [D loss: 1.746950, acc: 41%] [G loss: 2.216496]\n",
      "[Epoch 14/200] [Batch 260/340] [D loss: 1.818881, acc: 38%] [G loss: 2.044131]\n",
      "[Epoch 14/200] [Batch 280/340] [D loss: 1.785932, acc: 37%] [G loss: 2.396486]\n",
      "[Epoch 14/200] [Batch 300/340] [D loss: 1.789481, acc: 32%] [G loss: 2.221539]\n",
      "[Epoch 14/200] [Batch 320/340] [D loss: 1.765258, acc: 38%] [G loss: 2.182145]\n",
      "[Epoch 15/200] [Batch 0/340] [D loss: 1.783416, acc: 36%] [G loss: 2.360043]\n",
      "[Epoch 15/200] [Batch 20/340] [D loss: 1.757866, acc: 37%] [G loss: 2.190745]\n",
      "[Epoch 15/200] [Batch 40/340] [D loss: 1.787452, acc: 37%] [G loss: 2.246237]\n",
      "[Epoch 15/200] [Batch 60/340] [D loss: 1.786093, acc: 35%] [G loss: 2.236645]\n",
      "[Epoch 15/200] [Batch 80/340] [D loss: 1.787094, acc: 35%] [G loss: 2.228172]\n",
      "[Epoch 15/200] [Batch 100/340] [D loss: 1.765900, acc: 37%] [G loss: 2.238084]\n",
      "[Epoch 15/200] [Batch 120/340] [D loss: 1.765280, acc: 40%] [G loss: 2.161632]\n",
      "[Epoch 15/200] [Batch 140/340] [D loss: 1.774810, acc: 36%] [G loss: 2.275956]\n",
      "[Epoch 15/200] [Batch 160/340] [D loss: 1.748093, acc: 37%] [G loss: 2.177089]\n",
      "[Epoch 15/200] [Batch 180/340] [D loss: 1.779836, acc: 36%] [G loss: 2.191583]\n",
      "[Epoch 15/200] [Batch 200/340] [D loss: 1.787671, acc: 35%] [G loss: 2.301926]\n",
      "[Epoch 15/200] [Batch 220/340] [D loss: 1.789898, acc: 36%] [G loss: 2.263830]\n",
      "[Epoch 15/200] [Batch 240/340] [D loss: 1.781589, acc: 39%] [G loss: 2.255880]\n",
      "[Epoch 15/200] [Batch 260/340] [D loss: 1.758670, acc: 37%] [G loss: 2.253411]\n",
      "[Epoch 15/200] [Batch 280/340] [D loss: 1.776401, acc: 39%] [G loss: 2.189730]\n",
      "[Epoch 15/200] [Batch 300/340] [D loss: 1.782164, acc: 38%] [G loss: 2.103477]\n",
      "[Epoch 15/200] [Batch 320/340] [D loss: 1.780869, acc: 37%] [G loss: 2.227619]\n",
      "[Epoch 16/200] [Batch 0/340] [D loss: 1.765464, acc: 36%] [G loss: 2.286826]\n",
      "[Epoch 16/200] [Batch 20/340] [D loss: 1.777496, acc: 34%] [G loss: 2.343884]\n",
      "[Epoch 16/200] [Batch 40/340] [D loss: 1.794154, acc: 34%] [G loss: 2.196288]\n",
      "[Epoch 16/200] [Batch 60/340] [D loss: 1.794519, acc: 37%] [G loss: 2.243727]\n",
      "[Epoch 16/200] [Batch 80/340] [D loss: 1.803546, acc: 35%] [G loss: 2.259047]\n",
      "[Epoch 16/200] [Batch 100/340] [D loss: 1.789226, acc: 35%] [G loss: 2.413974]\n",
      "[Epoch 16/200] [Batch 120/340] [D loss: 1.791407, acc: 38%] [G loss: 2.164916]\n",
      "[Epoch 16/200] [Batch 140/340] [D loss: 1.775282, acc: 38%] [G loss: 2.184460]\n",
      "[Epoch 16/200] [Batch 160/340] [D loss: 1.754495, acc: 40%] [G loss: 2.201046]\n",
      "[Epoch 16/200] [Batch 180/340] [D loss: 1.757797, acc: 37%] [G loss: 2.207529]\n",
      "[Epoch 16/200] [Batch 200/340] [D loss: 1.790868, acc: 33%] [G loss: 2.165451]\n",
      "[Epoch 16/200] [Batch 220/340] [D loss: 1.786143, acc: 35%] [G loss: 2.211900]\n",
      "[Epoch 16/200] [Batch 240/340] [D loss: 1.792966, acc: 36%] [G loss: 2.169631]\n",
      "[Epoch 16/200] [Batch 260/340] [D loss: 1.814298, acc: 35%] [G loss: 2.490519]\n",
      "[Epoch 16/200] [Batch 280/340] [D loss: 1.768519, acc: 36%] [G loss: 2.198869]\n",
      "[Epoch 16/200] [Batch 300/340] [D loss: 1.775001, acc: 41%] [G loss: 2.269347]\n",
      "[Epoch 16/200] [Batch 320/340] [D loss: 1.792675, acc: 40%] [G loss: 2.127328]\n",
      "[Epoch 17/200] [Batch 0/340] [D loss: 1.754019, acc: 37%] [G loss: 2.295400]\n",
      "[Epoch 17/200] [Batch 20/340] [D loss: 1.760874, acc: 33%] [G loss: 2.391299]\n",
      "[Epoch 17/200] [Batch 40/340] [D loss: 1.789895, acc: 36%] [G loss: 2.244092]\n",
      "[Epoch 17/200] [Batch 60/340] [D loss: 1.756192, acc: 40%] [G loss: 2.279129]\n",
      "[Epoch 17/200] [Batch 80/340] [D loss: 1.761186, acc: 39%] [G loss: 2.324901]\n",
      "[Epoch 17/200] [Batch 100/340] [D loss: 1.760465, acc: 39%] [G loss: 2.256328]\n",
      "[Epoch 17/200] [Batch 120/340] [D loss: 1.774425, acc: 35%] [G loss: 2.149840]\n",
      "[Epoch 17/200] [Batch 140/340] [D loss: 1.755271, acc: 37%] [G loss: 2.417760]\n",
      "[Epoch 17/200] [Batch 160/340] [D loss: 1.756232, acc: 38%] [G loss: 2.198228]\n",
      "[Epoch 17/200] [Batch 180/340] [D loss: 1.728858, acc: 39%] [G loss: 2.267568]\n",
      "[Epoch 17/200] [Batch 200/340] [D loss: 1.756032, acc: 39%] [G loss: 2.375046]\n",
      "[Epoch 17/200] [Batch 220/340] [D loss: 1.751792, acc: 33%] [G loss: 2.339320]\n",
      "[Epoch 17/200] [Batch 240/340] [D loss: 1.782451, acc: 37%] [G loss: 2.034437]\n",
      "[Epoch 17/200] [Batch 260/340] [D loss: 1.768176, acc: 38%] [G loss: 2.224440]\n",
      "[Epoch 17/200] [Batch 280/340] [D loss: 1.787452, acc: 37%] [G loss: 2.342827]\n",
      "[Epoch 17/200] [Batch 300/340] [D loss: 1.775447, acc: 40%] [G loss: 2.344818]\n",
      "[Epoch 17/200] [Batch 320/340] [D loss: 1.752758, acc: 39%] [G loss: 2.205493]\n",
      "[Epoch 18/200] [Batch 0/340] [D loss: 1.743898, acc: 41%] [G loss: 2.263324]\n",
      "[Epoch 18/200] [Batch 20/340] [D loss: 1.766806, acc: 35%] [G loss: 2.301106]\n",
      "[Epoch 18/200] [Batch 40/340] [D loss: 1.779185, acc: 37%] [G loss: 2.217617]\n",
      "[Epoch 18/200] [Batch 60/340] [D loss: 1.844435, acc: 37%] [G loss: 2.223548]\n",
      "[Epoch 18/200] [Batch 80/340] [D loss: 1.765958, acc: 36%] [G loss: 2.259626]\n",
      "[Epoch 18/200] [Batch 100/340] [D loss: 1.775357, acc: 36%] [G loss: 2.301039]\n",
      "[Epoch 18/200] [Batch 120/340] [D loss: 1.794766, acc: 38%] [G loss: 2.576444]\n",
      "[Epoch 18/200] [Batch 140/340] [D loss: 1.749852, acc: 40%] [G loss: 2.339578]\n",
      "[Epoch 18/200] [Batch 160/340] [D loss: 1.773139, acc: 35%] [G loss: 2.215043]\n",
      "[Epoch 18/200] [Batch 180/340] [D loss: 1.797506, acc: 39%] [G loss: 2.359585]\n",
      "[Epoch 18/200] [Batch 200/340] [D loss: 1.738757, acc: 41%] [G loss: 2.316443]\n",
      "[Epoch 18/200] [Batch 220/340] [D loss: 1.772463, acc: 38%] [G loss: 2.278450]\n",
      "[Epoch 18/200] [Batch 240/340] [D loss: 1.758539, acc: 39%] [G loss: 2.360683]\n",
      "[Epoch 18/200] [Batch 260/340] [D loss: 1.741217, acc: 40%] [G loss: 2.269344]\n",
      "[Epoch 18/200] [Batch 280/340] [D loss: 1.766772, acc: 37%] [G loss: 2.289131]\n",
      "[Epoch 18/200] [Batch 300/340] [D loss: 1.772709, acc: 38%] [G loss: 2.262775]\n",
      "[Epoch 18/200] [Batch 320/340] [D loss: 1.816337, acc: 38%] [G loss: 2.344037]\n",
      "[Epoch 19/200] [Batch 0/340] [D loss: 1.759302, acc: 40%] [G loss: 2.128491]\n",
      "[Epoch 19/200] [Batch 20/340] [D loss: 1.761228, acc: 38%] [G loss: 2.326999]\n",
      "[Epoch 19/200] [Batch 40/340] [D loss: 1.770910, acc: 36%] [G loss: 2.228720]\n",
      "[Epoch 19/200] [Batch 60/340] [D loss: 1.759292, acc: 43%] [G loss: 2.140408]\n",
      "[Epoch 19/200] [Batch 80/340] [D loss: 1.772005, acc: 36%] [G loss: 2.331866]\n",
      "[Epoch 19/200] [Batch 100/340] [D loss: 1.746361, acc: 38%] [G loss: 2.367340]\n",
      "[Epoch 19/200] [Batch 120/340] [D loss: 1.720014, acc: 41%] [G loss: 2.325813]\n",
      "[Epoch 19/200] [Batch 140/340] [D loss: 1.749884, acc: 36%] [G loss: 2.226520]\n",
      "[Epoch 19/200] [Batch 160/340] [D loss: 1.754876, acc: 36%] [G loss: 2.168255]\n",
      "[Epoch 19/200] [Batch 180/340] [D loss: 1.735666, acc: 40%] [G loss: 2.349895]\n",
      "[Epoch 19/200] [Batch 200/340] [D loss: 1.770836, acc: 42%] [G loss: 2.315314]\n",
      "[Epoch 19/200] [Batch 220/340] [D loss: 1.806497, acc: 37%] [G loss: 2.393088]\n",
      "[Epoch 19/200] [Batch 240/340] [D loss: 1.783183, acc: 40%] [G loss: 2.044610]\n",
      "[Epoch 19/200] [Batch 260/340] [D loss: 1.772899, acc: 41%] [G loss: 2.344188]\n",
      "[Epoch 19/200] [Batch 280/340] [D loss: 1.739902, acc: 38%] [G loss: 2.246336]\n",
      "[Epoch 19/200] [Batch 300/340] [D loss: 1.782674, acc: 37%] [G loss: 2.227338]\n",
      "[Epoch 19/200] [Batch 320/340] [D loss: 1.772828, acc: 36%] [G loss: 2.236081]\n",
      "[Epoch 20/200] [Batch 0/340] [D loss: 1.771903, acc: 36%] [G loss: 2.207576]\n",
      "[Epoch 20/200] [Batch 20/340] [D loss: 1.737619, acc: 44%] [G loss: 2.189390]\n",
      "[Epoch 20/200] [Batch 40/340] [D loss: 1.766570, acc: 39%] [G loss: 2.451020]\n",
      "[Epoch 20/200] [Batch 60/340] [D loss: 1.770728, acc: 38%] [G loss: 2.383622]\n",
      "[Epoch 20/200] [Batch 80/340] [D loss: 1.828006, acc: 37%] [G loss: 1.969579]\n",
      "[Epoch 20/200] [Batch 100/340] [D loss: 1.797316, acc: 38%] [G loss: 2.349023]\n",
      "[Epoch 20/200] [Batch 120/340] [D loss: 1.787100, acc: 37%] [G loss: 2.143775]\n",
      "[Epoch 20/200] [Batch 140/340] [D loss: 1.738516, acc: 37%] [G loss: 2.356930]\n",
      "[Epoch 20/200] [Batch 160/340] [D loss: 1.783941, acc: 36%] [G loss: 2.164446]\n",
      "[Epoch 20/200] [Batch 180/340] [D loss: 1.738516, acc: 41%] [G loss: 2.341207]\n",
      "[Epoch 20/200] [Batch 200/340] [D loss: 1.739796, acc: 42%] [G loss: 2.468959]\n",
      "[Epoch 20/200] [Batch 220/340] [D loss: 1.730203, acc: 42%] [G loss: 2.347051]\n",
      "[Epoch 20/200] [Batch 240/340] [D loss: 1.799538, acc: 41%] [G loss: 2.212564]\n",
      "[Epoch 20/200] [Batch 260/340] [D loss: 1.735898, acc: 39%] [G loss: 2.429480]\n",
      "[Epoch 20/200] [Batch 280/340] [D loss: 1.751137, acc: 40%] [G loss: 2.207468]\n",
      "[Epoch 20/200] [Batch 300/340] [D loss: 1.759354, acc: 39%] [G loss: 2.330776]\n",
      "[Epoch 20/200] [Batch 320/340] [D loss: 1.725582, acc: 40%] [G loss: 2.536348]\n",
      "[Epoch 21/200] [Batch 0/340] [D loss: 1.740285, acc: 43%] [G loss: 2.247888]\n",
      "[Epoch 21/200] [Batch 20/340] [D loss: 1.739169, acc: 41%] [G loss: 2.150462]\n",
      "[Epoch 21/200] [Batch 40/340] [D loss: 1.790947, acc: 38%] [G loss: 2.160757]\n",
      "[Epoch 21/200] [Batch 60/340] [D loss: 1.731532, acc: 38%] [G loss: 2.291344]\n",
      "[Epoch 21/200] [Batch 80/340] [D loss: 1.746215, acc: 39%] [G loss: 2.375227]\n",
      "[Epoch 21/200] [Batch 100/340] [D loss: 1.736704, acc: 40%] [G loss: 2.291605]\n",
      "[Epoch 21/200] [Batch 120/340] [D loss: 1.769558, acc: 43%] [G loss: 2.302133]\n",
      "[Epoch 21/200] [Batch 140/340] [D loss: 1.762210, acc: 39%] [G loss: 2.417906]\n",
      "[Epoch 21/200] [Batch 160/340] [D loss: 1.725629, acc: 41%] [G loss: 2.165571]\n",
      "[Epoch 21/200] [Batch 180/340] [D loss: 1.766784, acc: 42%] [G loss: 2.335592]\n",
      "[Epoch 21/200] [Batch 200/340] [D loss: 1.781930, acc: 41%] [G loss: 2.285915]\n",
      "[Epoch 21/200] [Batch 220/340] [D loss: 1.769395, acc: 40%] [G loss: 2.462386]\n",
      "[Epoch 21/200] [Batch 240/340] [D loss: 1.728365, acc: 46%] [G loss: 2.203367]\n",
      "[Epoch 21/200] [Batch 260/340] [D loss: 1.769362, acc: 40%] [G loss: 2.407996]\n",
      "[Epoch 21/200] [Batch 280/340] [D loss: 1.776733, acc: 38%] [G loss: 2.411159]\n",
      "[Epoch 21/200] [Batch 300/340] [D loss: 1.777255, acc: 41%] [G loss: 2.232407]\n",
      "[Epoch 21/200] [Batch 320/340] [D loss: 1.720835, acc: 44%] [G loss: 2.113866]\n",
      "[Epoch 22/200] [Batch 0/340] [D loss: 1.741908, acc: 38%] [G loss: 2.241221]\n",
      "[Epoch 22/200] [Batch 20/340] [D loss: 1.731767, acc: 40%] [G loss: 2.392077]\n",
      "[Epoch 22/200] [Batch 40/340] [D loss: 1.741723, acc: 42%] [G loss: 2.167203]\n",
      "[Epoch 22/200] [Batch 60/340] [D loss: 1.765773, acc: 39%] [G loss: 2.087487]\n",
      "[Epoch 22/200] [Batch 80/340] [D loss: 1.750586, acc: 39%] [G loss: 2.335717]\n",
      "[Epoch 22/200] [Batch 100/340] [D loss: 1.741350, acc: 42%] [G loss: 2.247708]\n",
      "[Epoch 22/200] [Batch 120/340] [D loss: 1.769823, acc: 38%] [G loss: 2.583819]\n",
      "[Epoch 22/200] [Batch 140/340] [D loss: 1.767273, acc: 38%] [G loss: 2.159080]\n",
      "[Epoch 22/200] [Batch 160/340] [D loss: 1.740144, acc: 38%] [G loss: 2.391031]\n",
      "[Epoch 22/200] [Batch 180/340] [D loss: 1.710290, acc: 42%] [G loss: 2.470056]\n",
      "[Epoch 22/200] [Batch 200/340] [D loss: 1.751672, acc: 41%] [G loss: 2.495440]\n",
      "[Epoch 22/200] [Batch 220/340] [D loss: 1.785277, acc: 40%] [G loss: 2.134837]\n",
      "[Epoch 22/200] [Batch 240/340] [D loss: 1.783309, acc: 39%] [G loss: 2.112850]\n",
      "[Epoch 22/200] [Batch 260/340] [D loss: 1.739207, acc: 42%] [G loss: 2.255607]\n",
      "[Epoch 22/200] [Batch 280/340] [D loss: 1.722760, acc: 41%] [G loss: 2.105655]\n",
      "[Epoch 22/200] [Batch 300/340] [D loss: 1.742861, acc: 39%] [G loss: 2.277916]\n",
      "[Epoch 22/200] [Batch 320/340] [D loss: 1.821563, acc: 41%] [G loss: 2.131479]\n",
      "[Epoch 23/200] [Batch 0/340] [D loss: 1.743186, acc: 42%] [G loss: 2.349063]\n",
      "[Epoch 23/200] [Batch 20/340] [D loss: 1.722307, acc: 37%] [G loss: 2.354884]\n",
      "[Epoch 23/200] [Batch 40/340] [D loss: 1.762995, acc: 39%] [G loss: 2.239961]\n",
      "[Epoch 23/200] [Batch 60/340] [D loss: 1.726645, acc: 42%] [G loss: 2.326281]\n",
      "[Epoch 23/200] [Batch 80/340] [D loss: 1.766265, acc: 42%] [G loss: 2.397919]\n",
      "[Epoch 23/200] [Batch 100/340] [D loss: 1.742469, acc: 43%] [G loss: 2.236300]\n",
      "[Epoch 23/200] [Batch 120/340] [D loss: 1.703246, acc: 42%] [G loss: 2.335621]\n",
      "[Epoch 23/200] [Batch 140/340] [D loss: 1.743663, acc: 44%] [G loss: 2.253052]\n",
      "[Epoch 23/200] [Batch 160/340] [D loss: 1.783253, acc: 38%] [G loss: 2.058952]\n",
      "[Epoch 23/200] [Batch 180/340] [D loss: 1.725236, acc: 40%] [G loss: 2.272475]\n",
      "[Epoch 23/200] [Batch 200/340] [D loss: 1.800770, acc: 38%] [G loss: 2.335075]\n",
      "[Epoch 23/200] [Batch 220/340] [D loss: 1.719280, acc: 44%] [G loss: 2.132071]\n",
      "[Epoch 23/200] [Batch 240/340] [D loss: 1.782788, acc: 41%] [G loss: 2.118457]\n",
      "[Epoch 23/200] [Batch 260/340] [D loss: 1.772808, acc: 40%] [G loss: 2.251215]\n",
      "[Epoch 23/200] [Batch 280/340] [D loss: 1.732951, acc: 41%] [G loss: 2.298113]\n",
      "[Epoch 23/200] [Batch 300/340] [D loss: 1.745857, acc: 42%] [G loss: 2.413841]\n",
      "[Epoch 23/200] [Batch 320/340] [D loss: 1.755051, acc: 40%] [G loss: 2.244310]\n",
      "[Epoch 24/200] [Batch 0/340] [D loss: 1.758211, acc: 39%] [G loss: 2.150006]\n",
      "[Epoch 24/200] [Batch 20/340] [D loss: 1.722587, acc: 41%] [G loss: 2.187866]\n",
      "[Epoch 24/200] [Batch 40/340] [D loss: 1.753526, acc: 41%] [G loss: 2.122662]\n",
      "[Epoch 24/200] [Batch 60/340] [D loss: 1.751484, acc: 43%] [G loss: 2.512728]\n",
      "[Epoch 24/200] [Batch 80/340] [D loss: 1.767255, acc: 40%] [G loss: 2.337769]\n",
      "[Epoch 24/200] [Batch 100/340] [D loss: 1.766688, acc: 39%] [G loss: 2.394556]\n",
      "[Epoch 24/200] [Batch 120/340] [D loss: 1.736009, acc: 41%] [G loss: 2.240799]\n",
      "[Epoch 24/200] [Batch 140/340] [D loss: 1.707744, acc: 44%] [G loss: 2.139978]\n",
      "[Epoch 24/200] [Batch 160/340] [D loss: 1.727868, acc: 41%] [G loss: 2.310916]\n",
      "[Epoch 24/200] [Batch 180/340] [D loss: 1.748572, acc: 42%] [G loss: 2.208404]\n",
      "[Epoch 24/200] [Batch 200/340] [D loss: 1.777950, acc: 40%] [G loss: 2.167057]\n",
      "[Epoch 24/200] [Batch 220/340] [D loss: 1.753460, acc: 44%] [G loss: 2.211374]\n",
      "[Epoch 24/200] [Batch 240/340] [D loss: 1.761473, acc: 41%] [G loss: 2.299712]\n",
      "[Epoch 24/200] [Batch 260/340] [D loss: 1.739934, acc: 41%] [G loss: 2.339417]\n",
      "[Epoch 24/200] [Batch 280/340] [D loss: 1.739686, acc: 43%] [G loss: 2.317917]\n",
      "[Epoch 24/200] [Batch 300/340] [D loss: 1.715936, acc: 43%] [G loss: 2.373419]\n",
      "[Epoch 24/200] [Batch 320/340] [D loss: 1.745888, acc: 43%] [G loss: 2.464690]\n",
      "[Epoch 25/200] [Batch 0/340] [D loss: 1.724866, acc: 41%] [G loss: 2.387479]\n",
      "[Epoch 25/200] [Batch 20/340] [D loss: 1.733715, acc: 43%] [G loss: 2.285532]\n",
      "[Epoch 25/200] [Batch 40/340] [D loss: 1.706314, acc: 42%] [G loss: 2.258097]\n",
      "[Epoch 25/200] [Batch 60/340] [D loss: 1.767216, acc: 43%] [G loss: 2.209790]\n",
      "[Epoch 25/200] [Batch 80/340] [D loss: 1.764931, acc: 39%] [G loss: 2.202825]\n",
      "[Epoch 25/200] [Batch 100/340] [D loss: 1.796202, acc: 41%] [G loss: 2.084009]\n",
      "[Epoch 25/200] [Batch 120/340] [D loss: 1.774597, acc: 45%] [G loss: 2.429372]\n",
      "[Epoch 25/200] [Batch 140/340] [D loss: 1.756438, acc: 43%] [G loss: 2.074431]\n",
      "[Epoch 25/200] [Batch 160/340] [D loss: 1.760727, acc: 38%] [G loss: 2.098022]\n",
      "[Epoch 25/200] [Batch 180/340] [D loss: 1.759545, acc: 44%] [G loss: 2.291482]\n",
      "[Epoch 25/200] [Batch 200/340] [D loss: 1.731235, acc: 40%] [G loss: 2.071786]\n",
      "[Epoch 25/200] [Batch 220/340] [D loss: 1.743902, acc: 41%] [G loss: 2.468089]\n",
      "[Epoch 25/200] [Batch 240/340] [D loss: 1.748839, acc: 40%] [G loss: 2.453440]\n",
      "[Epoch 25/200] [Batch 260/340] [D loss: 1.783044, acc: 41%] [G loss: 2.250438]\n",
      "[Epoch 25/200] [Batch 280/340] [D loss: 1.731148, acc: 41%] [G loss: 2.234592]\n",
      "[Epoch 25/200] [Batch 300/340] [D loss: 1.721613, acc: 40%] [G loss: 2.362179]\n",
      "[Epoch 25/200] [Batch 320/340] [D loss: 1.722354, acc: 40%] [G loss: 2.281085]\n",
      "[Epoch 26/200] [Batch 0/340] [D loss: 1.770422, acc: 41%] [G loss: 2.140853]\n",
      "[Epoch 26/200] [Batch 20/340] [D loss: 1.756526, acc: 46%] [G loss: 2.474953]\n",
      "[Epoch 26/200] [Batch 40/340] [D loss: 1.680084, acc: 45%] [G loss: 2.208798]\n",
      "[Epoch 26/200] [Batch 60/340] [D loss: 1.740534, acc: 41%] [G loss: 2.175152]\n",
      "[Epoch 26/200] [Batch 80/340] [D loss: 1.808435, acc: 39%] [G loss: 2.062495]\n",
      "[Epoch 26/200] [Batch 100/340] [D loss: 1.764550, acc: 40%] [G loss: 2.208757]\n",
      "[Epoch 26/200] [Batch 120/340] [D loss: 1.699178, acc: 45%] [G loss: 2.158531]\n",
      "[Epoch 26/200] [Batch 140/340] [D loss: 1.779794, acc: 40%] [G loss: 2.453764]\n",
      "[Epoch 26/200] [Batch 160/340] [D loss: 1.777690, acc: 41%] [G loss: 2.379983]\n",
      "[Epoch 26/200] [Batch 180/340] [D loss: 1.719308, acc: 45%] [G loss: 2.334681]\n",
      "[Epoch 26/200] [Batch 200/340] [D loss: 1.780143, acc: 41%] [G loss: 2.222156]\n",
      "[Epoch 26/200] [Batch 220/340] [D loss: 1.690250, acc: 40%] [G loss: 2.289947]\n",
      "[Epoch 26/200] [Batch 240/340] [D loss: 1.739491, acc: 42%] [G loss: 1.985152]\n",
      "[Epoch 26/200] [Batch 260/340] [D loss: 1.763925, acc: 40%] [G loss: 2.098426]\n",
      "[Epoch 26/200] [Batch 280/340] [D loss: 1.723590, acc: 42%] [G loss: 2.358422]\n",
      "[Epoch 26/200] [Batch 300/340] [D loss: 1.739058, acc: 44%] [G loss: 2.219777]\n",
      "[Epoch 26/200] [Batch 320/340] [D loss: 1.763784, acc: 41%] [G loss: 2.545083]\n",
      "[Epoch 27/200] [Batch 0/340] [D loss: 1.757538, acc: 43%] [G loss: 2.294314]\n",
      "[Epoch 27/200] [Batch 20/340] [D loss: 1.749571, acc: 39%] [G loss: 2.428465]\n",
      "[Epoch 27/200] [Batch 40/340] [D loss: 1.734191, acc: 42%] [G loss: 2.169216]\n",
      "[Epoch 27/200] [Batch 60/340] [D loss: 1.747578, acc: 42%] [G loss: 2.195530]\n",
      "[Epoch 27/200] [Batch 80/340] [D loss: 1.728052, acc: 44%] [G loss: 2.353803]\n",
      "[Epoch 27/200] [Batch 100/340] [D loss: 1.778822, acc: 41%] [G loss: 2.338012]\n",
      "[Epoch 27/200] [Batch 120/340] [D loss: 1.734950, acc: 48%] [G loss: 2.039848]\n",
      "[Epoch 27/200] [Batch 140/340] [D loss: 1.736322, acc: 41%] [G loss: 2.122751]\n",
      "[Epoch 27/200] [Batch 160/340] [D loss: 1.743674, acc: 42%] [G loss: 2.512029]\n",
      "[Epoch 27/200] [Batch 180/340] [D loss: 1.740497, acc: 43%] [G loss: 2.337404]\n",
      "[Epoch 27/200] [Batch 200/340] [D loss: 1.726336, acc: 43%] [G loss: 2.328835]\n",
      "[Epoch 27/200] [Batch 220/340] [D loss: 1.753669, acc: 42%] [G loss: 2.299859]\n",
      "[Epoch 27/200] [Batch 240/340] [D loss: 1.732329, acc: 42%] [G loss: 2.514281]\n",
      "[Epoch 27/200] [Batch 260/340] [D loss: 1.761874, acc: 41%] [G loss: 2.218002]\n",
      "[Epoch 27/200] [Batch 280/340] [D loss: 1.742319, acc: 44%] [G loss: 2.365855]\n",
      "[Epoch 27/200] [Batch 300/340] [D loss: 1.715798, acc: 45%] [G loss: 2.358532]\n",
      "[Epoch 27/200] [Batch 320/340] [D loss: 1.770989, acc: 41%] [G loss: 2.324453]\n",
      "[Epoch 28/200] [Batch 0/340] [D loss: 1.747264, acc: 40%] [G loss: 2.255297]\n",
      "[Epoch 28/200] [Batch 20/340] [D loss: 1.730213, acc: 41%] [G loss: 2.434143]\n",
      "[Epoch 28/200] [Batch 40/340] [D loss: 1.771451, acc: 38%] [G loss: 2.302424]\n",
      "[Epoch 28/200] [Batch 60/340] [D loss: 1.749094, acc: 41%] [G loss: 2.407696]\n",
      "[Epoch 28/200] [Batch 80/340] [D loss: 1.785730, acc: 40%] [G loss: 2.634303]\n",
      "[Epoch 28/200] [Batch 100/340] [D loss: 1.723996, acc: 43%] [G loss: 2.304610]\n",
      "[Epoch 28/200] [Batch 120/340] [D loss: 1.702247, acc: 46%] [G loss: 2.278028]\n",
      "[Epoch 28/200] [Batch 140/340] [D loss: 1.752372, acc: 41%] [G loss: 2.181986]\n",
      "[Epoch 28/200] [Batch 160/340] [D loss: 1.739571, acc: 44%] [G loss: 2.240610]\n",
      "[Epoch 28/200] [Batch 180/340] [D loss: 1.757541, acc: 43%] [G loss: 2.232242]\n",
      "[Epoch 28/200] [Batch 200/340] [D loss: 1.727670, acc: 45%] [G loss: 2.481308]\n",
      "[Epoch 28/200] [Batch 220/340] [D loss: 1.742300, acc: 42%] [G loss: 2.343012]\n",
      "[Epoch 28/200] [Batch 240/340] [D loss: 1.756833, acc: 39%] [G loss: 2.400187]\n",
      "[Epoch 28/200] [Batch 260/340] [D loss: 1.737339, acc: 42%] [G loss: 2.151955]\n",
      "[Epoch 28/200] [Batch 280/340] [D loss: 1.740343, acc: 44%] [G loss: 2.215405]\n",
      "[Epoch 28/200] [Batch 300/340] [D loss: 1.710191, acc: 43%] [G loss: 2.310029]\n",
      "[Epoch 28/200] [Batch 320/340] [D loss: 1.746624, acc: 43%] [G loss: 2.290874]\n",
      "[Epoch 29/200] [Batch 0/340] [D loss: 1.745466, acc: 44%] [G loss: 2.373550]\n",
      "[Epoch 29/200] [Batch 20/340] [D loss: 1.701699, acc: 43%] [G loss: 2.323340]\n",
      "[Epoch 29/200] [Batch 40/340] [D loss: 1.747651, acc: 47%] [G loss: 2.103179]\n",
      "[Epoch 29/200] [Batch 60/340] [D loss: 1.722664, acc: 43%] [G loss: 2.379924]\n",
      "[Epoch 29/200] [Batch 80/340] [D loss: 1.763537, acc: 43%] [G loss: 2.132881]\n",
      "[Epoch 29/200] [Batch 100/340] [D loss: 1.751157, acc: 42%] [G loss: 2.200668]\n",
      "[Epoch 29/200] [Batch 120/340] [D loss: 1.722218, acc: 41%] [G loss: 2.176972]\n",
      "[Epoch 29/200] [Batch 140/340] [D loss: 1.762799, acc: 39%] [G loss: 2.317276]\n",
      "[Epoch 29/200] [Batch 160/340] [D loss: 1.722791, acc: 46%] [G loss: 2.496381]\n",
      "[Epoch 29/200] [Batch 180/340] [D loss: 1.750585, acc: 43%] [G loss: 2.373240]\n",
      "[Epoch 29/200] [Batch 200/340] [D loss: 1.719195, acc: 39%] [G loss: 2.584890]\n",
      "[Epoch 29/200] [Batch 220/340] [D loss: 1.707999, acc: 41%] [G loss: 2.489687]\n",
      "[Epoch 29/200] [Batch 240/340] [D loss: 1.732584, acc: 41%] [G loss: 2.356108]\n",
      "[Epoch 29/200] [Batch 260/340] [D loss: 1.722380, acc: 45%] [G loss: 2.344947]\n",
      "[Epoch 29/200] [Batch 280/340] [D loss: 1.756421, acc: 44%] [G loss: 2.175196]\n",
      "[Epoch 29/200] [Batch 300/340] [D loss: 1.730740, acc: 43%] [G loss: 2.193148]\n",
      "[Epoch 29/200] [Batch 320/340] [D loss: 1.748804, acc: 41%] [G loss: 2.189044]\n",
      "[Epoch 30/200] [Batch 0/340] [D loss: 1.746429, acc: 43%] [G loss: 2.408797]\n",
      "[Epoch 30/200] [Batch 20/340] [D loss: 1.736737, acc: 45%] [G loss: 2.164284]\n",
      "[Epoch 30/200] [Batch 40/340] [D loss: 1.696122, acc: 47%] [G loss: 2.358744]\n",
      "[Epoch 30/200] [Batch 60/340] [D loss: 1.773196, acc: 39%] [G loss: 2.395808]\n",
      "[Epoch 30/200] [Batch 80/340] [D loss: 1.782639, acc: 39%] [G loss: 2.287678]\n",
      "[Epoch 30/200] [Batch 100/340] [D loss: 1.809078, acc: 43%] [G loss: 2.173796]\n",
      "[Epoch 30/200] [Batch 120/340] [D loss: 1.751740, acc: 41%] [G loss: 2.367924]\n",
      "[Epoch 30/200] [Batch 140/340] [D loss: 1.754074, acc: 42%] [G loss: 2.637968]\n",
      "[Epoch 30/200] [Batch 160/340] [D loss: 1.737994, acc: 43%] [G loss: 2.099895]\n",
      "[Epoch 30/200] [Batch 180/340] [D loss: 1.741799, acc: 43%] [G loss: 2.122648]\n",
      "[Epoch 30/200] [Batch 200/340] [D loss: 1.713206, acc: 45%] [G loss: 2.267562]\n",
      "[Epoch 30/200] [Batch 220/340] [D loss: 1.716641, acc: 45%] [G loss: 2.111612]\n",
      "[Epoch 30/200] [Batch 240/340] [D loss: 1.722192, acc: 44%] [G loss: 2.288771]\n",
      "[Epoch 30/200] [Batch 260/340] [D loss: 1.778060, acc: 43%] [G loss: 2.242898]\n",
      "[Epoch 30/200] [Batch 280/340] [D loss: 1.730652, acc: 45%] [G loss: 2.338075]\n",
      "[Epoch 30/200] [Batch 300/340] [D loss: 1.724719, acc: 47%] [G loss: 2.383255]\n",
      "[Epoch 30/200] [Batch 320/340] [D loss: 1.737737, acc: 43%] [G loss: 2.169429]\n",
      "[Epoch 31/200] [Batch 0/340] [D loss: 1.695850, acc: 45%] [G loss: 2.281824]\n",
      "[Epoch 31/200] [Batch 20/340] [D loss: 1.724768, acc: 42%] [G loss: 2.186875]\n",
      "[Epoch 31/200] [Batch 40/340] [D loss: 1.759842, acc: 41%] [G loss: 2.091215]\n",
      "[Epoch 31/200] [Batch 60/340] [D loss: 1.737374, acc: 45%] [G loss: 2.052884]\n",
      "[Epoch 31/200] [Batch 80/340] [D loss: 1.696004, acc: 46%] [G loss: 2.416807]\n",
      "[Epoch 31/200] [Batch 100/340] [D loss: 1.736699, acc: 44%] [G loss: 2.231443]\n",
      "[Epoch 31/200] [Batch 120/340] [D loss: 1.714291, acc: 45%] [G loss: 2.091785]\n",
      "[Epoch 31/200] [Batch 140/340] [D loss: 1.714278, acc: 44%] [G loss: 2.415205]\n",
      "[Epoch 31/200] [Batch 160/340] [D loss: 1.705134, acc: 49%] [G loss: 2.150096]\n",
      "[Epoch 31/200] [Batch 180/340] [D loss: 1.675412, acc: 43%] [G loss: 2.322201]\n",
      "[Epoch 31/200] [Batch 200/340] [D loss: 1.726349, acc: 46%] [G loss: 2.766311]\n",
      "[Epoch 31/200] [Batch 220/340] [D loss: 1.707351, acc: 43%] [G loss: 2.303278]\n",
      "[Epoch 31/200] [Batch 240/340] [D loss: 1.777874, acc: 41%] [G loss: 2.115756]\n",
      "[Epoch 31/200] [Batch 260/340] [D loss: 1.766104, acc: 45%] [G loss: 2.016568]\n",
      "[Epoch 31/200] [Batch 280/340] [D loss: 1.728116, acc: 46%] [G loss: 2.478141]\n",
      "[Epoch 31/200] [Batch 300/340] [D loss: 1.714640, acc: 45%] [G loss: 2.486793]\n",
      "[Epoch 31/200] [Batch 320/340] [D loss: 1.760185, acc: 41%] [G loss: 2.220934]\n",
      "[Epoch 32/200] [Batch 0/340] [D loss: 1.727278, acc: 44%] [G loss: 2.387850]\n",
      "[Epoch 32/200] [Batch 20/340] [D loss: 1.728291, acc: 44%] [G loss: 2.241790]\n",
      "[Epoch 32/200] [Batch 40/340] [D loss: 1.710831, acc: 47%] [G loss: 2.051465]\n",
      "[Epoch 32/200] [Batch 60/340] [D loss: 1.733320, acc: 46%] [G loss: 2.139637]\n",
      "[Epoch 32/200] [Batch 80/340] [D loss: 1.737849, acc: 45%] [G loss: 2.180040]\n",
      "[Epoch 32/200] [Batch 100/340] [D loss: 1.738238, acc: 46%] [G loss: 2.407402]\n",
      "[Epoch 32/200] [Batch 120/340] [D loss: 1.748973, acc: 43%] [G loss: 2.278491]\n",
      "[Epoch 32/200] [Batch 140/340] [D loss: 1.711159, acc: 42%] [G loss: 2.285005]\n",
      "[Epoch 32/200] [Batch 160/340] [D loss: 1.702636, acc: 45%] [G loss: 2.217980]\n",
      "[Epoch 32/200] [Batch 180/340] [D loss: 1.753619, acc: 42%] [G loss: 2.221034]\n",
      "[Epoch 32/200] [Batch 200/340] [D loss: 1.729524, acc: 42%] [G loss: 2.138814]\n",
      "[Epoch 32/200] [Batch 220/340] [D loss: 1.723063, acc: 42%] [G loss: 2.539836]\n",
      "[Epoch 32/200] [Batch 240/340] [D loss: 1.728261, acc: 42%] [G loss: 2.077555]\n",
      "[Epoch 32/200] [Batch 260/340] [D loss: 1.749410, acc: 40%] [G loss: 2.184684]\n",
      "[Epoch 32/200] [Batch 280/340] [D loss: 1.699518, acc: 48%] [G loss: 2.154391]\n",
      "[Epoch 32/200] [Batch 300/340] [D loss: 1.727889, acc: 45%] [G loss: 2.208249]\n",
      "[Epoch 32/200] [Batch 320/340] [D loss: 1.731336, acc: 45%] [G loss: 2.160693]\n",
      "[Epoch 33/200] [Batch 0/340] [D loss: 1.720840, acc: 44%] [G loss: 2.339973]\n",
      "[Epoch 33/200] [Batch 20/340] [D loss: 1.734454, acc: 48%] [G loss: 2.355171]\n",
      "[Epoch 33/200] [Batch 40/340] [D loss: 1.733122, acc: 44%] [G loss: 2.181346]\n",
      "[Epoch 33/200] [Batch 60/340] [D loss: 1.747639, acc: 39%] [G loss: 2.313536]\n",
      "[Epoch 33/200] [Batch 80/340] [D loss: 1.710634, acc: 44%] [G loss: 2.310532]\n",
      "[Epoch 33/200] [Batch 100/340] [D loss: 1.737408, acc: 47%] [G loss: 2.161326]\n",
      "[Epoch 33/200] [Batch 120/340] [D loss: 1.751728, acc: 42%] [G loss: 2.398304]\n",
      "[Epoch 33/200] [Batch 140/340] [D loss: 1.699911, acc: 45%] [G loss: 2.189943]\n",
      "[Epoch 33/200] [Batch 160/340] [D loss: 1.716845, acc: 41%] [G loss: 2.052139]\n",
      "[Epoch 33/200] [Batch 180/340] [D loss: 1.745083, acc: 42%] [G loss: 2.303062]\n",
      "[Epoch 33/200] [Batch 200/340] [D loss: 1.756837, acc: 40%] [G loss: 2.189928]\n",
      "[Epoch 33/200] [Batch 220/340] [D loss: 1.736524, acc: 45%] [G loss: 2.382228]\n",
      "[Epoch 33/200] [Batch 240/340] [D loss: 1.738711, acc: 45%] [G loss: 2.466809]\n",
      "[Epoch 33/200] [Batch 260/340] [D loss: 1.743742, acc: 44%] [G loss: 2.083353]\n",
      "[Epoch 33/200] [Batch 280/340] [D loss: 1.741636, acc: 49%] [G loss: 2.532990]\n",
      "[Epoch 33/200] [Batch 300/340] [D loss: 1.712540, acc: 43%] [G loss: 2.399415]\n",
      "[Epoch 33/200] [Batch 320/340] [D loss: 1.735465, acc: 44%] [G loss: 2.326001]\n",
      "[Epoch 34/200] [Batch 0/340] [D loss: 1.727874, acc: 44%] [G loss: 2.251804]\n",
      "[Epoch 34/200] [Batch 20/340] [D loss: 1.741828, acc: 41%] [G loss: 2.517521]\n",
      "[Epoch 34/200] [Batch 40/340] [D loss: 1.733730, acc: 42%] [G loss: 2.217689]\n",
      "[Epoch 34/200] [Batch 60/340] [D loss: 1.749662, acc: 46%] [G loss: 2.258454]\n",
      "[Epoch 34/200] [Batch 80/340] [D loss: 1.708602, acc: 44%] [G loss: 2.370606]\n",
      "[Epoch 34/200] [Batch 100/340] [D loss: 1.738775, acc: 41%] [G loss: 2.504428]\n",
      "[Epoch 34/200] [Batch 120/340] [D loss: 1.753506, acc: 46%] [G loss: 2.126546]\n",
      "[Epoch 34/200] [Batch 140/340] [D loss: 1.726929, acc: 41%] [G loss: 2.343443]\n",
      "[Epoch 34/200] [Batch 160/340] [D loss: 1.745982, acc: 40%] [G loss: 2.091200]\n",
      "[Epoch 34/200] [Batch 180/340] [D loss: 1.700126, acc: 44%] [G loss: 2.331401]\n",
      "[Epoch 34/200] [Batch 200/340] [D loss: 1.761474, acc: 42%] [G loss: 2.345581]\n",
      "[Epoch 34/200] [Batch 220/340] [D loss: 1.692368, acc: 47%] [G loss: 2.174783]\n",
      "[Epoch 34/200] [Batch 240/340] [D loss: 1.761214, acc: 44%] [G loss: 2.031664]\n",
      "[Epoch 34/200] [Batch 260/340] [D loss: 1.709050, acc: 47%] [G loss: 2.003569]\n",
      "[Epoch 34/200] [Batch 280/340] [D loss: 1.715117, acc: 46%] [G loss: 2.387977]\n",
      "[Epoch 34/200] [Batch 300/340] [D loss: 1.699538, acc: 45%] [G loss: 2.264906]\n",
      "[Epoch 34/200] [Batch 320/340] [D loss: 1.739755, acc: 43%] [G loss: 2.117161]\n",
      "[Epoch 35/200] [Batch 0/340] [D loss: 1.744312, acc: 47%] [G loss: 2.239224]\n",
      "[Epoch 35/200] [Batch 20/340] [D loss: 1.732881, acc: 46%] [G loss: 2.468350]\n",
      "[Epoch 35/200] [Batch 40/340] [D loss: 1.708692, acc: 44%] [G loss: 2.354423]\n",
      "[Epoch 35/200] [Batch 60/340] [D loss: 1.714953, acc: 53%] [G loss: 2.056794]\n",
      "[Epoch 35/200] [Batch 80/340] [D loss: 1.715538, acc: 50%] [G loss: 1.969792]\n",
      "[Epoch 35/200] [Batch 100/340] [D loss: 1.726069, acc: 45%] [G loss: 2.284331]\n",
      "[Epoch 35/200] [Batch 120/340] [D loss: 1.733148, acc: 46%] [G loss: 2.132014]\n",
      "[Epoch 35/200] [Batch 140/340] [D loss: 1.722921, acc: 44%] [G loss: 2.455253]\n",
      "[Epoch 35/200] [Batch 160/340] [D loss: 1.679053, acc: 48%] [G loss: 2.185239]\n",
      "[Epoch 35/200] [Batch 180/340] [D loss: 1.769882, acc: 45%] [G loss: 2.545953]\n",
      "[Epoch 35/200] [Batch 200/340] [D loss: 1.694497, acc: 46%] [G loss: 2.314154]\n",
      "[Epoch 35/200] [Batch 220/340] [D loss: 1.706543, acc: 46%] [G loss: 2.624738]\n",
      "[Epoch 35/200] [Batch 240/340] [D loss: 1.734204, acc: 42%] [G loss: 2.571933]\n",
      "[Epoch 35/200] [Batch 260/340] [D loss: 1.707303, acc: 47%] [G loss: 2.345607]\n",
      "[Epoch 35/200] [Batch 280/340] [D loss: 1.723069, acc: 45%] [G loss: 2.409187]\n",
      "[Epoch 35/200] [Batch 300/340] [D loss: 1.706712, acc: 45%] [G loss: 2.173420]\n",
      "[Epoch 35/200] [Batch 320/340] [D loss: 1.757309, acc: 44%] [G loss: 2.103578]\n",
      "[Epoch 36/200] [Batch 0/340] [D loss: 1.727899, acc: 45%] [G loss: 2.283015]\n",
      "[Epoch 36/200] [Batch 20/340] [D loss: 1.723431, acc: 46%] [G loss: 2.396260]\n",
      "[Epoch 36/200] [Batch 40/340] [D loss: 1.753166, acc: 49%] [G loss: 2.211702]\n",
      "[Epoch 36/200] [Batch 60/340] [D loss: 1.714020, acc: 47%] [G loss: 2.217702]\n",
      "[Epoch 36/200] [Batch 80/340] [D loss: 1.702255, acc: 44%] [G loss: 2.260690]\n",
      "[Epoch 36/200] [Batch 100/340] [D loss: 1.780989, acc: 41%] [G loss: 2.437714]\n",
      "[Epoch 36/200] [Batch 120/340] [D loss: 1.714828, acc: 44%] [G loss: 2.322221]\n",
      "[Epoch 36/200] [Batch 140/340] [D loss: 1.743988, acc: 42%] [G loss: 2.416047]\n",
      "[Epoch 36/200] [Batch 160/340] [D loss: 1.757326, acc: 44%] [G loss: 2.318944]\n",
      "[Epoch 36/200] [Batch 180/340] [D loss: 1.742483, acc: 39%] [G loss: 2.266679]\n",
      "[Epoch 36/200] [Batch 200/340] [D loss: 1.750057, acc: 50%] [G loss: 2.242232]\n",
      "[Epoch 36/200] [Batch 220/340] [D loss: 1.732118, acc: 44%] [G loss: 2.062611]\n",
      "[Epoch 36/200] [Batch 240/340] [D loss: 1.729921, acc: 45%] [G loss: 2.326209]\n",
      "[Epoch 36/200] [Batch 260/340] [D loss: 1.682968, acc: 42%] [G loss: 2.384879]\n",
      "[Epoch 36/200] [Batch 280/340] [D loss: 1.763899, acc: 43%] [G loss: 2.451368]\n",
      "[Epoch 36/200] [Batch 300/340] [D loss: 1.740741, acc: 46%] [G loss: 2.347475]\n",
      "[Epoch 36/200] [Batch 320/340] [D loss: 1.695869, acc: 45%] [G loss: 2.299830]\n",
      "[Epoch 37/200] [Batch 0/340] [D loss: 1.704200, acc: 44%] [G loss: 2.376807]\n",
      "[Epoch 37/200] [Batch 20/340] [D loss: 1.699118, acc: 44%] [G loss: 2.248458]\n",
      "[Epoch 37/200] [Batch 40/340] [D loss: 1.728928, acc: 46%] [G loss: 2.462908]\n",
      "[Epoch 37/200] [Batch 60/340] [D loss: 1.723684, acc: 48%] [G loss: 2.195435]\n",
      "[Epoch 37/200] [Batch 80/340] [D loss: 1.742308, acc: 47%] [G loss: 2.133103]\n",
      "[Epoch 37/200] [Batch 100/340] [D loss: 1.698795, acc: 46%] [G loss: 2.216184]\n",
      "[Epoch 37/200] [Batch 120/340] [D loss: 1.737339, acc: 42%] [G loss: 2.387843]\n",
      "[Epoch 37/200] [Batch 140/340] [D loss: 1.674922, acc: 46%] [G loss: 2.373768]\n",
      "[Epoch 37/200] [Batch 160/340] [D loss: 1.801645, acc: 45%] [G loss: 2.485671]\n",
      "[Epoch 37/200] [Batch 180/340] [D loss: 1.774201, acc: 45%] [G loss: 2.385915]\n",
      "[Epoch 37/200] [Batch 200/340] [D loss: 1.746228, acc: 44%] [G loss: 2.051548]\n",
      "[Epoch 37/200] [Batch 220/340] [D loss: 1.734540, acc: 47%] [G loss: 2.109140]\n",
      "[Epoch 37/200] [Batch 240/340] [D loss: 1.710741, acc: 46%] [G loss: 2.270377]\n",
      "[Epoch 37/200] [Batch 260/340] [D loss: 1.674957, acc: 49%] [G loss: 2.329839]\n",
      "[Epoch 37/200] [Batch 280/340] [D loss: 1.717225, acc: 46%] [G loss: 2.190485]\n",
      "[Epoch 37/200] [Batch 300/340] [D loss: 1.735834, acc: 47%] [G loss: 2.046994]\n",
      "[Epoch 37/200] [Batch 320/340] [D loss: 1.703080, acc: 44%] [G loss: 2.295653]\n",
      "[Epoch 38/200] [Batch 0/340] [D loss: 1.715079, acc: 45%] [G loss: 2.526897]\n",
      "[Epoch 38/200] [Batch 20/340] [D loss: 1.723728, acc: 45%] [G loss: 2.342968]\n",
      "[Epoch 38/200] [Batch 40/340] [D loss: 1.717134, acc: 46%] [G loss: 2.287601]\n",
      "[Epoch 38/200] [Batch 60/340] [D loss: 1.715373, acc: 46%] [G loss: 2.273940]\n",
      "[Epoch 38/200] [Batch 80/340] [D loss: 1.746869, acc: 47%] [G loss: 2.199945]\n",
      "[Epoch 38/200] [Batch 100/340] [D loss: 1.761296, acc: 47%] [G loss: 2.241971]\n",
      "[Epoch 38/200] [Batch 120/340] [D loss: 1.766129, acc: 45%] [G loss: 2.347078]\n",
      "[Epoch 38/200] [Batch 140/340] [D loss: 1.672683, acc: 46%] [G loss: 2.417401]\n",
      "[Epoch 38/200] [Batch 160/340] [D loss: 1.754251, acc: 45%] [G loss: 1.997076]\n",
      "[Epoch 38/200] [Batch 180/340] [D loss: 1.666480, acc: 48%] [G loss: 2.243256]\n",
      "[Epoch 38/200] [Batch 200/340] [D loss: 1.726572, acc: 41%] [G loss: 2.524635]\n",
      "[Epoch 38/200] [Batch 220/340] [D loss: 1.730260, acc: 46%] [G loss: 2.205943]\n",
      "[Epoch 38/200] [Batch 240/340] [D loss: 1.768231, acc: 45%] [G loss: 2.179739]\n",
      "[Epoch 38/200] [Batch 260/340] [D loss: 1.727127, acc: 43%] [G loss: 2.085092]\n",
      "[Epoch 38/200] [Batch 280/340] [D loss: 1.757115, acc: 44%] [G loss: 2.281468]\n",
      "[Epoch 38/200] [Batch 300/340] [D loss: 1.733602, acc: 47%] [G loss: 2.081273]\n",
      "[Epoch 38/200] [Batch 320/340] [D loss: 1.744185, acc: 47%] [G loss: 2.409911]\n",
      "[Epoch 39/200] [Batch 0/340] [D loss: 1.749884, acc: 49%] [G loss: 2.219919]\n",
      "[Epoch 39/200] [Batch 20/340] [D loss: 1.761654, acc: 47%] [G loss: 2.241142]\n",
      "[Epoch 39/200] [Batch 40/340] [D loss: 1.693722, acc: 51%] [G loss: 2.111110]\n",
      "[Epoch 39/200] [Batch 60/340] [D loss: 1.694433, acc: 49%] [G loss: 2.309498]\n",
      "[Epoch 39/200] [Batch 80/340] [D loss: 1.712941, acc: 47%] [G loss: 2.226945]\n",
      "[Epoch 39/200] [Batch 100/340] [D loss: 1.770444, acc: 43%] [G loss: 2.228197]\n",
      "[Epoch 39/200] [Batch 120/340] [D loss: 1.716352, acc: 44%] [G loss: 2.424382]\n",
      "[Epoch 39/200] [Batch 140/340] [D loss: 1.704295, acc: 48%] [G loss: 2.483552]\n",
      "[Epoch 39/200] [Batch 160/340] [D loss: 1.727595, acc: 45%] [G loss: 2.144433]\n",
      "[Epoch 39/200] [Batch 180/340] [D loss: 1.731483, acc: 47%] [G loss: 2.296777]\n",
      "[Epoch 39/200] [Batch 200/340] [D loss: 1.712799, acc: 47%] [G loss: 2.316854]\n",
      "[Epoch 39/200] [Batch 220/340] [D loss: 1.707981, acc: 48%] [G loss: 2.259203]\n",
      "[Epoch 39/200] [Batch 240/340] [D loss: 1.682413, acc: 47%] [G loss: 2.308301]\n",
      "[Epoch 39/200] [Batch 260/340] [D loss: 1.695904, acc: 48%] [G loss: 2.175279]\n",
      "[Epoch 39/200] [Batch 280/340] [D loss: 1.688724, acc: 47%] [G loss: 2.204823]\n",
      "[Epoch 39/200] [Batch 300/340] [D loss: 1.718026, acc: 43%] [G loss: 2.310886]\n",
      "[Epoch 39/200] [Batch 320/340] [D loss: 1.729456, acc: 45%] [G loss: 2.534364]\n",
      "[Epoch 40/200] [Batch 0/340] [D loss: 1.719453, acc: 44%] [G loss: 2.166711]\n",
      "[Epoch 40/200] [Batch 20/340] [D loss: 1.745416, acc: 45%] [G loss: 2.257570]\n",
      "[Epoch 40/200] [Batch 40/340] [D loss: 1.744209, acc: 41%] [G loss: 2.469128]\n",
      "[Epoch 40/200] [Batch 60/340] [D loss: 1.684659, acc: 48%] [G loss: 2.367632]\n",
      "[Epoch 40/200] [Batch 80/340] [D loss: 1.716856, acc: 46%] [G loss: 2.228118]\n",
      "[Epoch 40/200] [Batch 100/340] [D loss: 1.741820, acc: 46%] [G loss: 2.242600]\n",
      "[Epoch 40/200] [Batch 120/340] [D loss: 1.705718, acc: 48%] [G loss: 2.258988]\n",
      "[Epoch 40/200] [Batch 140/340] [D loss: 1.704721, acc: 47%] [G loss: 2.298934]\n",
      "[Epoch 40/200] [Batch 160/340] [D loss: 1.769267, acc: 48%] [G loss: 2.149590]\n",
      "[Epoch 40/200] [Batch 180/340] [D loss: 1.703066, acc: 47%] [G loss: 2.173276]\n",
      "[Epoch 40/200] [Batch 200/340] [D loss: 1.700534, acc: 44%] [G loss: 2.510342]\n",
      "[Epoch 40/200] [Batch 220/340] [D loss: 1.703257, acc: 45%] [G loss: 2.296916]\n",
      "[Epoch 40/200] [Batch 240/340] [D loss: 1.696980, acc: 50%] [G loss: 2.350688]\n",
      "[Epoch 40/200] [Batch 260/340] [D loss: 1.739317, acc: 49%] [G loss: 2.319059]\n",
      "[Epoch 40/200] [Batch 280/340] [D loss: 1.739026, acc: 47%] [G loss: 2.083018]\n",
      "[Epoch 40/200] [Batch 300/340] [D loss: 1.704499, acc: 48%] [G loss: 2.393789]\n",
      "[Epoch 40/200] [Batch 320/340] [D loss: 1.708747, acc: 49%] [G loss: 2.325322]\n",
      "[Epoch 41/200] [Batch 0/340] [D loss: 1.727247, acc: 48%] [G loss: 2.345250]\n",
      "[Epoch 41/200] [Batch 20/340] [D loss: 1.712866, acc: 49%] [G loss: 2.333176]\n",
      "[Epoch 41/200] [Batch 40/340] [D loss: 1.745868, acc: 49%] [G loss: 2.327870]\n",
      "[Epoch 41/200] [Batch 60/340] [D loss: 1.733002, acc: 43%] [G loss: 2.013967]\n",
      "[Epoch 41/200] [Batch 80/340] [D loss: 1.706972, acc: 46%] [G loss: 2.338258]\n",
      "[Epoch 41/200] [Batch 100/340] [D loss: 1.706097, acc: 48%] [G loss: 2.246658]\n",
      "[Epoch 41/200] [Batch 120/340] [D loss: 1.713182, acc: 45%] [G loss: 2.397953]\n",
      "[Epoch 41/200] [Batch 140/340] [D loss: 1.727204, acc: 44%] [G loss: 2.486720]\n",
      "[Epoch 41/200] [Batch 160/340] [D loss: 1.677193, acc: 50%] [G loss: 2.387164]\n",
      "[Epoch 41/200] [Batch 180/340] [D loss: 1.738861, acc: 46%] [G loss: 2.094174]\n",
      "[Epoch 41/200] [Batch 200/340] [D loss: 1.703860, acc: 47%] [G loss: 2.195319]\n",
      "[Epoch 41/200] [Batch 220/340] [D loss: 1.762416, acc: 49%] [G loss: 2.128448]\n",
      "[Epoch 41/200] [Batch 240/340] [D loss: 1.766309, acc: 46%] [G loss: 2.241658]\n",
      "[Epoch 41/200] [Batch 260/340] [D loss: 1.672967, acc: 47%] [G loss: 2.525346]\n",
      "[Epoch 41/200] [Batch 280/340] [D loss: 1.704925, acc: 50%] [G loss: 2.690054]\n",
      "[Epoch 41/200] [Batch 300/340] [D loss: 1.705891, acc: 50%] [G loss: 2.427866]\n",
      "[Epoch 41/200] [Batch 320/340] [D loss: 1.733227, acc: 47%] [G loss: 2.180503]\n",
      "[Epoch 42/200] [Batch 0/340] [D loss: 1.722772, acc: 50%] [G loss: 2.222290]\n",
      "[Epoch 42/200] [Batch 20/340] [D loss: 1.707479, acc: 49%] [G loss: 2.296959]\n",
      "[Epoch 42/200] [Batch 40/340] [D loss: 1.688104, acc: 46%] [G loss: 2.324941]\n",
      "[Epoch 42/200] [Batch 60/340] [D loss: 1.674948, acc: 50%] [G loss: 2.280514]\n",
      "[Epoch 42/200] [Batch 80/340] [D loss: 1.705765, acc: 49%] [G loss: 2.344961]\n",
      "[Epoch 42/200] [Batch 100/340] [D loss: 1.744440, acc: 46%] [G loss: 2.135654]\n",
      "[Epoch 42/200] [Batch 120/340] [D loss: 1.747678, acc: 49%] [G loss: 2.215889]\n",
      "[Epoch 42/200] [Batch 140/340] [D loss: 1.721311, acc: 46%] [G loss: 2.065741]\n",
      "[Epoch 42/200] [Batch 160/340] [D loss: 1.788385, acc: 49%] [G loss: 2.157639]\n",
      "[Epoch 42/200] [Batch 180/340] [D loss: 1.777325, acc: 44%] [G loss: 2.264328]\n",
      "[Epoch 42/200] [Batch 200/340] [D loss: 1.725619, acc: 47%] [G loss: 2.260825]\n",
      "[Epoch 42/200] [Batch 220/340] [D loss: 1.685599, acc: 49%] [G loss: 2.436926]\n",
      "[Epoch 42/200] [Batch 240/340] [D loss: 1.702691, acc: 48%] [G loss: 2.190545]\n",
      "[Epoch 42/200] [Batch 260/340] [D loss: 1.744500, acc: 49%] [G loss: 2.123329]\n",
      "[Epoch 42/200] [Batch 280/340] [D loss: 1.718416, acc: 46%] [G loss: 2.388518]\n",
      "[Epoch 42/200] [Batch 300/340] [D loss: 1.686647, acc: 51%] [G loss: 2.271064]\n",
      "[Epoch 42/200] [Batch 320/340] [D loss: 1.692329, acc: 48%] [G loss: 2.317392]\n",
      "[Epoch 43/200] [Batch 0/340] [D loss: 1.700902, acc: 48%] [G loss: 2.543594]\n",
      "[Epoch 43/200] [Batch 20/340] [D loss: 1.742016, acc: 47%] [G loss: 2.141881]\n",
      "[Epoch 43/200] [Batch 40/340] [D loss: 1.717694, acc: 49%] [G loss: 2.172298]\n",
      "[Epoch 43/200] [Batch 60/340] [D loss: 1.683228, acc: 51%] [G loss: 2.294524]\n",
      "[Epoch 43/200] [Batch 80/340] [D loss: 1.730827, acc: 51%] [G loss: 2.264102]\n",
      "[Epoch 43/200] [Batch 100/340] [D loss: 1.717976, acc: 50%] [G loss: 2.328422]\n",
      "[Epoch 43/200] [Batch 120/340] [D loss: 1.731717, acc: 48%] [G loss: 1.960356]\n",
      "[Epoch 43/200] [Batch 140/340] [D loss: 1.741937, acc: 47%] [G loss: 2.424360]\n",
      "[Epoch 43/200] [Batch 160/340] [D loss: 1.734310, acc: 50%] [G loss: 2.042470]\n",
      "[Epoch 43/200] [Batch 180/340] [D loss: 1.672056, acc: 49%] [G loss: 2.106627]\n",
      "[Epoch 43/200] [Batch 200/340] [D loss: 1.668132, acc: 50%] [G loss: 2.345131]\n",
      "[Epoch 43/200] [Batch 220/340] [D loss: 1.698045, acc: 50%] [G loss: 2.211916]\n",
      "[Epoch 43/200] [Batch 240/340] [D loss: 1.717645, acc: 48%] [G loss: 2.015968]\n",
      "[Epoch 43/200] [Batch 260/340] [D loss: 1.766492, acc: 48%] [G loss: 2.276373]\n",
      "[Epoch 43/200] [Batch 280/340] [D loss: 1.694084, acc: 53%] [G loss: 2.065069]\n",
      "[Epoch 43/200] [Batch 300/340] [D loss: 1.666451, acc: 49%] [G loss: 2.189569]\n",
      "[Epoch 43/200] [Batch 320/340] [D loss: 1.731930, acc: 45%] [G loss: 2.496026]\n",
      "[Epoch 44/200] [Batch 0/340] [D loss: 1.699884, acc: 51%] [G loss: 2.597662]\n",
      "[Epoch 44/200] [Batch 20/340] [D loss: 1.709619, acc: 47%] [G loss: 2.021433]\n",
      "[Epoch 44/200] [Batch 40/340] [D loss: 1.758325, acc: 52%] [G loss: 2.209960]\n",
      "[Epoch 44/200] [Batch 60/340] [D loss: 1.684171, acc: 47%] [G loss: 2.192400]\n",
      "[Epoch 44/200] [Batch 80/340] [D loss: 1.717463, acc: 46%] [G loss: 2.070517]\n",
      "[Epoch 44/200] [Batch 100/340] [D loss: 1.776144, acc: 51%] [G loss: 2.093618]\n",
      "[Epoch 44/200] [Batch 120/340] [D loss: 1.755402, acc: 49%] [G loss: 2.109451]\n",
      "[Epoch 44/200] [Batch 140/340] [D loss: 1.751202, acc: 47%] [G loss: 2.323140]\n",
      "[Epoch 44/200] [Batch 160/340] [D loss: 1.782315, acc: 48%] [G loss: 2.054104]\n",
      "[Epoch 44/200] [Batch 180/340] [D loss: 1.716847, acc: 49%] [G loss: 2.271559]\n",
      "[Epoch 44/200] [Batch 200/340] [D loss: 1.717676, acc: 47%] [G loss: 2.204843]\n",
      "[Epoch 44/200] [Batch 220/340] [D loss: 1.804128, acc: 48%] [G loss: 2.022462]\n",
      "[Epoch 44/200] [Batch 240/340] [D loss: 1.679143, acc: 46%] [G loss: 2.248286]\n",
      "[Epoch 44/200] [Batch 260/340] [D loss: 1.673865, acc: 50%] [G loss: 2.377141]\n",
      "[Epoch 44/200] [Batch 280/340] [D loss: 1.720881, acc: 46%] [G loss: 2.672337]\n",
      "[Epoch 44/200] [Batch 300/340] [D loss: 1.728011, acc: 48%] [G loss: 2.166187]\n",
      "[Epoch 44/200] [Batch 320/340] [D loss: 1.718808, acc: 44%] [G loss: 2.288924]\n",
      "[Epoch 45/200] [Batch 0/340] [D loss: 1.678583, acc: 49%] [G loss: 2.459116]\n",
      "[Epoch 45/200] [Batch 20/340] [D loss: 1.716648, acc: 48%] [G loss: 2.227543]\n",
      "[Epoch 45/200] [Batch 40/340] [D loss: 1.682646, acc: 47%] [G loss: 2.384516]\n",
      "[Epoch 45/200] [Batch 60/340] [D loss: 1.694188, acc: 46%] [G loss: 2.330261]\n",
      "[Epoch 45/200] [Batch 80/340] [D loss: 1.728947, acc: 47%] [G loss: 2.297768]\n",
      "[Epoch 45/200] [Batch 100/340] [D loss: 1.684610, acc: 52%] [G loss: 2.270240]\n",
      "[Epoch 45/200] [Batch 120/340] [D loss: 1.670845, acc: 50%] [G loss: 2.263424]\n",
      "[Epoch 45/200] [Batch 140/340] [D loss: 1.741279, acc: 46%] [G loss: 2.396841]\n",
      "[Epoch 45/200] [Batch 160/340] [D loss: 1.711925, acc: 50%] [G loss: 2.169714]\n",
      "[Epoch 45/200] [Batch 180/340] [D loss: 1.748562, acc: 47%] [G loss: 2.011218]\n",
      "[Epoch 45/200] [Batch 200/340] [D loss: 1.696425, acc: 47%] [G loss: 2.429545]\n",
      "[Epoch 45/200] [Batch 220/340] [D loss: 1.774732, acc: 48%] [G loss: 2.430590]\n",
      "[Epoch 45/200] [Batch 240/340] [D loss: 1.746414, acc: 48%] [G loss: 1.958416]\n",
      "[Epoch 45/200] [Batch 260/340] [D loss: 1.699855, acc: 49%] [G loss: 2.135455]\n",
      "[Epoch 45/200] [Batch 280/340] [D loss: 1.695607, acc: 50%] [G loss: 2.616584]\n",
      "[Epoch 45/200] [Batch 300/340] [D loss: 1.713727, acc: 47%] [G loss: 2.243906]\n",
      "[Epoch 45/200] [Batch 320/340] [D loss: 1.715126, acc: 47%] [G loss: 2.270231]\n",
      "[Epoch 46/200] [Batch 0/340] [D loss: 1.718921, acc: 48%] [G loss: 2.320140]\n",
      "[Epoch 46/200] [Batch 20/340] [D loss: 1.719106, acc: 48%] [G loss: 2.229434]\n",
      "[Epoch 46/200] [Batch 40/340] [D loss: 1.680610, acc: 50%] [G loss: 2.213547]\n",
      "[Epoch 46/200] [Batch 60/340] [D loss: 1.713600, acc: 49%] [G loss: 2.395729]\n",
      "[Epoch 46/200] [Batch 80/340] [D loss: 1.708581, acc: 50%] [G loss: 2.055399]\n",
      "[Epoch 46/200] [Batch 100/340] [D loss: 1.723872, acc: 49%] [G loss: 2.280701]\n",
      "[Epoch 46/200] [Batch 120/340] [D loss: 1.716003, acc: 50%] [G loss: 2.354194]\n",
      "[Epoch 46/200] [Batch 140/340] [D loss: 1.743681, acc: 48%] [G loss: 2.094183]\n",
      "[Epoch 46/200] [Batch 160/340] [D loss: 1.717070, acc: 49%] [G loss: 2.278242]\n",
      "[Epoch 46/200] [Batch 180/340] [D loss: 1.701354, acc: 50%] [G loss: 2.111289]\n",
      "[Epoch 46/200] [Batch 200/340] [D loss: 1.693483, acc: 50%] [G loss: 2.030964]\n",
      "[Epoch 46/200] [Batch 220/340] [D loss: 1.717723, acc: 48%] [G loss: 2.289291]\n",
      "[Epoch 46/200] [Batch 240/340] [D loss: 1.654145, acc: 49%] [G loss: 2.306662]\n",
      "[Epoch 46/200] [Batch 260/340] [D loss: 1.736491, acc: 46%] [G loss: 2.051964]\n",
      "[Epoch 46/200] [Batch 280/340] [D loss: 1.667127, acc: 48%] [G loss: 2.253876]\n",
      "[Epoch 46/200] [Batch 300/340] [D loss: 1.690784, acc: 48%] [G loss: 2.217035]\n",
      "[Epoch 46/200] [Batch 320/340] [D loss: 1.671689, acc: 52%] [G loss: 2.416886]\n",
      "[Epoch 47/200] [Batch 0/340] [D loss: 1.753554, acc: 49%] [G loss: 2.406234]\n",
      "[Epoch 47/200] [Batch 20/340] [D loss: 1.730650, acc: 50%] [G loss: 2.073796]\n",
      "[Epoch 47/200] [Batch 40/340] [D loss: 1.730562, acc: 47%] [G loss: 2.211020]\n",
      "[Epoch 47/200] [Batch 60/340] [D loss: 1.739399, acc: 50%] [G loss: 2.209759]\n",
      "[Epoch 47/200] [Batch 80/340] [D loss: 1.695268, acc: 50%] [G loss: 2.242019]\n",
      "[Epoch 47/200] [Batch 100/340] [D loss: 1.755002, acc: 47%] [G loss: 2.318237]\n",
      "[Epoch 47/200] [Batch 120/340] [D loss: 1.688746, acc: 48%] [G loss: 2.213374]\n",
      "[Epoch 47/200] [Batch 140/340] [D loss: 1.756249, acc: 47%] [G loss: 2.401301]\n",
      "[Epoch 47/200] [Batch 160/340] [D loss: 1.736364, acc: 51%] [G loss: 2.222288]\n",
      "[Epoch 47/200] [Batch 180/340] [D loss: 1.696032, acc: 48%] [G loss: 1.937286]\n",
      "[Epoch 47/200] [Batch 200/340] [D loss: 1.715160, acc: 46%] [G loss: 2.194460]\n",
      "[Epoch 47/200] [Batch 220/340] [D loss: 1.702210, acc: 53%] [G loss: 2.446777]\n",
      "[Epoch 47/200] [Batch 240/340] [D loss: 1.702022, acc: 51%] [G loss: 2.277397]\n",
      "[Epoch 47/200] [Batch 260/340] [D loss: 1.731416, acc: 51%] [G loss: 2.239730]\n",
      "[Epoch 47/200] [Batch 280/340] [D loss: 1.728525, acc: 49%] [G loss: 2.227660]\n",
      "[Epoch 47/200] [Batch 300/340] [D loss: 1.708731, acc: 49%] [G loss: 2.190789]\n",
      "[Epoch 47/200] [Batch 320/340] [D loss: 1.709448, acc: 49%] [G loss: 2.329498]\n",
      "[Epoch 48/200] [Batch 0/340] [D loss: 1.761757, acc: 47%] [G loss: 1.987414]\n",
      "[Epoch 48/200] [Batch 20/340] [D loss: 1.670970, acc: 53%] [G loss: 2.121334]\n",
      "[Epoch 48/200] [Batch 40/340] [D loss: 1.684375, acc: 51%] [G loss: 2.042448]\n",
      "[Epoch 48/200] [Batch 60/340] [D loss: 1.786047, acc: 42%] [G loss: 2.344997]\n",
      "[Epoch 48/200] [Batch 80/340] [D loss: 1.703390, acc: 44%] [G loss: 2.107724]\n",
      "[Epoch 48/200] [Batch 100/340] [D loss: 1.679924, acc: 47%] [G loss: 2.309827]\n",
      "[Epoch 48/200] [Batch 120/340] [D loss: 1.738843, acc: 48%] [G loss: 2.229441]\n",
      "[Epoch 48/200] [Batch 140/340] [D loss: 1.662243, acc: 50%] [G loss: 2.202374]\n",
      "[Epoch 48/200] [Batch 160/340] [D loss: 1.720594, acc: 49%] [G loss: 2.211862]\n",
      "[Epoch 48/200] [Batch 180/340] [D loss: 1.655042, acc: 52%] [G loss: 2.453662]\n",
      "[Epoch 48/200] [Batch 200/340] [D loss: 1.646151, acc: 55%] [G loss: 2.360882]\n",
      "[Epoch 48/200] [Batch 220/340] [D loss: 1.753690, acc: 47%] [G loss: 2.002590]\n",
      "[Epoch 48/200] [Batch 240/340] [D loss: 1.707088, acc: 47%] [G loss: 2.343577]\n",
      "[Epoch 48/200] [Batch 260/340] [D loss: 1.683937, acc: 48%] [G loss: 2.269258]\n",
      "[Epoch 48/200] [Batch 280/340] [D loss: 1.706918, acc: 52%] [G loss: 2.178724]\n",
      "[Epoch 48/200] [Batch 300/340] [D loss: 1.719330, acc: 49%] [G loss: 2.374835]\n",
      "[Epoch 48/200] [Batch 320/340] [D loss: 1.745723, acc: 49%] [G loss: 2.177119]\n",
      "[Epoch 49/200] [Batch 0/340] [D loss: 1.674043, acc: 45%] [G loss: 2.271631]\n",
      "[Epoch 49/200] [Batch 20/340] [D loss: 1.711653, acc: 53%] [G loss: 2.120475]\n",
      "[Epoch 49/200] [Batch 40/340] [D loss: 1.757873, acc: 51%] [G loss: 2.488961]\n",
      "[Epoch 49/200] [Batch 60/340] [D loss: 1.690243, acc: 49%] [G loss: 2.318063]\n",
      "[Epoch 49/200] [Batch 80/340] [D loss: 1.703370, acc: 50%] [G loss: 2.197565]\n",
      "[Epoch 49/200] [Batch 100/340] [D loss: 1.728693, acc: 47%] [G loss: 2.316185]\n",
      "[Epoch 49/200] [Batch 120/340] [D loss: 1.703251, acc: 50%] [G loss: 2.285036]\n",
      "[Epoch 49/200] [Batch 140/340] [D loss: 1.714195, acc: 47%] [G loss: 2.182127]\n",
      "[Epoch 49/200] [Batch 160/340] [D loss: 1.719234, acc: 48%] [G loss: 2.144552]\n",
      "[Epoch 49/200] [Batch 180/340] [D loss: 1.697789, acc: 52%] [G loss: 2.005268]\n",
      "[Epoch 49/200] [Batch 200/340] [D loss: 1.713861, acc: 48%] [G loss: 2.351219]\n",
      "[Epoch 49/200] [Batch 220/340] [D loss: 1.748276, acc: 48%] [G loss: 2.107350]\n",
      "[Epoch 49/200] [Batch 240/340] [D loss: 1.722090, acc: 52%] [G loss: 2.289451]\n",
      "[Epoch 49/200] [Batch 260/340] [D loss: 1.674559, acc: 51%] [G loss: 2.108266]\n",
      "[Epoch 49/200] [Batch 280/340] [D loss: 1.720054, acc: 46%] [G loss: 2.309958]\n",
      "[Epoch 49/200] [Batch 300/340] [D loss: 1.739268, acc: 50%] [G loss: 2.559990]\n",
      "[Epoch 49/200] [Batch 320/340] [D loss: 1.745305, acc: 51%] [G loss: 2.088283]\n",
      "[Epoch 50/200] [Batch 0/340] [D loss: 1.712503, acc: 54%] [G loss: 2.552317]\n",
      "[Epoch 50/200] [Batch 20/340] [D loss: 1.775734, acc: 44%] [G loss: 2.106778]\n",
      "[Epoch 50/200] [Batch 40/340] [D loss: 1.721024, acc: 50%] [G loss: 2.346833]\n",
      "[Epoch 50/200] [Batch 60/340] [D loss: 1.742520, acc: 52%] [G loss: 2.094230]\n",
      "[Epoch 50/200] [Batch 80/340] [D loss: 1.715273, acc: 51%] [G loss: 2.145970]\n",
      "[Epoch 50/200] [Batch 100/340] [D loss: 1.685378, acc: 51%] [G loss: 2.296129]\n",
      "[Epoch 50/200] [Batch 120/340] [D loss: 1.729136, acc: 54%] [G loss: 2.284945]\n",
      "[Epoch 50/200] [Batch 140/340] [D loss: 1.708543, acc: 51%] [G loss: 2.274580]\n",
      "[Epoch 50/200] [Batch 160/340] [D loss: 1.727573, acc: 50%] [G loss: 2.354155]\n",
      "[Epoch 50/200] [Batch 180/340] [D loss: 1.710118, acc: 48%] [G loss: 2.265337]\n",
      "[Epoch 50/200] [Batch 200/340] [D loss: 1.733911, acc: 50%] [G loss: 2.121337]\n",
      "[Epoch 50/200] [Batch 220/340] [D loss: 1.665291, acc: 53%] [G loss: 2.179828]\n",
      "[Epoch 50/200] [Batch 240/340] [D loss: 1.726049, acc: 50%] [G loss: 2.061095]\n",
      "[Epoch 50/200] [Batch 260/340] [D loss: 1.669353, acc: 50%] [G loss: 2.412362]\n",
      "[Epoch 50/200] [Batch 280/340] [D loss: 1.817514, acc: 48%] [G loss: 2.385535]\n",
      "[Epoch 50/200] [Batch 300/340] [D loss: 1.679299, acc: 49%] [G loss: 2.291558]\n",
      "[Epoch 50/200] [Batch 320/340] [D loss: 1.686658, acc: 48%] [G loss: 2.261663]\n",
      "[Epoch 51/200] [Batch 0/340] [D loss: 1.698474, acc: 52%] [G loss: 2.281217]\n",
      "[Epoch 51/200] [Batch 20/340] [D loss: 1.704697, acc: 51%] [G loss: 2.098543]\n",
      "[Epoch 51/200] [Batch 40/340] [D loss: 1.687073, acc: 51%] [G loss: 2.103786]\n",
      "[Epoch 51/200] [Batch 60/340] [D loss: 1.764698, acc: 49%] [G loss: 1.977861]\n",
      "[Epoch 51/200] [Batch 80/340] [D loss: 1.707639, acc: 48%] [G loss: 2.358148]\n",
      "[Epoch 51/200] [Batch 100/340] [D loss: 1.703735, acc: 49%] [G loss: 2.128416]\n",
      "[Epoch 51/200] [Batch 120/340] [D loss: 1.728337, acc: 50%] [G loss: 2.306225]\n",
      "[Epoch 51/200] [Batch 140/340] [D loss: 1.720193, acc: 50%] [G loss: 2.516681]\n",
      "[Epoch 51/200] [Batch 160/340] [D loss: 1.694200, acc: 52%] [G loss: 2.282540]\n",
      "[Epoch 51/200] [Batch 180/340] [D loss: 1.710536, acc: 49%] [G loss: 2.017786]\n",
      "[Epoch 51/200] [Batch 200/340] [D loss: 1.737738, acc: 48%] [G loss: 2.155251]\n",
      "[Epoch 51/200] [Batch 220/340] [D loss: 1.685245, acc: 50%] [G loss: 2.173978]\n",
      "[Epoch 51/200] [Batch 240/340] [D loss: 1.752442, acc: 52%] [G loss: 2.340278]\n",
      "[Epoch 51/200] [Batch 260/340] [D loss: 1.724835, acc: 54%] [G loss: 2.247355]\n",
      "[Epoch 51/200] [Batch 280/340] [D loss: 1.723382, acc: 47%] [G loss: 1.987888]\n",
      "[Epoch 51/200] [Batch 300/340] [D loss: 1.694432, acc: 49%] [G loss: 2.077473]\n",
      "[Epoch 51/200] [Batch 320/340] [D loss: 1.692871, acc: 48%] [G loss: 2.325607]\n",
      "[Epoch 52/200] [Batch 0/340] [D loss: 1.700763, acc: 48%] [G loss: 2.271716]\n",
      "[Epoch 52/200] [Batch 20/340] [D loss: 1.690478, acc: 51%] [G loss: 2.140399]\n",
      "[Epoch 52/200] [Batch 40/340] [D loss: 1.707306, acc: 49%] [G loss: 2.317701]\n",
      "[Epoch 52/200] [Batch 60/340] [D loss: 1.688734, acc: 49%] [G loss: 2.020845]\n",
      "[Epoch 52/200] [Batch 80/340] [D loss: 1.750369, acc: 50%] [G loss: 2.273674]\n",
      "[Epoch 52/200] [Batch 100/340] [D loss: 1.742679, acc: 55%] [G loss: 2.005325]\n",
      "[Epoch 52/200] [Batch 120/340] [D loss: 1.776001, acc: 48%] [G loss: 2.687069]\n",
      "[Epoch 52/200] [Batch 140/340] [D loss: 1.678747, acc: 52%] [G loss: 2.030189]\n",
      "[Epoch 52/200] [Batch 160/340] [D loss: 1.713825, acc: 49%] [G loss: 2.189781]\n",
      "[Epoch 52/200] [Batch 180/340] [D loss: 1.710398, acc: 52%] [G loss: 2.070063]\n",
      "[Epoch 52/200] [Batch 200/340] [D loss: 1.679013, acc: 54%] [G loss: 2.207519]\n",
      "[Epoch 52/200] [Batch 220/340] [D loss: 1.672595, acc: 51%] [G loss: 2.202841]\n",
      "[Epoch 52/200] [Batch 240/340] [D loss: 1.700917, acc: 44%] [G loss: 2.178860]\n",
      "[Epoch 52/200] [Batch 260/340] [D loss: 1.690854, acc: 48%] [G loss: 2.244927]\n",
      "[Epoch 52/200] [Batch 280/340] [D loss: 1.654469, acc: 53%] [G loss: 2.696426]\n",
      "[Epoch 52/200] [Batch 300/340] [D loss: 1.622151, acc: 50%] [G loss: 2.076718]\n",
      "[Epoch 52/200] [Batch 320/340] [D loss: 1.716558, acc: 50%] [G loss: 2.021509]\n",
      "[Epoch 53/200] [Batch 0/340] [D loss: 1.762301, acc: 50%] [G loss: 1.987739]\n",
      "[Epoch 53/200] [Batch 20/340] [D loss: 1.690176, acc: 55%] [G loss: 2.157429]\n",
      "[Epoch 53/200] [Batch 40/340] [D loss: 1.744658, acc: 49%] [G loss: 2.545080]\n",
      "[Epoch 53/200] [Batch 60/340] [D loss: 1.747087, acc: 50%] [G loss: 2.220645]\n",
      "[Epoch 53/200] [Batch 80/340] [D loss: 1.646789, acc: 48%] [G loss: 2.145196]\n",
      "[Epoch 53/200] [Batch 100/340] [D loss: 1.677090, acc: 50%] [G loss: 2.157578]\n",
      "[Epoch 53/200] [Batch 120/340] [D loss: 1.772532, acc: 47%] [G loss: 2.126074]\n",
      "[Epoch 53/200] [Batch 140/340] [D loss: 1.717960, acc: 48%] [G loss: 2.243094]\n",
      "[Epoch 53/200] [Batch 160/340] [D loss: 1.650802, acc: 51%] [G loss: 2.199450]\n",
      "[Epoch 53/200] [Batch 180/340] [D loss: 1.709724, acc: 49%] [G loss: 2.381067]\n",
      "[Epoch 53/200] [Batch 200/340] [D loss: 1.730775, acc: 52%] [G loss: 2.211216]\n",
      "[Epoch 53/200] [Batch 220/340] [D loss: 1.728702, acc: 48%] [G loss: 2.103045]\n",
      "[Epoch 53/200] [Batch 240/340] [D loss: 1.703335, acc: 52%] [G loss: 2.201989]\n",
      "[Epoch 53/200] [Batch 260/340] [D loss: 1.705365, acc: 51%] [G loss: 2.157444]\n",
      "[Epoch 53/200] [Batch 280/340] [D loss: 1.716190, acc: 49%] [G loss: 1.962912]\n",
      "[Epoch 53/200] [Batch 300/340] [D loss: 1.733696, acc: 50%] [G loss: 2.008152]\n",
      "[Epoch 53/200] [Batch 320/340] [D loss: 1.722114, acc: 47%] [G loss: 2.286814]\n",
      "[Epoch 54/200] [Batch 0/340] [D loss: 1.711793, acc: 49%] [G loss: 2.130857]\n",
      "[Epoch 54/200] [Batch 20/340] [D loss: 1.726374, acc: 51%] [G loss: 2.361427]\n",
      "[Epoch 54/200] [Batch 40/340] [D loss: 1.699758, acc: 50%] [G loss: 2.369195]\n",
      "[Epoch 54/200] [Batch 60/340] [D loss: 1.707711, acc: 50%] [G loss: 2.241995]\n",
      "[Epoch 54/200] [Batch 80/340] [D loss: 1.630121, acc: 50%] [G loss: 2.332393]\n",
      "[Epoch 54/200] [Batch 100/340] [D loss: 1.717618, acc: 50%] [G loss: 2.209197]\n",
      "[Epoch 54/200] [Batch 120/340] [D loss: 1.728137, acc: 48%] [G loss: 2.193950]\n",
      "[Epoch 54/200] [Batch 140/340] [D loss: 1.733922, acc: 50%] [G loss: 2.075457]\n",
      "[Epoch 54/200] [Batch 160/340] [D loss: 1.672912, acc: 50%] [G loss: 2.198020]\n",
      "[Epoch 54/200] [Batch 180/340] [D loss: 1.793564, acc: 50%] [G loss: 2.195837]\n",
      "[Epoch 54/200] [Batch 200/340] [D loss: 1.780598, acc: 50%] [G loss: 2.627686]\n",
      "[Epoch 54/200] [Batch 220/340] [D loss: 1.676418, acc: 53%] [G loss: 2.100015]\n",
      "[Epoch 54/200] [Batch 240/340] [D loss: 1.690672, acc: 49%] [G loss: 2.423833]\n",
      "[Epoch 54/200] [Batch 260/340] [D loss: 1.712204, acc: 51%] [G loss: 2.200879]\n",
      "[Epoch 54/200] [Batch 280/340] [D loss: 1.752393, acc: 51%] [G loss: 2.153123]\n",
      "[Epoch 54/200] [Batch 300/340] [D loss: 1.689251, acc: 53%] [G loss: 2.270023]\n",
      "[Epoch 54/200] [Batch 320/340] [D loss: 1.709615, acc: 51%] [G loss: 2.389013]\n",
      "[Epoch 55/200] [Batch 0/340] [D loss: 1.687189, acc: 50%] [G loss: 2.205251]\n",
      "[Epoch 55/200] [Batch 20/340] [D loss: 1.715130, acc: 56%] [G loss: 2.322603]\n",
      "[Epoch 55/200] [Batch 40/340] [D loss: 1.676436, acc: 51%] [G loss: 2.180602]\n",
      "[Epoch 55/200] [Batch 60/340] [D loss: 1.739348, acc: 47%] [G loss: 2.061274]\n",
      "[Epoch 55/200] [Batch 80/340] [D loss: 1.729948, acc: 51%] [G loss: 1.993495]\n",
      "[Epoch 55/200] [Batch 100/340] [D loss: 1.745718, acc: 50%] [G loss: 2.396914]\n",
      "[Epoch 55/200] [Batch 120/340] [D loss: 1.720760, acc: 47%] [G loss: 2.239779]\n",
      "[Epoch 55/200] [Batch 140/340] [D loss: 1.694708, acc: 53%] [G loss: 2.154762]\n",
      "[Epoch 55/200] [Batch 160/340] [D loss: 1.681644, acc: 47%] [G loss: 2.170609]\n",
      "[Epoch 55/200] [Batch 180/340] [D loss: 1.783602, acc: 50%] [G loss: 2.165612]\n",
      "[Epoch 55/200] [Batch 200/340] [D loss: 1.660077, acc: 50%] [G loss: 2.198301]\n",
      "[Epoch 55/200] [Batch 220/340] [D loss: 1.683575, acc: 49%] [G loss: 2.179759]\n",
      "[Epoch 55/200] [Batch 240/340] [D loss: 1.715656, acc: 50%] [G loss: 2.310345]\n",
      "[Epoch 55/200] [Batch 260/340] [D loss: 1.725182, acc: 53%] [G loss: 2.146946]\n",
      "[Epoch 55/200] [Batch 280/340] [D loss: 1.706932, acc: 52%] [G loss: 2.230428]\n",
      "[Epoch 55/200] [Batch 300/340] [D loss: 1.712084, acc: 52%] [G loss: 2.174960]\n",
      "[Epoch 55/200] [Batch 320/340] [D loss: 1.705081, acc: 49%] [G loss: 2.107733]\n",
      "[Epoch 56/200] [Batch 0/340] [D loss: 1.729242, acc: 51%] [G loss: 2.072429]\n",
      "[Epoch 56/200] [Batch 20/340] [D loss: 1.713758, acc: 55%] [G loss: 1.987191]\n",
      "[Epoch 56/200] [Batch 40/340] [D loss: 1.773383, acc: 49%] [G loss: 2.174319]\n",
      "[Epoch 56/200] [Batch 60/340] [D loss: 1.678742, acc: 54%] [G loss: 2.423090]\n",
      "[Epoch 56/200] [Batch 80/340] [D loss: 1.722962, acc: 54%] [G loss: 1.993685]\n",
      "[Epoch 56/200] [Batch 100/340] [D loss: 1.736722, acc: 48%] [G loss: 2.269413]\n",
      "[Epoch 56/200] [Batch 120/340] [D loss: 1.710242, acc: 54%] [G loss: 2.156317]\n",
      "[Epoch 56/200] [Batch 140/340] [D loss: 1.703264, acc: 53%] [G loss: 2.142190]\n",
      "[Epoch 56/200] [Batch 160/340] [D loss: 1.710790, acc: 53%] [G loss: 2.065727]\n",
      "[Epoch 56/200] [Batch 180/340] [D loss: 1.693951, acc: 50%] [G loss: 2.149123]\n",
      "[Epoch 56/200] [Batch 200/340] [D loss: 1.679162, acc: 53%] [G loss: 2.413194]\n",
      "[Epoch 56/200] [Batch 220/340] [D loss: 1.716414, acc: 51%] [G loss: 1.822171]\n",
      "[Epoch 56/200] [Batch 240/340] [D loss: 1.722630, acc: 54%] [G loss: 2.274990]\n",
      "[Epoch 56/200] [Batch 260/340] [D loss: 1.662031, acc: 53%] [G loss: 1.968422]\n",
      "[Epoch 56/200] [Batch 280/340] [D loss: 1.691989, acc: 52%] [G loss: 2.264516]\n",
      "[Epoch 56/200] [Batch 300/340] [D loss: 1.696483, acc: 50%] [G loss: 2.096994]\n",
      "[Epoch 56/200] [Batch 320/340] [D loss: 1.649858, acc: 56%] [G loss: 2.320026]\n",
      "[Epoch 57/200] [Batch 0/340] [D loss: 1.727355, acc: 52%] [G loss: 2.338476]\n",
      "[Epoch 57/200] [Batch 20/340] [D loss: 1.632055, acc: 53%] [G loss: 2.287736]\n",
      "[Epoch 57/200] [Batch 40/340] [D loss: 1.712854, acc: 56%] [G loss: 2.560955]\n",
      "[Epoch 57/200] [Batch 60/340] [D loss: 1.712879, acc: 52%] [G loss: 2.589288]\n",
      "[Epoch 57/200] [Batch 80/340] [D loss: 1.735545, acc: 54%] [G loss: 2.246532]\n",
      "[Epoch 57/200] [Batch 100/340] [D loss: 1.687331, acc: 49%] [G loss: 2.154405]\n",
      "[Epoch 57/200] [Batch 120/340] [D loss: 1.744382, acc: 52%] [G loss: 2.183358]\n",
      "[Epoch 57/200] [Batch 140/340] [D loss: 1.716520, acc: 50%] [G loss: 2.262412]\n",
      "[Epoch 57/200] [Batch 160/340] [D loss: 1.725402, acc: 50%] [G loss: 2.122135]\n",
      "[Epoch 57/200] [Batch 180/340] [D loss: 1.685551, acc: 51%] [G loss: 2.218875]\n",
      "[Epoch 57/200] [Batch 200/340] [D loss: 1.768013, acc: 49%] [G loss: 2.531767]\n",
      "[Epoch 57/200] [Batch 220/340] [D loss: 1.714298, acc: 54%] [G loss: 2.290873]\n",
      "[Epoch 57/200] [Batch 240/340] [D loss: 1.621171, acc: 54%] [G loss: 2.179246]\n",
      "[Epoch 57/200] [Batch 260/340] [D loss: 1.738974, acc: 53%] [G loss: 2.077339]\n",
      "[Epoch 57/200] [Batch 280/340] [D loss: 1.747051, acc: 52%] [G loss: 1.976977]\n",
      "[Epoch 57/200] [Batch 300/340] [D loss: 1.681659, acc: 52%] [G loss: 2.352808]\n",
      "[Epoch 57/200] [Batch 320/340] [D loss: 1.747619, acc: 51%] [G loss: 2.019260]\n",
      "[Epoch 58/200] [Batch 0/340] [D loss: 1.770926, acc: 52%] [G loss: 2.063627]\n",
      "[Epoch 58/200] [Batch 20/340] [D loss: 1.640589, acc: 53%] [G loss: 2.396566]\n",
      "[Epoch 58/200] [Batch 40/340] [D loss: 1.777112, acc: 56%] [G loss: 2.137207]\n",
      "[Epoch 58/200] [Batch 60/340] [D loss: 1.723763, acc: 53%] [G loss: 2.002181]\n",
      "[Epoch 58/200] [Batch 80/340] [D loss: 1.707326, acc: 49%] [G loss: 2.263768]\n",
      "[Epoch 58/200] [Batch 100/340] [D loss: 1.792099, acc: 51%] [G loss: 1.976566]\n",
      "[Epoch 58/200] [Batch 120/340] [D loss: 1.693602, acc: 49%] [G loss: 2.122784]\n",
      "[Epoch 58/200] [Batch 140/340] [D loss: 1.733944, acc: 52%] [G loss: 2.141324]\n",
      "[Epoch 58/200] [Batch 160/340] [D loss: 1.685258, acc: 55%] [G loss: 2.104361]\n",
      "[Epoch 58/200] [Batch 180/340] [D loss: 1.665618, acc: 52%] [G loss: 2.157449]\n",
      "[Epoch 58/200] [Batch 200/340] [D loss: 1.667816, acc: 55%] [G loss: 2.093567]\n",
      "[Epoch 58/200] [Batch 220/340] [D loss: 1.738830, acc: 51%] [G loss: 1.902822]\n",
      "[Epoch 58/200] [Batch 240/340] [D loss: 1.697152, acc: 50%] [G loss: 2.445286]\n",
      "[Epoch 58/200] [Batch 260/340] [D loss: 1.738336, acc: 53%] [G loss: 2.372462]\n",
      "[Epoch 58/200] [Batch 280/340] [D loss: 1.732944, acc: 54%] [G loss: 2.122897]\n",
      "[Epoch 58/200] [Batch 300/340] [D loss: 1.725731, acc: 50%] [G loss: 2.018592]\n",
      "[Epoch 58/200] [Batch 320/340] [D loss: 1.692026, acc: 57%] [G loss: 2.052706]\n",
      "[Epoch 59/200] [Batch 0/340] [D loss: 1.800291, acc: 52%] [G loss: 2.438670]\n",
      "[Epoch 59/200] [Batch 20/340] [D loss: 1.717725, acc: 49%] [G loss: 2.064834]\n",
      "[Epoch 59/200] [Batch 40/340] [D loss: 1.740931, acc: 53%] [G loss: 2.302848]\n",
      "[Epoch 59/200] [Batch 60/340] [D loss: 1.740609, acc: 50%] [G loss: 2.026726]\n",
      "[Epoch 59/200] [Batch 80/340] [D loss: 1.715808, acc: 49%] [G loss: 1.979005]\n",
      "[Epoch 59/200] [Batch 100/340] [D loss: 1.827346, acc: 50%] [G loss: 1.842803]\n",
      "[Epoch 59/200] [Batch 120/340] [D loss: 1.677119, acc: 54%] [G loss: 2.037026]\n",
      "[Epoch 59/200] [Batch 140/340] [D loss: 1.740601, acc: 53%] [G loss: 2.147266]\n",
      "[Epoch 59/200] [Batch 160/340] [D loss: 1.695807, acc: 51%] [G loss: 2.245663]\n",
      "[Epoch 59/200] [Batch 180/340] [D loss: 1.738449, acc: 52%] [G loss: 2.429773]\n",
      "[Epoch 59/200] [Batch 200/340] [D loss: 1.728399, acc: 50%] [G loss: 2.399661]\n",
      "[Epoch 59/200] [Batch 220/340] [D loss: 1.686420, acc: 52%] [G loss: 2.256870]\n",
      "[Epoch 59/200] [Batch 240/340] [D loss: 1.701531, acc: 55%] [G loss: 2.122506]\n",
      "[Epoch 59/200] [Batch 260/340] [D loss: 1.661912, acc: 53%] [G loss: 2.375309]\n",
      "[Epoch 59/200] [Batch 280/340] [D loss: 1.731520, acc: 50%] [G loss: 2.300650]\n",
      "[Epoch 59/200] [Batch 300/340] [D loss: 1.721704, acc: 52%] [G loss: 1.972137]\n",
      "[Epoch 59/200] [Batch 320/340] [D loss: 1.657523, acc: 54%] [G loss: 2.034109]\n",
      "[Epoch 60/200] [Batch 0/340] [D loss: 1.733097, acc: 51%] [G loss: 2.132375]\n",
      "[Epoch 60/200] [Batch 20/340] [D loss: 1.675133, acc: 51%] [G loss: 1.998455]\n",
      "[Epoch 60/200] [Batch 40/340] [D loss: 1.730553, acc: 55%] [G loss: 2.070900]\n",
      "[Epoch 60/200] [Batch 60/340] [D loss: 1.711648, acc: 51%] [G loss: 2.369859]\n",
      "[Epoch 60/200] [Batch 80/340] [D loss: 1.641721, acc: 51%] [G loss: 2.041870]\n",
      "[Epoch 60/200] [Batch 100/340] [D loss: 1.723277, acc: 57%] [G loss: 2.261266]\n",
      "[Epoch 60/200] [Batch 120/340] [D loss: 1.736121, acc: 53%] [G loss: 2.008689]\n",
      "[Epoch 60/200] [Batch 140/340] [D loss: 1.798443, acc: 51%] [G loss: 2.595524]\n",
      "[Epoch 60/200] [Batch 160/340] [D loss: 1.733910, acc: 50%] [G loss: 2.217243]\n",
      "[Epoch 60/200] [Batch 180/340] [D loss: 1.723000, acc: 56%] [G loss: 2.220841]\n",
      "[Epoch 60/200] [Batch 200/340] [D loss: 1.659729, acc: 57%] [G loss: 2.016065]\n",
      "[Epoch 60/200] [Batch 220/340] [D loss: 1.760645, acc: 51%] [G loss: 2.203124]\n",
      "[Epoch 60/200] [Batch 240/340] [D loss: 1.684508, acc: 53%] [G loss: 2.587819]\n",
      "[Epoch 60/200] [Batch 260/340] [D loss: 1.666342, acc: 50%] [G loss: 2.306110]\n",
      "[Epoch 60/200] [Batch 280/340] [D loss: 1.665155, acc: 49%] [G loss: 2.328776]\n",
      "[Epoch 60/200] [Batch 300/340] [D loss: 1.668171, acc: 52%] [G loss: 2.516037]\n",
      "[Epoch 60/200] [Batch 320/340] [D loss: 1.685600, acc: 53%] [G loss: 2.298904]\n",
      "[Epoch 61/200] [Batch 0/340] [D loss: 1.706297, acc: 53%] [G loss: 2.050847]\n",
      "[Epoch 61/200] [Batch 20/340] [D loss: 1.702514, acc: 51%] [G loss: 2.515389]\n",
      "[Epoch 61/200] [Batch 40/340] [D loss: 1.724151, acc: 50%] [G loss: 2.246083]\n",
      "[Epoch 61/200] [Batch 60/340] [D loss: 1.679206, acc: 56%] [G loss: 2.178895]\n",
      "[Epoch 61/200] [Batch 80/340] [D loss: 1.689848, acc: 53%] [G loss: 2.379672]\n",
      "[Epoch 61/200] [Batch 100/340] [D loss: 1.764184, acc: 53%] [G loss: 2.076083]\n",
      "[Epoch 61/200] [Batch 120/340] [D loss: 1.671199, acc: 57%] [G loss: 2.021062]\n",
      "[Epoch 61/200] [Batch 140/340] [D loss: 1.734896, acc: 50%] [G loss: 2.186044]\n",
      "[Epoch 61/200] [Batch 160/340] [D loss: 1.697803, acc: 51%] [G loss: 1.995695]\n",
      "[Epoch 61/200] [Batch 180/340] [D loss: 1.689864, acc: 50%] [G loss: 2.349231]\n",
      "[Epoch 61/200] [Batch 200/340] [D loss: 1.705986, acc: 53%] [G loss: 2.454197]\n",
      "[Epoch 61/200] [Batch 220/340] [D loss: 1.709461, acc: 53%] [G loss: 2.315360]\n",
      "[Epoch 61/200] [Batch 240/340] [D loss: 1.696039, acc: 49%] [G loss: 2.312433]\n",
      "[Epoch 61/200] [Batch 260/340] [D loss: 1.692684, acc: 51%] [G loss: 2.207948]\n",
      "[Epoch 61/200] [Batch 280/340] [D loss: 1.734150, acc: 51%] [G loss: 2.065513]\n",
      "[Epoch 61/200] [Batch 300/340] [D loss: 1.700298, acc: 52%] [G loss: 2.018462]\n",
      "[Epoch 61/200] [Batch 320/340] [D loss: 1.717119, acc: 52%] [G loss: 2.225700]\n",
      "[Epoch 62/200] [Batch 0/340] [D loss: 1.819368, acc: 53%] [G loss: 2.280750]\n",
      "[Epoch 62/200] [Batch 20/340] [D loss: 1.675067, acc: 57%] [G loss: 2.125476]\n",
      "[Epoch 62/200] [Batch 40/340] [D loss: 1.712602, acc: 51%] [G loss: 2.278667]\n",
      "[Epoch 62/200] [Batch 60/340] [D loss: 1.696394, acc: 51%] [G loss: 2.490044]\n",
      "[Epoch 62/200] [Batch 80/340] [D loss: 1.816130, acc: 49%] [G loss: 2.694593]\n",
      "[Epoch 62/200] [Batch 100/340] [D loss: 1.729965, acc: 51%] [G loss: 2.136282]\n",
      "[Epoch 62/200] [Batch 120/340] [D loss: 1.657146, acc: 52%] [G loss: 2.479326]\n",
      "[Epoch 62/200] [Batch 140/340] [D loss: 1.724238, acc: 51%] [G loss: 1.993008]\n",
      "[Epoch 62/200] [Batch 160/340] [D loss: 1.644420, acc: 56%] [G loss: 2.361387]\n",
      "[Epoch 62/200] [Batch 180/340] [D loss: 1.697015, acc: 54%] [G loss: 2.077783]\n",
      "[Epoch 62/200] [Batch 200/340] [D loss: 1.696000, acc: 53%] [G loss: 2.051469]\n",
      "[Epoch 62/200] [Batch 220/340] [D loss: 1.711865, acc: 53%] [G loss: 2.156223]\n",
      "[Epoch 62/200] [Batch 240/340] [D loss: 1.701393, acc: 51%] [G loss: 2.242453]\n",
      "[Epoch 62/200] [Batch 260/340] [D loss: 1.689716, acc: 53%] [G loss: 2.364254]\n",
      "[Epoch 62/200] [Batch 280/340] [D loss: 1.716280, acc: 54%] [G loss: 2.135934]\n",
      "[Epoch 62/200] [Batch 300/340] [D loss: 1.674525, acc: 53%] [G loss: 2.047668]\n",
      "[Epoch 62/200] [Batch 320/340] [D loss: 1.699727, acc: 53%] [G loss: 2.158841]\n",
      "[Epoch 63/200] [Batch 0/340] [D loss: 1.668567, acc: 55%] [G loss: 2.074193]\n",
      "[Epoch 63/200] [Batch 20/340] [D loss: 1.714398, acc: 52%] [G loss: 1.952091]\n",
      "[Epoch 63/200] [Batch 40/340] [D loss: 1.693215, acc: 53%] [G loss: 2.397818]\n",
      "[Epoch 63/200] [Batch 60/340] [D loss: 1.807178, acc: 50%] [G loss: 1.885178]\n",
      "[Epoch 63/200] [Batch 80/340] [D loss: 1.678004, acc: 54%] [G loss: 1.992896]\n",
      "[Epoch 63/200] [Batch 100/340] [D loss: 1.769258, acc: 51%] [G loss: 2.160797]\n",
      "[Epoch 63/200] [Batch 120/340] [D loss: 1.718735, acc: 54%] [G loss: 2.210222]\n",
      "[Epoch 63/200] [Batch 140/340] [D loss: 1.674092, acc: 53%] [G loss: 2.023906]\n",
      "[Epoch 63/200] [Batch 160/340] [D loss: 1.696361, acc: 54%] [G loss: 2.082558]\n",
      "[Epoch 63/200] [Batch 180/340] [D loss: 1.694748, acc: 50%] [G loss: 2.204910]\n",
      "[Epoch 63/200] [Batch 200/340] [D loss: 1.747951, acc: 58%] [G loss: 2.163502]\n",
      "[Epoch 63/200] [Batch 220/340] [D loss: 1.666397, acc: 56%] [G loss: 2.351532]\n",
      "[Epoch 63/200] [Batch 240/340] [D loss: 1.672285, acc: 54%] [G loss: 2.268935]\n",
      "[Epoch 63/200] [Batch 260/340] [D loss: 1.718809, acc: 54%] [G loss: 2.214712]\n",
      "[Epoch 63/200] [Batch 280/340] [D loss: 1.728699, acc: 51%] [G loss: 2.028345]\n",
      "[Epoch 63/200] [Batch 300/340] [D loss: 1.693105, acc: 52%] [G loss: 2.088209]\n",
      "[Epoch 63/200] [Batch 320/340] [D loss: 1.652557, acc: 55%] [G loss: 2.097596]\n",
      "[Epoch 64/200] [Batch 0/340] [D loss: 1.737021, acc: 53%] [G loss: 2.195668]\n",
      "[Epoch 64/200] [Batch 20/340] [D loss: 1.689496, acc: 50%] [G loss: 2.392039]\n",
      "[Epoch 64/200] [Batch 40/340] [D loss: 1.676283, acc: 54%] [G loss: 2.036721]\n",
      "[Epoch 64/200] [Batch 60/340] [D loss: 1.713541, acc: 58%] [G loss: 2.212678]\n",
      "[Epoch 64/200] [Batch 80/340] [D loss: 1.653398, acc: 55%] [G loss: 2.197099]\n",
      "[Epoch 64/200] [Batch 100/340] [D loss: 1.719154, acc: 52%] [G loss: 2.152580]\n",
      "[Epoch 64/200] [Batch 120/340] [D loss: 1.679289, acc: 55%] [G loss: 2.138256]\n",
      "[Epoch 64/200] [Batch 140/340] [D loss: 1.665120, acc: 56%] [G loss: 2.422695]\n",
      "[Epoch 64/200] [Batch 160/340] [D loss: 1.712744, acc: 50%] [G loss: 2.428773]\n",
      "[Epoch 64/200] [Batch 180/340] [D loss: 1.769950, acc: 56%] [G loss: 2.065748]\n",
      "[Epoch 64/200] [Batch 200/340] [D loss: 1.767158, acc: 51%] [G loss: 2.048809]\n",
      "[Epoch 64/200] [Batch 220/340] [D loss: 1.697947, acc: 56%] [G loss: 2.142941]\n",
      "[Epoch 64/200] [Batch 240/340] [D loss: 1.670441, acc: 53%] [G loss: 2.237868]\n",
      "[Epoch 64/200] [Batch 260/340] [D loss: 1.715202, acc: 56%] [G loss: 1.906686]\n",
      "[Epoch 64/200] [Batch 280/340] [D loss: 1.714712, acc: 53%] [G loss: 2.407677]\n",
      "[Epoch 64/200] [Batch 300/340] [D loss: 1.708014, acc: 50%] [G loss: 2.252366]\n",
      "[Epoch 64/200] [Batch 320/340] [D loss: 1.707517, acc: 51%] [G loss: 1.939803]\n",
      "[Epoch 65/200] [Batch 0/340] [D loss: 1.730250, acc: 54%] [G loss: 2.067777]\n",
      "[Epoch 65/200] [Batch 20/340] [D loss: 1.668443, acc: 57%] [G loss: 2.241439]\n",
      "[Epoch 65/200] [Batch 40/340] [D loss: 1.710789, acc: 55%] [G loss: 2.413173]\n",
      "[Epoch 65/200] [Batch 60/340] [D loss: 1.730728, acc: 53%] [G loss: 2.228584]\n",
      "[Epoch 65/200] [Batch 80/340] [D loss: 1.747290, acc: 51%] [G loss: 1.873379]\n",
      "[Epoch 65/200] [Batch 100/340] [D loss: 1.679734, acc: 52%] [G loss: 2.408298]\n",
      "[Epoch 65/200] [Batch 120/340] [D loss: 1.763457, acc: 49%] [G loss: 2.503211]\n",
      "[Epoch 65/200] [Batch 140/340] [D loss: 1.712475, acc: 51%] [G loss: 2.292971]\n",
      "[Epoch 65/200] [Batch 160/340] [D loss: 1.670752, acc: 54%] [G loss: 2.318658]\n",
      "[Epoch 65/200] [Batch 180/340] [D loss: 1.754853, acc: 53%] [G loss: 2.059214]\n",
      "[Epoch 65/200] [Batch 200/340] [D loss: 1.686706, acc: 51%] [G loss: 2.194006]\n",
      "[Epoch 65/200] [Batch 220/340] [D loss: 1.689563, acc: 56%] [G loss: 2.438419]\n",
      "[Epoch 65/200] [Batch 240/340] [D loss: 1.665097, acc: 54%] [G loss: 2.065912]\n",
      "[Epoch 65/200] [Batch 260/340] [D loss: 1.700031, acc: 51%] [G loss: 1.982818]\n",
      "[Epoch 65/200] [Batch 280/340] [D loss: 1.757269, acc: 52%] [G loss: 1.966478]\n",
      "[Epoch 65/200] [Batch 300/340] [D loss: 1.767733, acc: 51%] [G loss: 2.344615]\n",
      "[Epoch 65/200] [Batch 320/340] [D loss: 1.656961, acc: 56%] [G loss: 2.258559]\n",
      "[Epoch 66/200] [Batch 0/340] [D loss: 1.678447, acc: 53%] [G loss: 2.236486]\n",
      "[Epoch 66/200] [Batch 20/340] [D loss: 1.660521, acc: 52%] [G loss: 2.194545]\n",
      "[Epoch 66/200] [Batch 40/340] [D loss: 1.680482, acc: 58%] [G loss: 2.356966]\n",
      "[Epoch 66/200] [Batch 60/340] [D loss: 1.729502, acc: 52%] [G loss: 2.186762]\n",
      "[Epoch 66/200] [Batch 80/340] [D loss: 1.690257, acc: 56%] [G loss: 1.963651]\n",
      "[Epoch 66/200] [Batch 100/340] [D loss: 1.739701, acc: 53%] [G loss: 2.059380]\n",
      "[Epoch 66/200] [Batch 120/340] [D loss: 1.706515, acc: 53%] [G loss: 2.195831]\n",
      "[Epoch 66/200] [Batch 140/340] [D loss: 1.672491, acc: 54%] [G loss: 2.032434]\n",
      "[Epoch 66/200] [Batch 160/340] [D loss: 1.681367, acc: 56%] [G loss: 2.023499]\n",
      "[Epoch 66/200] [Batch 180/340] [D loss: 1.673732, acc: 46%] [G loss: 2.109612]\n",
      "[Epoch 66/200] [Batch 200/340] [D loss: 1.681931, acc: 54%] [G loss: 2.176265]\n",
      "[Epoch 66/200] [Batch 220/340] [D loss: 1.695483, acc: 56%] [G loss: 2.018637]\n",
      "[Epoch 66/200] [Batch 240/340] [D loss: 1.700394, acc: 56%] [G loss: 2.016403]\n",
      "[Epoch 66/200] [Batch 260/340] [D loss: 1.730935, acc: 52%] [G loss: 2.297438]\n",
      "[Epoch 66/200] [Batch 280/340] [D loss: 1.746984, acc: 52%] [G loss: 2.011688]\n",
      "[Epoch 66/200] [Batch 300/340] [D loss: 1.682414, acc: 51%] [G loss: 2.442609]\n",
      "[Epoch 66/200] [Batch 320/340] [D loss: 1.688947, acc: 50%] [G loss: 2.137404]\n",
      "[Epoch 67/200] [Batch 0/340] [D loss: 1.684829, acc: 53%] [G loss: 2.151321]\n",
      "[Epoch 67/200] [Batch 20/340] [D loss: 1.733930, acc: 51%] [G loss: 2.062956]\n",
      "[Epoch 67/200] [Batch 40/340] [D loss: 1.651784, acc: 55%] [G loss: 2.187119]\n",
      "[Epoch 67/200] [Batch 60/340] [D loss: 1.697354, acc: 55%] [G loss: 2.220906]\n",
      "[Epoch 67/200] [Batch 80/340] [D loss: 1.692797, acc: 55%] [G loss: 1.979385]\n",
      "[Epoch 67/200] [Batch 100/340] [D loss: 1.678029, acc: 51%] [G loss: 2.348554]\n",
      "[Epoch 67/200] [Batch 120/340] [D loss: 1.648700, acc: 56%] [G loss: 2.027946]\n",
      "[Epoch 67/200] [Batch 140/340] [D loss: 1.722136, acc: 53%] [G loss: 2.253528]\n",
      "[Epoch 67/200] [Batch 160/340] [D loss: 1.704436, acc: 56%] [G loss: 2.359658]\n",
      "[Epoch 67/200] [Batch 180/340] [D loss: 1.657512, acc: 53%] [G loss: 2.483663]\n",
      "[Epoch 67/200] [Batch 200/340] [D loss: 1.701862, acc: 56%] [G loss: 2.100993]\n",
      "[Epoch 67/200] [Batch 220/340] [D loss: 1.737732, acc: 54%] [G loss: 2.358859]\n",
      "[Epoch 67/200] [Batch 240/340] [D loss: 1.723918, acc: 54%] [G loss: 2.210959]\n",
      "[Epoch 67/200] [Batch 260/340] [D loss: 1.699080, acc: 55%] [G loss: 1.978157]\n",
      "[Epoch 67/200] [Batch 280/340] [D loss: 1.727831, acc: 54%] [G loss: 2.180488]\n",
      "[Epoch 67/200] [Batch 300/340] [D loss: 1.732371, acc: 52%] [G loss: 2.318744]\n",
      "[Epoch 67/200] [Batch 320/340] [D loss: 1.667971, acc: 56%] [G loss: 2.142648]\n",
      "[Epoch 68/200] [Batch 0/340] [D loss: 1.685227, acc: 56%] [G loss: 2.264334]\n",
      "[Epoch 68/200] [Batch 20/340] [D loss: 1.647905, acc: 53%] [G loss: 2.216905]\n",
      "[Epoch 68/200] [Batch 40/340] [D loss: 1.702030, acc: 57%] [G loss: 2.095912]\n",
      "[Epoch 68/200] [Batch 60/340] [D loss: 1.738058, acc: 55%] [G loss: 2.369938]\n",
      "[Epoch 68/200] [Batch 80/340] [D loss: 1.655010, acc: 56%] [G loss: 1.989990]\n",
      "[Epoch 68/200] [Batch 100/340] [D loss: 1.684176, acc: 53%] [G loss: 2.080887]\n",
      "[Epoch 68/200] [Batch 120/340] [D loss: 1.675414, acc: 53%] [G loss: 2.222506]\n",
      "[Epoch 68/200] [Batch 140/340] [D loss: 1.712274, acc: 55%] [G loss: 2.041999]\n",
      "[Epoch 68/200] [Batch 160/340] [D loss: 1.731251, acc: 51%] [G loss: 2.357555]\n",
      "[Epoch 68/200] [Batch 180/340] [D loss: 1.687024, acc: 57%] [G loss: 2.166469]\n",
      "[Epoch 68/200] [Batch 200/340] [D loss: 1.761331, acc: 54%] [G loss: 2.228971]\n",
      "[Epoch 68/200] [Batch 220/340] [D loss: 1.741037, acc: 53%] [G loss: 2.259270]\n",
      "[Epoch 68/200] [Batch 240/340] [D loss: 1.756160, acc: 53%] [G loss: 2.114332]\n",
      "[Epoch 68/200] [Batch 260/340] [D loss: 1.698461, acc: 53%] [G loss: 2.189147]\n",
      "[Epoch 68/200] [Batch 280/340] [D loss: 1.652020, acc: 53%] [G loss: 2.478299]\n",
      "[Epoch 68/200] [Batch 300/340] [D loss: 1.721910, acc: 58%] [G loss: 1.989126]\n",
      "[Epoch 68/200] [Batch 320/340] [D loss: 1.699737, acc: 57%] [G loss: 2.132658]\n",
      "[Epoch 69/200] [Batch 0/340] [D loss: 1.726848, acc: 56%] [G loss: 1.844982]\n",
      "[Epoch 69/200] [Batch 20/340] [D loss: 1.658908, acc: 56%] [G loss: 2.274309]\n",
      "[Epoch 69/200] [Batch 40/340] [D loss: 1.702965, acc: 56%] [G loss: 2.309461]\n",
      "[Epoch 69/200] [Batch 60/340] [D loss: 1.719369, acc: 54%] [G loss: 2.070897]\n",
      "[Epoch 69/200] [Batch 80/340] [D loss: 1.668473, acc: 55%] [G loss: 2.171062]\n",
      "[Epoch 69/200] [Batch 100/340] [D loss: 1.742558, acc: 55%] [G loss: 2.245528]\n",
      "[Epoch 69/200] [Batch 120/340] [D loss: 1.724687, acc: 56%] [G loss: 1.942798]\n",
      "[Epoch 69/200] [Batch 140/340] [D loss: 1.730728, acc: 54%] [G loss: 2.347680]\n",
      "[Epoch 69/200] [Batch 160/340] [D loss: 1.701554, acc: 52%] [G loss: 2.344116]\n",
      "[Epoch 69/200] [Batch 180/340] [D loss: 1.677253, acc: 57%] [G loss: 2.266644]\n",
      "[Epoch 69/200] [Batch 200/340] [D loss: 1.691109, acc: 52%] [G loss: 2.150622]\n",
      "[Epoch 69/200] [Batch 220/340] [D loss: 1.652468, acc: 55%] [G loss: 2.430360]\n",
      "[Epoch 69/200] [Batch 240/340] [D loss: 1.656492, acc: 57%] [G loss: 1.982402]\n",
      "[Epoch 69/200] [Batch 260/340] [D loss: 1.709381, acc: 55%] [G loss: 2.010695]\n",
      "[Epoch 69/200] [Batch 280/340] [D loss: 1.680025, acc: 55%] [G loss: 1.960529]\n",
      "[Epoch 69/200] [Batch 300/340] [D loss: 1.696675, acc: 56%] [G loss: 2.165219]\n",
      "[Epoch 69/200] [Batch 320/340] [D loss: 1.705694, acc: 50%] [G loss: 2.320523]\n",
      "[Epoch 70/200] [Batch 0/340] [D loss: 1.680275, acc: 52%] [G loss: 2.255454]\n",
      "[Epoch 70/200] [Batch 20/340] [D loss: 1.749414, acc: 55%] [G loss: 1.967597]\n",
      "[Epoch 70/200] [Batch 40/340] [D loss: 1.703184, acc: 50%] [G loss: 2.317132]\n",
      "[Epoch 70/200] [Batch 60/340] [D loss: 1.692096, acc: 52%] [G loss: 2.456036]\n",
      "[Epoch 70/200] [Batch 80/340] [D loss: 1.740822, acc: 53%] [G loss: 2.031965]\n",
      "[Epoch 70/200] [Batch 100/340] [D loss: 1.644205, acc: 59%] [G loss: 2.306933]\n",
      "[Epoch 70/200] [Batch 120/340] [D loss: 1.685426, acc: 58%] [G loss: 2.148791]\n",
      "[Epoch 70/200] [Batch 140/340] [D loss: 1.726646, acc: 54%] [G loss: 2.197501]\n",
      "[Epoch 70/200] [Batch 160/340] [D loss: 1.730698, acc: 57%] [G loss: 1.991875]\n",
      "[Epoch 70/200] [Batch 180/340] [D loss: 1.650650, acc: 54%] [G loss: 2.173872]\n",
      "[Epoch 70/200] [Batch 200/340] [D loss: 1.685325, acc: 57%] [G loss: 2.215739]\n",
      "[Epoch 70/200] [Batch 220/340] [D loss: 1.722463, acc: 53%] [G loss: 2.131517]\n",
      "[Epoch 70/200] [Batch 240/340] [D loss: 1.724067, acc: 54%] [G loss: 2.007079]\n",
      "[Epoch 70/200] [Batch 260/340] [D loss: 1.675303, acc: 57%] [G loss: 2.279362]\n",
      "[Epoch 70/200] [Batch 280/340] [D loss: 1.697174, acc: 55%] [G loss: 2.146539]\n",
      "[Epoch 70/200] [Batch 300/340] [D loss: 1.668071, acc: 57%] [G loss: 2.012165]\n",
      "[Epoch 70/200] [Batch 320/340] [D loss: 1.726136, acc: 55%] [G loss: 1.999315]\n",
      "[Epoch 71/200] [Batch 0/340] [D loss: 1.687753, acc: 58%] [G loss: 2.042507]\n",
      "[Epoch 71/200] [Batch 20/340] [D loss: 1.688719, acc: 56%] [G loss: 2.154999]\n",
      "[Epoch 71/200] [Batch 40/340] [D loss: 1.723940, acc: 55%] [G loss: 2.368242]\n",
      "[Epoch 71/200] [Batch 60/340] [D loss: 1.658190, acc: 56%] [G loss: 2.208514]\n",
      "[Epoch 71/200] [Batch 80/340] [D loss: 1.699618, acc: 57%] [G loss: 2.004930]\n",
      "[Epoch 71/200] [Batch 100/340] [D loss: 1.692357, acc: 58%] [G loss: 2.301000]\n",
      "[Epoch 71/200] [Batch 120/340] [D loss: 1.782461, acc: 57%] [G loss: 1.976501]\n",
      "[Epoch 71/200] [Batch 140/340] [D loss: 1.679123, acc: 52%] [G loss: 2.098719]\n",
      "[Epoch 71/200] [Batch 160/340] [D loss: 1.686176, acc: 52%] [G loss: 2.132877]\n",
      "[Epoch 71/200] [Batch 180/340] [D loss: 1.712203, acc: 54%] [G loss: 2.535534]\n",
      "[Epoch 71/200] [Batch 200/340] [D loss: 1.706038, acc: 53%] [G loss: 2.252823]\n",
      "[Epoch 71/200] [Batch 220/340] [D loss: 1.735312, acc: 53%] [G loss: 2.239260]\n",
      "[Epoch 71/200] [Batch 240/340] [D loss: 1.692223, acc: 56%] [G loss: 2.126610]\n",
      "[Epoch 71/200] [Batch 260/340] [D loss: 1.655098, acc: 54%] [G loss: 2.204731]\n",
      "[Epoch 71/200] [Batch 280/340] [D loss: 1.695188, acc: 56%] [G loss: 2.280286]\n",
      "[Epoch 71/200] [Batch 300/340] [D loss: 1.655402, acc: 55%] [G loss: 2.303944]\n",
      "[Epoch 71/200] [Batch 320/340] [D loss: 1.705073, acc: 52%] [G loss: 2.246604]\n",
      "[Epoch 72/200] [Batch 0/340] [D loss: 1.681825, acc: 55%] [G loss: 2.000458]\n",
      "[Epoch 72/200] [Batch 20/340] [D loss: 1.663893, acc: 52%] [G loss: 2.218094]\n",
      "[Epoch 72/200] [Batch 40/340] [D loss: 1.714309, acc: 58%] [G loss: 2.075790]\n",
      "[Epoch 72/200] [Batch 60/340] [D loss: 1.774860, acc: 53%] [G loss: 2.521009]\n",
      "[Epoch 72/200] [Batch 80/340] [D loss: 1.680348, acc: 55%] [G loss: 2.624200]\n",
      "[Epoch 72/200] [Batch 100/340] [D loss: 1.698941, acc: 56%] [G loss: 2.521291]\n",
      "[Epoch 72/200] [Batch 120/340] [D loss: 1.672805, acc: 56%] [G loss: 2.325662]\n",
      "[Epoch 72/200] [Batch 140/340] [D loss: 1.704181, acc: 52%] [G loss: 1.882915]\n",
      "[Epoch 72/200] [Batch 160/340] [D loss: 1.665101, acc: 55%] [G loss: 2.434156]\n",
      "[Epoch 72/200] [Batch 180/340] [D loss: 1.718119, acc: 53%] [G loss: 2.251758]\n",
      "[Epoch 72/200] [Batch 200/340] [D loss: 1.750108, acc: 59%] [G loss: 2.074971]\n",
      "[Epoch 72/200] [Batch 220/340] [D loss: 1.642299, acc: 56%] [G loss: 2.120540]\n",
      "[Epoch 72/200] [Batch 240/340] [D loss: 1.684146, acc: 53%] [G loss: 1.991543]\n",
      "[Epoch 72/200] [Batch 260/340] [D loss: 1.743695, acc: 55%] [G loss: 1.893394]\n",
      "[Epoch 72/200] [Batch 280/340] [D loss: 1.667916, acc: 58%] [G loss: 2.073691]\n",
      "[Epoch 72/200] [Batch 300/340] [D loss: 1.713396, acc: 56%] [G loss: 2.466134]\n",
      "[Epoch 72/200] [Batch 320/340] [D loss: 1.748057, acc: 56%] [G loss: 1.954554]\n",
      "[Epoch 73/200] [Batch 0/340] [D loss: 1.662732, acc: 56%] [G loss: 2.439972]\n",
      "[Epoch 73/200] [Batch 20/340] [D loss: 1.695478, acc: 57%] [G loss: 2.504364]\n",
      "[Epoch 73/200] [Batch 40/340] [D loss: 1.680033, acc: 57%] [G loss: 2.068745]\n",
      "[Epoch 73/200] [Batch 60/340] [D loss: 1.710006, acc: 55%] [G loss: 2.135115]\n",
      "[Epoch 73/200] [Batch 80/340] [D loss: 1.664524, acc: 57%] [G loss: 2.009908]\n",
      "[Epoch 73/200] [Batch 100/340] [D loss: 1.659420, acc: 54%] [G loss: 2.103333]\n",
      "[Epoch 73/200] [Batch 120/340] [D loss: 1.719859, acc: 57%] [G loss: 2.234205]\n",
      "[Epoch 73/200] [Batch 140/340] [D loss: 1.782593, acc: 53%] [G loss: 2.475735]\n",
      "[Epoch 73/200] [Batch 160/340] [D loss: 1.641539, acc: 60%] [G loss: 2.125942]\n",
      "[Epoch 73/200] [Batch 180/340] [D loss: 1.677484, acc: 55%] [G loss: 2.011535]\n",
      "[Epoch 73/200] [Batch 200/340] [D loss: 1.625781, acc: 55%] [G loss: 2.188482]\n",
      "[Epoch 73/200] [Batch 220/340] [D loss: 1.687811, acc: 58%] [G loss: 1.819679]\n",
      "[Epoch 73/200] [Batch 240/340] [D loss: 1.606667, acc: 60%] [G loss: 2.250106]\n",
      "[Epoch 73/200] [Batch 260/340] [D loss: 1.640942, acc: 61%] [G loss: 2.005462]\n",
      "[Epoch 73/200] [Batch 280/340] [D loss: 1.660114, acc: 54%] [G loss: 2.015765]\n",
      "[Epoch 73/200] [Batch 300/340] [D loss: 1.708694, acc: 62%] [G loss: 2.068743]\n",
      "[Epoch 73/200] [Batch 320/340] [D loss: 1.613765, acc: 60%] [G loss: 2.060412]\n",
      "[Epoch 74/200] [Batch 0/340] [D loss: 1.653516, acc: 54%] [G loss: 2.223992]\n",
      "[Epoch 74/200] [Batch 20/340] [D loss: 1.701302, acc: 53%] [G loss: 1.977793]\n",
      "[Epoch 74/200] [Batch 40/340] [D loss: 1.763723, acc: 53%] [G loss: 1.961242]\n",
      "[Epoch 74/200] [Batch 60/340] [D loss: 1.650419, acc: 60%] [G loss: 2.035569]\n",
      "[Epoch 74/200] [Batch 80/340] [D loss: 1.715330, acc: 58%] [G loss: 1.957762]\n",
      "[Epoch 74/200] [Batch 100/340] [D loss: 1.686088, acc: 52%] [G loss: 2.163997]\n",
      "[Epoch 74/200] [Batch 120/340] [D loss: 1.696188, acc: 56%] [G loss: 2.141856]\n",
      "[Epoch 74/200] [Batch 140/340] [D loss: 1.697291, acc: 54%] [G loss: 2.008049]\n",
      "[Epoch 74/200] [Batch 160/340] [D loss: 1.699010, acc: 56%] [G loss: 2.344944]\n",
      "[Epoch 74/200] [Batch 180/340] [D loss: 1.701601, acc: 53%] [G loss: 2.041627]\n",
      "[Epoch 74/200] [Batch 200/340] [D loss: 1.667755, acc: 57%] [G loss: 2.190183]\n",
      "[Epoch 74/200] [Batch 220/340] [D loss: 1.708102, acc: 55%] [G loss: 2.199305]\n",
      "[Epoch 74/200] [Batch 240/340] [D loss: 1.702483, acc: 57%] [G loss: 1.946890]\n",
      "[Epoch 74/200] [Batch 260/340] [D loss: 1.720581, acc: 55%] [G loss: 2.570438]\n",
      "[Epoch 74/200] [Batch 280/340] [D loss: 1.679598, acc: 55%] [G loss: 2.268183]\n",
      "[Epoch 74/200] [Batch 300/340] [D loss: 1.713409, acc: 56%] [G loss: 2.056503]\n",
      "[Epoch 74/200] [Batch 320/340] [D loss: 1.687882, acc: 59%] [G loss: 2.242023]\n",
      "[Epoch 75/200] [Batch 0/340] [D loss: 1.665339, acc: 55%] [G loss: 2.069308]\n",
      "[Epoch 75/200] [Batch 20/340] [D loss: 1.677025, acc: 56%] [G loss: 1.977005]\n",
      "[Epoch 75/200] [Batch 40/340] [D loss: 1.696341, acc: 57%] [G loss: 1.888755]\n",
      "[Epoch 75/200] [Batch 60/340] [D loss: 1.645296, acc: 59%] [G loss: 2.016561]\n",
      "[Epoch 75/200] [Batch 80/340] [D loss: 1.670382, acc: 59%] [G loss: 2.017113]\n",
      "[Epoch 75/200] [Batch 100/340] [D loss: 1.685423, acc: 55%] [G loss: 2.407224]\n",
      "[Epoch 75/200] [Batch 120/340] [D loss: 1.679434, acc: 58%] [G loss: 2.256251]\n",
      "[Epoch 75/200] [Batch 140/340] [D loss: 1.655167, acc: 58%] [G loss: 2.428719]\n",
      "[Epoch 75/200] [Batch 160/340] [D loss: 1.697417, acc: 52%] [G loss: 2.054806]\n",
      "[Epoch 75/200] [Batch 180/340] [D loss: 1.714400, acc: 56%] [G loss: 1.946163]\n",
      "[Epoch 75/200] [Batch 200/340] [D loss: 1.719861, acc: 56%] [G loss: 2.218218]\n",
      "[Epoch 75/200] [Batch 220/340] [D loss: 1.706677, acc: 56%] [G loss: 2.295869]\n",
      "[Epoch 75/200] [Batch 240/340] [D loss: 1.700088, acc: 55%] [G loss: 2.026923]\n",
      "[Epoch 75/200] [Batch 260/340] [D loss: 1.655350, acc: 53%] [G loss: 2.117375]\n",
      "[Epoch 75/200] [Batch 280/340] [D loss: 1.692209, acc: 56%] [G loss: 2.429831]\n",
      "[Epoch 75/200] [Batch 300/340] [D loss: 1.632777, acc: 58%] [G loss: 1.962475]\n",
      "[Epoch 75/200] [Batch 320/340] [D loss: 1.734045, acc: 56%] [G loss: 2.090651]\n",
      "[Epoch 76/200] [Batch 0/340] [D loss: 1.643100, acc: 61%] [G loss: 2.186498]\n",
      "[Epoch 76/200] [Batch 20/340] [D loss: 1.671109, acc: 58%] [G loss: 2.002329]\n",
      "[Epoch 76/200] [Batch 40/340] [D loss: 1.670409, acc: 60%] [G loss: 2.060277]\n",
      "[Epoch 76/200] [Batch 60/340] [D loss: 1.679026, acc: 58%] [G loss: 2.094875]\n",
      "[Epoch 76/200] [Batch 80/340] [D loss: 1.689652, acc: 53%] [G loss: 1.918119]\n",
      "[Epoch 76/200] [Batch 100/340] [D loss: 1.702302, acc: 56%] [G loss: 2.223876]\n",
      "[Epoch 76/200] [Batch 120/340] [D loss: 1.665292, acc: 57%] [G loss: 1.936795]\n",
      "[Epoch 76/200] [Batch 140/340] [D loss: 1.712856, acc: 53%] [G loss: 2.152158]\n",
      "[Epoch 76/200] [Batch 160/340] [D loss: 1.696768, acc: 55%] [G loss: 1.966640]\n",
      "[Epoch 76/200] [Batch 180/340] [D loss: 1.763964, acc: 56%] [G loss: 2.390384]\n",
      "[Epoch 76/200] [Batch 200/340] [D loss: 1.672155, acc: 53%] [G loss: 2.011853]\n",
      "[Epoch 76/200] [Batch 220/340] [D loss: 1.672292, acc: 57%] [G loss: 2.052650]\n",
      "[Epoch 76/200] [Batch 240/340] [D loss: 1.667781, acc: 58%] [G loss: 2.445468]\n",
      "[Epoch 76/200] [Batch 260/340] [D loss: 1.702457, acc: 57%] [G loss: 2.226495]\n",
      "[Epoch 76/200] [Batch 280/340] [D loss: 1.619185, acc: 63%] [G loss: 2.214063]\n",
      "[Epoch 76/200] [Batch 300/340] [D loss: 1.722208, acc: 57%] [G loss: 1.865455]\n",
      "[Epoch 76/200] [Batch 320/340] [D loss: 1.715111, acc: 58%] [G loss: 2.180101]\n",
      "[Epoch 77/200] [Batch 0/340] [D loss: 1.703079, acc: 55%] [G loss: 2.361069]\n",
      "[Epoch 77/200] [Batch 20/340] [D loss: 1.599021, acc: 56%] [G loss: 2.228492]\n",
      "[Epoch 77/200] [Batch 40/340] [D loss: 1.655752, acc: 57%] [G loss: 2.124521]\n",
      "[Epoch 77/200] [Batch 60/340] [D loss: 1.737677, acc: 56%] [G loss: 2.049190]\n",
      "[Epoch 77/200] [Batch 80/340] [D loss: 1.701444, acc: 57%] [G loss: 2.194429]\n",
      "[Epoch 77/200] [Batch 100/340] [D loss: 1.715895, acc: 59%] [G loss: 1.886788]\n",
      "[Epoch 77/200] [Batch 120/340] [D loss: 1.698169, acc: 56%] [G loss: 1.827426]\n",
      "[Epoch 77/200] [Batch 140/340] [D loss: 1.687965, acc: 54%] [G loss: 2.374596]\n",
      "[Epoch 77/200] [Batch 160/340] [D loss: 1.685271, acc: 57%] [G loss: 2.035039]\n",
      "[Epoch 77/200] [Batch 180/340] [D loss: 1.718137, acc: 57%] [G loss: 2.143949]\n",
      "[Epoch 77/200] [Batch 200/340] [D loss: 1.666989, acc: 59%] [G loss: 2.153962]\n",
      "[Epoch 77/200] [Batch 220/340] [D loss: 1.704273, acc: 53%] [G loss: 2.147151]\n",
      "[Epoch 77/200] [Batch 240/340] [D loss: 1.705068, acc: 59%] [G loss: 2.001439]\n",
      "[Epoch 77/200] [Batch 260/340] [D loss: 1.709719, acc: 56%] [G loss: 2.180811]\n",
      "[Epoch 77/200] [Batch 280/340] [D loss: 1.686808, acc: 57%] [G loss: 2.190651]\n",
      "[Epoch 77/200] [Batch 300/340] [D loss: 1.680311, acc: 57%] [G loss: 2.149649]\n",
      "[Epoch 77/200] [Batch 320/340] [D loss: 1.685917, acc: 58%] [G loss: 1.965515]\n",
      "[Epoch 78/200] [Batch 0/340] [D loss: 1.698634, acc: 58%] [G loss: 1.997280]\n",
      "[Epoch 78/200] [Batch 20/340] [D loss: 1.726229, acc: 60%] [G loss: 2.207040]\n",
      "[Epoch 78/200] [Batch 40/340] [D loss: 1.692226, acc: 57%] [G loss: 2.218609]\n",
      "[Epoch 78/200] [Batch 60/340] [D loss: 1.656343, acc: 59%] [G loss: 2.085736]\n",
      "[Epoch 78/200] [Batch 80/340] [D loss: 1.690585, acc: 60%] [G loss: 2.278228]\n",
      "[Epoch 78/200] [Batch 100/340] [D loss: 1.678343, acc: 59%] [G loss: 2.057822]\n",
      "[Epoch 78/200] [Batch 120/340] [D loss: 1.768700, acc: 55%] [G loss: 2.099003]\n",
      "[Epoch 78/200] [Batch 140/340] [D loss: 1.684023, acc: 61%] [G loss: 2.151916]\n",
      "[Epoch 78/200] [Batch 160/340] [D loss: 1.738570, acc: 57%] [G loss: 2.016076]\n",
      "[Epoch 78/200] [Batch 180/340] [D loss: 1.670050, acc: 56%] [G loss: 2.107831]\n",
      "[Epoch 78/200] [Batch 200/340] [D loss: 1.632170, acc: 58%] [G loss: 2.220260]\n",
      "[Epoch 78/200] [Batch 220/340] [D loss: 1.686082, acc: 58%] [G loss: 2.371462]\n",
      "[Epoch 78/200] [Batch 240/340] [D loss: 1.720366, acc: 52%] [G loss: 1.897103]\n",
      "[Epoch 78/200] [Batch 260/340] [D loss: 1.742164, acc: 57%] [G loss: 2.147944]\n",
      "[Epoch 78/200] [Batch 280/340] [D loss: 1.655071, acc: 59%] [G loss: 2.236473]\n",
      "[Epoch 78/200] [Batch 300/340] [D loss: 1.641718, acc: 59%] [G loss: 2.207628]\n",
      "[Epoch 78/200] [Batch 320/340] [D loss: 1.654348, acc: 61%] [G loss: 2.297722]\n",
      "[Epoch 79/200] [Batch 0/340] [D loss: 1.674798, acc: 59%] [G loss: 2.062037]\n",
      "[Epoch 79/200] [Batch 20/340] [D loss: 1.581509, acc: 58%] [G loss: 2.045974]\n",
      "[Epoch 79/200] [Batch 40/340] [D loss: 1.697661, acc: 59%] [G loss: 1.970559]\n",
      "[Epoch 79/200] [Batch 60/340] [D loss: 1.690868, acc: 56%] [G loss: 2.343636]\n",
      "[Epoch 79/200] [Batch 80/340] [D loss: 1.670085, acc: 57%] [G loss: 2.219893]\n",
      "[Epoch 79/200] [Batch 100/340] [D loss: 1.696425, acc: 57%] [G loss: 2.258414]\n",
      "[Epoch 79/200] [Batch 120/340] [D loss: 1.646779, acc: 60%] [G loss: 2.207283]\n",
      "[Epoch 79/200] [Batch 140/340] [D loss: 1.751666, acc: 57%] [G loss: 2.266587]\n",
      "[Epoch 79/200] [Batch 160/340] [D loss: 1.640666, acc: 59%] [G loss: 2.322785]\n",
      "[Epoch 79/200] [Batch 180/340] [D loss: 1.690027, acc: 54%] [G loss: 2.215337]\n",
      "[Epoch 79/200] [Batch 200/340] [D loss: 1.672928, acc: 57%] [G loss: 2.238783]\n",
      "[Epoch 79/200] [Batch 220/340] [D loss: 1.605040, acc: 58%] [G loss: 2.429116]\n",
      "[Epoch 79/200] [Batch 240/340] [D loss: 1.667654, acc: 55%] [G loss: 2.360816]\n",
      "[Epoch 79/200] [Batch 260/340] [D loss: 1.670447, acc: 57%] [G loss: 2.224857]\n",
      "[Epoch 79/200] [Batch 280/340] [D loss: 1.651729, acc: 58%] [G loss: 2.327989]\n",
      "[Epoch 79/200] [Batch 300/340] [D loss: 1.620381, acc: 56%] [G loss: 2.212147]\n",
      "[Epoch 79/200] [Batch 320/340] [D loss: 1.650727, acc: 58%] [G loss: 2.316299]\n",
      "[Epoch 80/200] [Batch 0/340] [D loss: 1.725476, acc: 57%] [G loss: 2.274840]\n",
      "[Epoch 80/200] [Batch 20/340] [D loss: 1.727354, acc: 55%] [G loss: 1.989392]\n",
      "[Epoch 80/200] [Batch 40/340] [D loss: 1.624599, acc: 60%] [G loss: 2.210078]\n",
      "[Epoch 80/200] [Batch 60/340] [D loss: 1.698521, acc: 56%] [G loss: 2.092928]\n",
      "[Epoch 80/200] [Batch 80/340] [D loss: 1.625398, acc: 59%] [G loss: 2.046047]\n",
      "[Epoch 80/200] [Batch 100/340] [D loss: 1.717355, acc: 58%] [G loss: 2.015973]\n",
      "[Epoch 80/200] [Batch 120/340] [D loss: 1.687037, acc: 56%] [G loss: 2.089820]\n",
      "[Epoch 80/200] [Batch 140/340] [D loss: 1.703724, acc: 58%] [G loss: 1.900782]\n",
      "[Epoch 80/200] [Batch 160/340] [D loss: 1.654546, acc: 60%] [G loss: 2.386451]\n",
      "[Epoch 80/200] [Batch 180/340] [D loss: 1.680090, acc: 59%] [G loss: 2.066210]\n",
      "[Epoch 80/200] [Batch 200/340] [D loss: 1.693744, acc: 58%] [G loss: 2.044422]\n",
      "[Epoch 80/200] [Batch 220/340] [D loss: 1.703425, acc: 58%] [G loss: 2.465971]\n",
      "[Epoch 80/200] [Batch 240/340] [D loss: 1.638425, acc: 58%] [G loss: 2.287685]\n",
      "[Epoch 80/200] [Batch 260/340] [D loss: 1.718843, acc: 59%] [G loss: 2.005604]\n",
      "[Epoch 80/200] [Batch 280/340] [D loss: 1.683133, acc: 54%] [G loss: 2.445049]\n",
      "[Epoch 80/200] [Batch 300/340] [D loss: 1.709829, acc: 59%] [G loss: 2.104445]\n",
      "[Epoch 80/200] [Batch 320/340] [D loss: 1.668116, acc: 58%] [G loss: 1.940600]\n",
      "[Epoch 81/200] [Batch 0/340] [D loss: 1.701312, acc: 58%] [G loss: 2.202891]\n",
      "[Epoch 81/200] [Batch 20/340] [D loss: 1.631816, acc: 56%] [G loss: 2.010136]\n",
      "[Epoch 81/200] [Batch 40/340] [D loss: 1.730625, acc: 58%] [G loss: 1.930494]\n",
      "[Epoch 81/200] [Batch 60/340] [D loss: 1.748018, acc: 58%] [G loss: 2.091259]\n",
      "[Epoch 81/200] [Batch 80/340] [D loss: 1.681503, acc: 56%] [G loss: 2.244729]\n",
      "[Epoch 81/200] [Batch 100/340] [D loss: 1.712186, acc: 56%] [G loss: 2.149033]\n",
      "[Epoch 81/200] [Batch 120/340] [D loss: 1.666691, acc: 58%] [G loss: 2.022659]\n",
      "[Epoch 81/200] [Batch 140/340] [D loss: 1.692987, acc: 58%] [G loss: 2.162902]\n",
      "[Epoch 81/200] [Batch 160/340] [D loss: 1.698757, acc: 60%] [G loss: 2.202049]\n",
      "[Epoch 81/200] [Batch 180/340] [D loss: 1.660930, acc: 59%] [G loss: 2.296270]\n",
      "[Epoch 81/200] [Batch 200/340] [D loss: 1.633277, acc: 59%] [G loss: 2.145604]\n",
      "[Epoch 81/200] [Batch 220/340] [D loss: 1.712319, acc: 57%] [G loss: 1.999494]\n",
      "[Epoch 81/200] [Batch 240/340] [D loss: 1.662258, acc: 56%] [G loss: 2.034036]\n",
      "[Epoch 81/200] [Batch 260/340] [D loss: 1.727391, acc: 56%] [G loss: 2.316953]\n",
      "[Epoch 81/200] [Batch 280/340] [D loss: 1.728566, acc: 56%] [G loss: 1.901669]\n",
      "[Epoch 81/200] [Batch 300/340] [D loss: 1.740724, acc: 56%] [G loss: 2.063075]\n",
      "[Epoch 81/200] [Batch 320/340] [D loss: 1.644781, acc: 56%] [G loss: 1.945180]\n",
      "[Epoch 82/200] [Batch 0/340] [D loss: 1.706590, acc: 57%] [G loss: 1.935006]\n",
      "[Epoch 82/200] [Batch 20/340] [D loss: 1.634836, acc: 58%] [G loss: 2.303508]\n",
      "[Epoch 82/200] [Batch 40/340] [D loss: 1.729169, acc: 60%] [G loss: 2.088738]\n",
      "[Epoch 82/200] [Batch 60/340] [D loss: 1.621071, acc: 61%] [G loss: 2.315792]\n",
      "[Epoch 82/200] [Batch 80/340] [D loss: 1.666942, acc: 58%] [G loss: 2.092723]\n",
      "[Epoch 82/200] [Batch 100/340] [D loss: 1.705539, acc: 54%] [G loss: 2.180133]\n",
      "[Epoch 82/200] [Batch 120/340] [D loss: 1.666899, acc: 59%] [G loss: 2.054929]\n",
      "[Epoch 82/200] [Batch 140/340] [D loss: 1.664681, acc: 58%] [G loss: 2.038273]\n",
      "[Epoch 82/200] [Batch 160/340] [D loss: 1.676930, acc: 60%] [G loss: 2.253054]\n",
      "[Epoch 82/200] [Batch 180/340] [D loss: 1.683960, acc: 57%] [G loss: 1.921576]\n",
      "[Epoch 82/200] [Batch 200/340] [D loss: 1.676512, acc: 55%] [G loss: 2.526220]\n",
      "[Epoch 82/200] [Batch 220/340] [D loss: 1.636246, acc: 59%] [G loss: 2.160257]\n",
      "[Epoch 82/200] [Batch 240/340] [D loss: 1.675888, acc: 58%] [G loss: 2.095593]\n",
      "[Epoch 82/200] [Batch 260/340] [D loss: 1.641348, acc: 55%] [G loss: 2.228058]\n",
      "[Epoch 82/200] [Batch 280/340] [D loss: 1.685502, acc: 59%] [G loss: 2.051970]\n",
      "[Epoch 82/200] [Batch 300/340] [D loss: 1.679222, acc: 60%] [G loss: 1.979136]\n",
      "[Epoch 82/200] [Batch 320/340] [D loss: 1.724216, acc: 59%] [G loss: 1.791053]\n",
      "[Epoch 83/200] [Batch 0/340] [D loss: 1.671974, acc: 62%] [G loss: 2.219268]\n",
      "[Epoch 83/200] [Batch 20/340] [D loss: 1.716526, acc: 58%] [G loss: 2.227669]\n",
      "[Epoch 83/200] [Batch 40/340] [D loss: 1.629214, acc: 58%] [G loss: 2.213667]\n",
      "[Epoch 83/200] [Batch 60/340] [D loss: 1.687142, acc: 57%] [G loss: 2.161355]\n",
      "[Epoch 83/200] [Batch 80/340] [D loss: 1.720429, acc: 56%] [G loss: 2.237813]\n",
      "[Epoch 83/200] [Batch 100/340] [D loss: 1.712056, acc: 59%] [G loss: 1.961221]\n",
      "[Epoch 83/200] [Batch 120/340] [D loss: 1.669001, acc: 58%] [G loss: 2.143057]\n",
      "[Epoch 83/200] [Batch 140/340] [D loss: 1.676513, acc: 60%] [G loss: 2.307713]\n",
      "[Epoch 83/200] [Batch 160/340] [D loss: 1.631608, acc: 60%] [G loss: 2.328082]\n",
      "[Epoch 83/200] [Batch 180/340] [D loss: 1.717424, acc: 59%] [G loss: 1.776962]\n",
      "[Epoch 83/200] [Batch 200/340] [D loss: 1.650378, acc: 60%] [G loss: 2.114988]\n",
      "[Epoch 83/200] [Batch 220/340] [D loss: 1.670055, acc: 55%] [G loss: 1.888868]\n",
      "[Epoch 83/200] [Batch 240/340] [D loss: 1.672956, acc: 59%] [G loss: 2.304095]\n",
      "[Epoch 83/200] [Batch 260/340] [D loss: 1.721687, acc: 59%] [G loss: 2.324764]\n",
      "[Epoch 83/200] [Batch 280/340] [D loss: 1.657042, acc: 58%] [G loss: 2.011853]\n",
      "[Epoch 83/200] [Batch 300/340] [D loss: 1.625753, acc: 62%] [G loss: 1.885971]\n",
      "[Epoch 83/200] [Batch 320/340] [D loss: 1.693876, acc: 58%] [G loss: 1.948627]\n",
      "[Epoch 84/200] [Batch 0/340] [D loss: 1.711320, acc: 54%] [G loss: 2.235780]\n",
      "[Epoch 84/200] [Batch 20/340] [D loss: 1.659578, acc: 61%] [G loss: 2.091207]\n",
      "[Epoch 84/200] [Batch 40/340] [D loss: 1.690084, acc: 58%] [G loss: 2.154746]\n",
      "[Epoch 84/200] [Batch 60/340] [D loss: 1.676864, acc: 56%] [G loss: 2.151128]\n",
      "[Epoch 84/200] [Batch 80/340] [D loss: 1.704200, acc: 56%] [G loss: 2.257284]\n",
      "[Epoch 84/200] [Batch 100/340] [D loss: 1.709465, acc: 58%] [G loss: 2.265298]\n",
      "[Epoch 84/200] [Batch 120/340] [D loss: 1.649713, acc: 59%] [G loss: 2.327500]\n",
      "[Epoch 84/200] [Batch 140/340] [D loss: 1.677849, acc: 56%] [G loss: 2.242059]\n",
      "[Epoch 84/200] [Batch 160/340] [D loss: 1.683236, acc: 56%] [G loss: 2.321598]\n",
      "[Epoch 84/200] [Batch 180/340] [D loss: 1.690288, acc: 57%] [G loss: 2.336929]\n",
      "[Epoch 84/200] [Batch 200/340] [D loss: 1.644603, acc: 58%] [G loss: 1.936976]\n",
      "[Epoch 84/200] [Batch 220/340] [D loss: 1.692294, acc: 59%] [G loss: 2.286551]\n",
      "[Epoch 84/200] [Batch 240/340] [D loss: 1.727321, acc: 58%] [G loss: 2.084454]\n",
      "[Epoch 84/200] [Batch 260/340] [D loss: 1.671708, acc: 58%] [G loss: 2.198967]\n",
      "[Epoch 84/200] [Batch 280/340] [D loss: 1.701077, acc: 58%] [G loss: 1.956429]\n",
      "[Epoch 84/200] [Batch 300/340] [D loss: 1.659349, acc: 56%] [G loss: 2.153515]\n",
      "[Epoch 84/200] [Batch 320/340] [D loss: 1.671323, acc: 62%] [G loss: 2.350617]\n",
      "[Epoch 85/200] [Batch 0/340] [D loss: 1.685715, acc: 60%] [G loss: 1.966081]\n",
      "[Epoch 85/200] [Batch 20/340] [D loss: 1.684959, acc: 58%] [G loss: 2.039166]\n",
      "[Epoch 85/200] [Batch 40/340] [D loss: 1.721615, acc: 58%] [G loss: 2.466010]\n",
      "[Epoch 85/200] [Batch 60/340] [D loss: 1.705089, acc: 51%] [G loss: 2.132154]\n",
      "[Epoch 85/200] [Batch 80/340] [D loss: 1.674250, acc: 60%] [G loss: 2.020519]\n",
      "[Epoch 85/200] [Batch 100/340] [D loss: 1.629284, acc: 59%] [G loss: 2.252365]\n",
      "[Epoch 85/200] [Batch 120/340] [D loss: 1.702516, acc: 55%] [G loss: 2.139805]\n",
      "[Epoch 85/200] [Batch 140/340] [D loss: 1.633255, acc: 59%] [G loss: 2.332185]\n",
      "[Epoch 85/200] [Batch 160/340] [D loss: 1.711581, acc: 60%] [G loss: 2.182231]\n",
      "[Epoch 85/200] [Batch 180/340] [D loss: 1.692810, acc: 60%] [G loss: 2.067674]\n",
      "[Epoch 85/200] [Batch 200/340] [D loss: 1.675786, acc: 57%] [G loss: 2.046912]\n",
      "[Epoch 85/200] [Batch 220/340] [D loss: 1.655416, acc: 56%] [G loss: 2.277796]\n",
      "[Epoch 85/200] [Batch 240/340] [D loss: 1.692270, acc: 59%] [G loss: 2.156709]\n",
      "[Epoch 85/200] [Batch 260/340] [D loss: 1.651479, acc: 63%] [G loss: 2.114503]\n",
      "[Epoch 85/200] [Batch 280/340] [D loss: 1.679893, acc: 58%] [G loss: 2.189602]\n",
      "[Epoch 85/200] [Batch 300/340] [D loss: 1.643270, acc: 62%] [G loss: 1.942622]\n",
      "[Epoch 85/200] [Batch 320/340] [D loss: 1.675546, acc: 58%] [G loss: 2.060512]\n",
      "[Epoch 86/200] [Batch 0/340] [D loss: 1.632717, acc: 60%] [G loss: 2.387694]\n",
      "[Epoch 86/200] [Batch 20/340] [D loss: 1.674914, acc: 60%] [G loss: 2.082403]\n",
      "[Epoch 86/200] [Batch 40/340] [D loss: 1.712723, acc: 56%] [G loss: 2.122923]\n",
      "[Epoch 86/200] [Batch 60/340] [D loss: 1.641922, acc: 58%] [G loss: 2.124193]\n",
      "[Epoch 86/200] [Batch 80/340] [D loss: 1.682835, acc: 57%] [G loss: 1.997735]\n",
      "[Epoch 86/200] [Batch 100/340] [D loss: 1.659348, acc: 57%] [G loss: 2.104658]\n",
      "[Epoch 86/200] [Batch 120/340] [D loss: 1.674728, acc: 59%] [G loss: 2.095239]\n",
      "[Epoch 86/200] [Batch 140/340] [D loss: 1.698175, acc: 59%] [G loss: 1.932885]\n",
      "[Epoch 86/200] [Batch 160/340] [D loss: 1.742996, acc: 60%] [G loss: 2.309443]\n",
      "[Epoch 86/200] [Batch 180/340] [D loss: 1.722037, acc: 62%] [G loss: 1.898489]\n",
      "[Epoch 86/200] [Batch 200/340] [D loss: 1.671540, acc: 59%] [G loss: 1.938464]\n",
      "[Epoch 86/200] [Batch 220/340] [D loss: 1.647650, acc: 61%] [G loss: 2.028135]\n",
      "[Epoch 86/200] [Batch 240/340] [D loss: 1.654188, acc: 59%] [G loss: 2.416326]\n",
      "[Epoch 86/200] [Batch 260/340] [D loss: 1.703474, acc: 55%] [G loss: 2.159186]\n",
      "[Epoch 86/200] [Batch 280/340] [D loss: 1.704069, acc: 58%] [G loss: 2.052699]\n",
      "[Epoch 86/200] [Batch 300/340] [D loss: 1.704831, acc: 55%] [G loss: 2.235515]\n",
      "[Epoch 86/200] [Batch 320/340] [D loss: 1.630089, acc: 60%] [G loss: 2.235307]\n",
      "[Epoch 87/200] [Batch 0/340] [D loss: 1.666989, acc: 61%] [G loss: 2.036730]\n",
      "[Epoch 87/200] [Batch 20/340] [D loss: 1.664531, acc: 60%] [G loss: 2.217084]\n",
      "[Epoch 87/200] [Batch 40/340] [D loss: 1.628389, acc: 59%] [G loss: 1.952397]\n",
      "[Epoch 87/200] [Batch 60/340] [D loss: 1.655166, acc: 58%] [G loss: 2.471097]\n",
      "[Epoch 87/200] [Batch 80/340] [D loss: 1.785136, acc: 58%] [G loss: 2.535464]\n",
      "[Epoch 87/200] [Batch 100/340] [D loss: 1.655729, acc: 56%] [G loss: 2.154097]\n",
      "[Epoch 87/200] [Batch 120/340] [D loss: 1.618715, acc: 56%] [G loss: 2.190215]\n",
      "[Epoch 87/200] [Batch 140/340] [D loss: 1.682179, acc: 58%] [G loss: 1.951637]\n",
      "[Epoch 87/200] [Batch 160/340] [D loss: 1.679093, acc: 58%] [G loss: 2.154237]\n",
      "[Epoch 87/200] [Batch 180/340] [D loss: 1.641370, acc: 60%] [G loss: 2.188533]\n",
      "[Epoch 87/200] [Batch 200/340] [D loss: 1.689969, acc: 57%] [G loss: 2.222777]\n",
      "[Epoch 87/200] [Batch 220/340] [D loss: 1.621760, acc: 61%] [G loss: 2.073552]\n",
      "[Epoch 87/200] [Batch 240/340] [D loss: 1.676790, acc: 59%] [G loss: 2.069242]\n",
      "[Epoch 87/200] [Batch 260/340] [D loss: 1.635800, acc: 61%] [G loss: 2.061640]\n",
      "[Epoch 87/200] [Batch 280/340] [D loss: 1.764534, acc: 58%] [G loss: 1.986400]\n",
      "[Epoch 87/200] [Batch 300/340] [D loss: 1.659599, acc: 58%] [G loss: 2.041098]\n",
      "[Epoch 87/200] [Batch 320/340] [D loss: 1.631559, acc: 61%] [G loss: 2.103775]\n",
      "[Epoch 88/200] [Batch 0/340] [D loss: 1.683799, acc: 58%] [G loss: 1.927599]\n",
      "[Epoch 88/200] [Batch 20/340] [D loss: 1.653958, acc: 57%] [G loss: 2.255890]\n",
      "[Epoch 88/200] [Batch 40/340] [D loss: 1.718641, acc: 57%] [G loss: 2.106786]\n",
      "[Epoch 88/200] [Batch 60/340] [D loss: 1.704513, acc: 56%] [G loss: 2.186329]\n",
      "[Epoch 88/200] [Batch 80/340] [D loss: 1.677297, acc: 58%] [G loss: 2.025581]\n",
      "[Epoch 88/200] [Batch 100/340] [D loss: 1.670285, acc: 58%] [G loss: 2.113655]\n",
      "[Epoch 88/200] [Batch 120/340] [D loss: 1.653920, acc: 59%] [G loss: 2.078434]\n",
      "[Epoch 88/200] [Batch 140/340] [D loss: 1.716194, acc: 54%] [G loss: 2.217377]\n",
      "[Epoch 88/200] [Batch 160/340] [D loss: 1.652329, acc: 60%] [G loss: 1.899273]\n",
      "[Epoch 88/200] [Batch 180/340] [D loss: 1.690114, acc: 60%] [G loss: 2.391593]\n",
      "[Epoch 88/200] [Batch 200/340] [D loss: 1.687642, acc: 56%] [G loss: 2.299613]\n",
      "[Epoch 88/200] [Batch 220/340] [D loss: 1.754725, acc: 58%] [G loss: 2.203243]\n",
      "[Epoch 88/200] [Batch 240/340] [D loss: 1.685786, acc: 61%] [G loss: 2.162063]\n",
      "[Epoch 88/200] [Batch 260/340] [D loss: 1.679956, acc: 60%] [G loss: 2.085865]\n",
      "[Epoch 88/200] [Batch 280/340] [D loss: 1.619992, acc: 60%] [G loss: 1.982169]\n",
      "[Epoch 88/200] [Batch 300/340] [D loss: 1.686926, acc: 58%] [G loss: 2.042442]\n",
      "[Epoch 88/200] [Batch 320/340] [D loss: 1.671027, acc: 57%] [G loss: 2.100399]\n",
      "[Epoch 89/200] [Batch 0/340] [D loss: 1.632580, acc: 59%] [G loss: 2.279192]\n",
      "[Epoch 89/200] [Batch 20/340] [D loss: 1.665090, acc: 55%] [G loss: 2.214778]\n",
      "[Epoch 89/200] [Batch 40/340] [D loss: 1.737216, acc: 60%] [G loss: 2.130332]\n",
      "[Epoch 89/200] [Batch 60/340] [D loss: 1.725631, acc: 57%] [G loss: 2.440363]\n",
      "[Epoch 89/200] [Batch 80/340] [D loss: 1.684106, acc: 60%] [G loss: 1.902058]\n",
      "[Epoch 89/200] [Batch 100/340] [D loss: 1.644534, acc: 57%] [G loss: 2.130037]\n",
      "[Epoch 89/200] [Batch 120/340] [D loss: 1.726300, acc: 58%] [G loss: 1.976991]\n",
      "[Epoch 89/200] [Batch 140/340] [D loss: 1.725393, acc: 59%] [G loss: 2.393324]\n",
      "[Epoch 89/200] [Batch 160/340] [D loss: 1.675046, acc: 60%] [G loss: 1.978968]\n",
      "[Epoch 89/200] [Batch 180/340] [D loss: 1.666909, acc: 59%] [G loss: 2.142747]\n",
      "[Epoch 89/200] [Batch 200/340] [D loss: 1.643366, acc: 63%] [G loss: 2.213595]\n",
      "[Epoch 89/200] [Batch 220/340] [D loss: 1.681537, acc: 62%] [G loss: 2.051244]\n",
      "[Epoch 89/200] [Batch 240/340] [D loss: 1.671893, acc: 55%] [G loss: 2.173032]\n",
      "[Epoch 89/200] [Batch 260/340] [D loss: 1.672635, acc: 60%] [G loss: 2.137277]\n",
      "[Epoch 89/200] [Batch 280/340] [D loss: 1.753029, acc: 61%] [G loss: 2.076198]\n",
      "[Epoch 89/200] [Batch 300/340] [D loss: 1.676464, acc: 55%] [G loss: 1.910887]\n",
      "[Epoch 89/200] [Batch 320/340] [D loss: 1.697329, acc: 59%] [G loss: 2.366405]\n",
      "[Epoch 90/200] [Batch 0/340] [D loss: 1.685422, acc: 58%] [G loss: 2.121909]\n",
      "[Epoch 90/200] [Batch 20/340] [D loss: 1.696863, acc: 58%] [G loss: 1.983423]\n",
      "[Epoch 90/200] [Batch 40/340] [D loss: 1.682522, acc: 57%] [G loss: 2.346462]\n",
      "[Epoch 90/200] [Batch 60/340] [D loss: 1.701415, acc: 60%] [G loss: 2.009217]\n",
      "[Epoch 90/200] [Batch 80/340] [D loss: 1.652084, acc: 57%] [G loss: 2.163379]\n",
      "[Epoch 90/200] [Batch 100/340] [D loss: 1.619713, acc: 61%] [G loss: 2.155169]\n",
      "[Epoch 90/200] [Batch 120/340] [D loss: 1.665884, acc: 56%] [G loss: 2.378427]\n",
      "[Epoch 90/200] [Batch 140/340] [D loss: 1.678252, acc: 59%] [G loss: 2.402073]\n",
      "[Epoch 90/200] [Batch 160/340] [D loss: 1.662314, acc: 58%] [G loss: 2.039559]\n",
      "[Epoch 90/200] [Batch 180/340] [D loss: 1.663557, acc: 60%] [G loss: 1.981619]\n",
      "[Epoch 90/200] [Batch 200/340] [D loss: 1.701383, acc: 64%] [G loss: 2.422993]\n",
      "[Epoch 90/200] [Batch 220/340] [D loss: 1.698437, acc: 56%] [G loss: 2.036332]\n",
      "[Epoch 90/200] [Batch 240/340] [D loss: 1.715714, acc: 62%] [G loss: 2.288340]\n",
      "[Epoch 90/200] [Batch 260/340] [D loss: 1.662979, acc: 62%] [G loss: 1.860628]\n",
      "[Epoch 90/200] [Batch 280/340] [D loss: 1.679404, acc: 60%] [G loss: 2.116896]\n",
      "[Epoch 90/200] [Batch 300/340] [D loss: 1.682992, acc: 58%] [G loss: 1.850753]\n",
      "[Epoch 90/200] [Batch 320/340] [D loss: 1.640582, acc: 61%] [G loss: 2.059347]\n",
      "[Epoch 91/200] [Batch 0/340] [D loss: 1.666259, acc: 56%] [G loss: 2.382276]\n",
      "[Epoch 91/200] [Batch 20/340] [D loss: 1.659964, acc: 57%] [G loss: 2.079459]\n",
      "[Epoch 91/200] [Batch 40/340] [D loss: 1.656947, acc: 59%] [G loss: 2.275535]\n",
      "[Epoch 91/200] [Batch 60/340] [D loss: 1.638287, acc: 59%] [G loss: 2.078728]\n",
      "[Epoch 91/200] [Batch 80/340] [D loss: 1.749110, acc: 60%] [G loss: 2.024580]\n",
      "[Epoch 91/200] [Batch 100/340] [D loss: 1.638466, acc: 64%] [G loss: 1.863887]\n",
      "[Epoch 91/200] [Batch 120/340] [D loss: 1.713414, acc: 58%] [G loss: 2.059078]\n",
      "[Epoch 91/200] [Batch 140/340] [D loss: 1.661357, acc: 60%] [G loss: 2.146802]\n",
      "[Epoch 91/200] [Batch 160/340] [D loss: 1.665670, acc: 56%] [G loss: 1.845410]\n",
      "[Epoch 91/200] [Batch 180/340] [D loss: 1.625074, acc: 60%] [G loss: 1.986998]\n",
      "[Epoch 91/200] [Batch 200/340] [D loss: 1.696499, acc: 59%] [G loss: 2.028193]\n",
      "[Epoch 91/200] [Batch 220/340] [D loss: 1.711314, acc: 56%] [G loss: 1.938105]\n",
      "[Epoch 91/200] [Batch 240/340] [D loss: 1.693435, acc: 60%] [G loss: 2.278219]\n",
      "[Epoch 91/200] [Batch 260/340] [D loss: 1.663700, acc: 61%] [G loss: 2.099980]\n",
      "[Epoch 91/200] [Batch 280/340] [D loss: 1.648823, acc: 58%] [G loss: 1.888649]\n",
      "[Epoch 91/200] [Batch 300/340] [D loss: 1.672473, acc: 56%] [G loss: 2.077852]\n",
      "[Epoch 91/200] [Batch 320/340] [D loss: 1.694092, acc: 58%] [G loss: 2.261230]\n",
      "[Epoch 92/200] [Batch 0/340] [D loss: 1.679398, acc: 61%] [G loss: 2.516624]\n",
      "[Epoch 92/200] [Batch 20/340] [D loss: 1.624680, acc: 61%] [G loss: 2.236546]\n",
      "[Epoch 92/200] [Batch 40/340] [D loss: 1.660140, acc: 62%] [G loss: 2.108575]\n",
      "[Epoch 92/200] [Batch 60/340] [D loss: 1.687865, acc: 65%] [G loss: 2.368400]\n",
      "[Epoch 92/200] [Batch 80/340] [D loss: 1.718669, acc: 63%] [G loss: 1.934956]\n",
      "[Epoch 92/200] [Batch 100/340] [D loss: 1.695999, acc: 62%] [G loss: 2.029627]\n",
      "[Epoch 92/200] [Batch 120/340] [D loss: 1.658113, acc: 58%] [G loss: 2.112355]\n",
      "[Epoch 92/200] [Batch 140/340] [D loss: 1.818800, acc: 59%] [G loss: 1.783531]\n",
      "[Epoch 92/200] [Batch 160/340] [D loss: 1.666299, acc: 60%] [G loss: 2.092422]\n",
      "[Epoch 92/200] [Batch 180/340] [D loss: 1.648820, acc: 60%] [G loss: 2.164678]\n",
      "[Epoch 92/200] [Batch 200/340] [D loss: 1.663958, acc: 59%] [G loss: 2.325189]\n",
      "[Epoch 92/200] [Batch 220/340] [D loss: 1.709786, acc: 55%] [G loss: 1.988763]\n",
      "[Epoch 92/200] [Batch 240/340] [D loss: 1.668054, acc: 60%] [G loss: 2.135575]\n",
      "[Epoch 92/200] [Batch 260/340] [D loss: 1.710723, acc: 56%] [G loss: 2.051412]\n",
      "[Epoch 92/200] [Batch 280/340] [D loss: 1.646842, acc: 59%] [G loss: 2.066721]\n",
      "[Epoch 92/200] [Batch 300/340] [D loss: 1.643214, acc: 58%] [G loss: 2.232992]\n",
      "[Epoch 92/200] [Batch 320/340] [D loss: 1.676377, acc: 60%] [G loss: 2.113379]\n",
      "[Epoch 93/200] [Batch 0/340] [D loss: 1.648132, acc: 59%] [G loss: 2.178716]\n",
      "[Epoch 93/200] [Batch 20/340] [D loss: 1.659804, acc: 58%] [G loss: 2.197031]\n",
      "[Epoch 93/200] [Batch 40/340] [D loss: 1.685323, acc: 60%] [G loss: 2.326996]\n",
      "[Epoch 93/200] [Batch 60/340] [D loss: 1.666806, acc: 61%] [G loss: 2.086396]\n",
      "[Epoch 93/200] [Batch 80/340] [D loss: 1.714703, acc: 58%] [G loss: 2.201026]\n",
      "[Epoch 93/200] [Batch 100/340] [D loss: 1.692329, acc: 62%] [G loss: 2.352715]\n",
      "[Epoch 93/200] [Batch 120/340] [D loss: 1.724710, acc: 58%] [G loss: 2.036004]\n",
      "[Epoch 93/200] [Batch 140/340] [D loss: 1.663208, acc: 61%] [G loss: 2.112870]\n",
      "[Epoch 93/200] [Batch 160/340] [D loss: 1.678370, acc: 58%] [G loss: 2.344811]\n",
      "[Epoch 93/200] [Batch 180/340] [D loss: 1.684256, acc: 61%] [G loss: 2.447897]\n",
      "[Epoch 93/200] [Batch 200/340] [D loss: 1.638950, acc: 62%] [G loss: 2.273273]\n",
      "[Epoch 93/200] [Batch 220/340] [D loss: 1.663294, acc: 63%] [G loss: 2.299544]\n",
      "[Epoch 93/200] [Batch 240/340] [D loss: 1.677083, acc: 58%] [G loss: 2.084726]\n",
      "[Epoch 93/200] [Batch 260/340] [D loss: 1.656095, acc: 58%] [G loss: 2.193184]\n",
      "[Epoch 93/200] [Batch 280/340] [D loss: 1.642785, acc: 64%] [G loss: 2.006579]\n",
      "[Epoch 93/200] [Batch 300/340] [D loss: 1.710280, acc: 60%] [G loss: 2.521141]\n",
      "[Epoch 93/200] [Batch 320/340] [D loss: 1.709122, acc: 60%] [G loss: 1.819435]\n",
      "[Epoch 94/200] [Batch 0/340] [D loss: 1.695884, acc: 60%] [G loss: 2.185628]\n",
      "[Epoch 94/200] [Batch 20/340] [D loss: 1.692743, acc: 58%] [G loss: 2.247046]\n",
      "[Epoch 94/200] [Batch 40/340] [D loss: 1.684055, acc: 58%] [G loss: 2.068191]\n",
      "[Epoch 94/200] [Batch 60/340] [D loss: 1.704950, acc: 61%] [G loss: 2.187772]\n",
      "[Epoch 94/200] [Batch 80/340] [D loss: 1.770064, acc: 59%] [G loss: 1.716085]\n",
      "[Epoch 94/200] [Batch 100/340] [D loss: 1.645364, acc: 63%] [G loss: 1.923645]\n",
      "[Epoch 94/200] [Batch 120/340] [D loss: 1.704062, acc: 61%] [G loss: 2.301669]\n",
      "[Epoch 94/200] [Batch 140/340] [D loss: 1.744549, acc: 60%] [G loss: 1.838414]\n",
      "[Epoch 94/200] [Batch 160/340] [D loss: 1.699688, acc: 58%] [G loss: 2.320125]\n",
      "[Epoch 94/200] [Batch 180/340] [D loss: 1.771729, acc: 62%] [G loss: 2.004499]\n",
      "[Epoch 94/200] [Batch 200/340] [D loss: 1.694246, acc: 60%] [G loss: 2.181421]\n",
      "[Epoch 94/200] [Batch 220/340] [D loss: 1.654881, acc: 60%] [G loss: 2.400481]\n",
      "[Epoch 94/200] [Batch 240/340] [D loss: 1.659611, acc: 66%] [G loss: 2.158375]\n",
      "[Epoch 94/200] [Batch 260/340] [D loss: 1.633656, acc: 57%] [G loss: 2.449352]\n",
      "[Epoch 94/200] [Batch 280/340] [D loss: 1.652250, acc: 59%] [G loss: 2.279367]\n",
      "[Epoch 94/200] [Batch 300/340] [D loss: 1.679673, acc: 61%] [G loss: 1.897906]\n",
      "[Epoch 94/200] [Batch 320/340] [D loss: 1.678925, acc: 63%] [G loss: 2.030215]\n",
      "[Epoch 95/200] [Batch 0/340] [D loss: 1.652312, acc: 64%] [G loss: 2.426205]\n",
      "[Epoch 95/200] [Batch 20/340] [D loss: 1.700094, acc: 60%] [G loss: 1.856777]\n",
      "[Epoch 95/200] [Batch 40/340] [D loss: 1.633886, acc: 58%] [G loss: 2.093730]\n",
      "[Epoch 95/200] [Batch 60/340] [D loss: 1.595318, acc: 66%] [G loss: 2.206030]\n",
      "[Epoch 95/200] [Batch 80/340] [D loss: 1.678507, acc: 60%] [G loss: 1.963156]\n",
      "[Epoch 95/200] [Batch 100/340] [D loss: 1.685709, acc: 58%] [G loss: 1.930791]\n",
      "[Epoch 95/200] [Batch 120/340] [D loss: 1.643411, acc: 64%] [G loss: 2.048063]\n",
      "[Epoch 95/200] [Batch 140/340] [D loss: 1.649991, acc: 58%] [G loss: 2.123167]\n",
      "[Epoch 95/200] [Batch 160/340] [D loss: 1.688283, acc: 59%] [G loss: 2.128384]\n",
      "[Epoch 95/200] [Batch 180/340] [D loss: 1.738816, acc: 61%] [G loss: 2.258381]\n",
      "[Epoch 95/200] [Batch 200/340] [D loss: 1.703446, acc: 62%] [G loss: 2.237384]\n",
      "[Epoch 95/200] [Batch 220/340] [D loss: 1.628823, acc: 62%] [G loss: 2.008454]\n",
      "[Epoch 95/200] [Batch 240/340] [D loss: 1.687113, acc: 58%] [G loss: 2.083318]\n",
      "[Epoch 95/200] [Batch 260/340] [D loss: 1.651854, acc: 63%] [G loss: 2.152092]\n",
      "[Epoch 95/200] [Batch 280/340] [D loss: 1.721095, acc: 58%] [G loss: 1.872499]\n",
      "[Epoch 95/200] [Batch 300/340] [D loss: 1.732426, acc: 60%] [G loss: 1.957753]\n",
      "[Epoch 95/200] [Batch 320/340] [D loss: 1.690588, acc: 61%] [G loss: 1.992307]\n",
      "[Epoch 96/200] [Batch 0/340] [D loss: 1.700365, acc: 59%] [G loss: 1.960503]\n",
      "[Epoch 96/200] [Batch 20/340] [D loss: 1.697637, acc: 61%] [G loss: 2.086562]\n",
      "[Epoch 96/200] [Batch 40/340] [D loss: 1.721654, acc: 60%] [G loss: 2.129655]\n",
      "[Epoch 96/200] [Batch 60/340] [D loss: 1.674481, acc: 60%] [G loss: 2.118737]\n",
      "[Epoch 96/200] [Batch 80/340] [D loss: 1.649305, acc: 61%] [G loss: 2.081028]\n",
      "[Epoch 96/200] [Batch 100/340] [D loss: 1.693352, acc: 62%] [G loss: 2.279567]\n",
      "[Epoch 96/200] [Batch 120/340] [D loss: 1.683319, acc: 60%] [G loss: 2.011456]\n",
      "[Epoch 96/200] [Batch 140/340] [D loss: 1.729217, acc: 60%] [G loss: 2.003203]\n",
      "[Epoch 96/200] [Batch 160/340] [D loss: 1.702377, acc: 58%] [G loss: 1.955014]\n",
      "[Epoch 96/200] [Batch 180/340] [D loss: 1.637995, acc: 59%] [G loss: 2.095067]\n",
      "[Epoch 96/200] [Batch 200/340] [D loss: 1.644440, acc: 60%] [G loss: 2.224241]\n",
      "[Epoch 96/200] [Batch 220/340] [D loss: 1.667070, acc: 58%] [G loss: 2.529317]\n",
      "[Epoch 96/200] [Batch 240/340] [D loss: 1.754955, acc: 61%] [G loss: 2.066952]\n",
      "[Epoch 96/200] [Batch 260/340] [D loss: 1.717295, acc: 60%] [G loss: 1.806068]\n",
      "[Epoch 96/200] [Batch 280/340] [D loss: 1.688204, acc: 63%] [G loss: 1.953151]\n",
      "[Epoch 96/200] [Batch 300/340] [D loss: 1.712195, acc: 58%] [G loss: 2.006546]\n",
      "[Epoch 96/200] [Batch 320/340] [D loss: 1.667689, acc: 57%] [G loss: 2.028610]\n",
      "[Epoch 97/200] [Batch 0/340] [D loss: 1.694325, acc: 61%] [G loss: 2.693660]\n",
      "[Epoch 97/200] [Batch 20/340] [D loss: 1.679247, acc: 57%] [G loss: 2.047489]\n",
      "[Epoch 97/200] [Batch 40/340] [D loss: 1.697215, acc: 58%] [G loss: 2.150105]\n",
      "[Epoch 97/200] [Batch 60/340] [D loss: 1.593906, acc: 63%] [G loss: 2.104850]\n",
      "[Epoch 97/200] [Batch 80/340] [D loss: 1.679538, acc: 58%] [G loss: 2.202044]\n",
      "[Epoch 97/200] [Batch 100/340] [D loss: 1.674137, acc: 61%] [G loss: 2.416414]\n",
      "[Epoch 97/200] [Batch 120/340] [D loss: 1.636438, acc: 63%] [G loss: 2.138965]\n",
      "[Epoch 97/200] [Batch 140/340] [D loss: 1.675459, acc: 59%] [G loss: 1.969730]\n",
      "[Epoch 97/200] [Batch 160/340] [D loss: 1.671906, acc: 64%] [G loss: 2.094676]\n",
      "[Epoch 97/200] [Batch 180/340] [D loss: 1.675785, acc: 61%] [G loss: 1.854008]\n",
      "[Epoch 97/200] [Batch 200/340] [D loss: 1.718400, acc: 60%] [G loss: 2.179603]\n",
      "[Epoch 97/200] [Batch 220/340] [D loss: 1.660869, acc: 56%] [G loss: 2.215814]\n",
      "[Epoch 97/200] [Batch 240/340] [D loss: 1.738567, acc: 63%] [G loss: 2.569185]\n",
      "[Epoch 97/200] [Batch 260/340] [D loss: 1.736979, acc: 58%] [G loss: 1.991163]\n",
      "[Epoch 97/200] [Batch 280/340] [D loss: 1.710532, acc: 62%] [G loss: 2.049468]\n",
      "[Epoch 97/200] [Batch 300/340] [D loss: 1.610987, acc: 62%] [G loss: 1.954680]\n",
      "[Epoch 97/200] [Batch 320/340] [D loss: 1.776673, acc: 59%] [G loss: 1.847449]\n",
      "[Epoch 98/200] [Batch 0/340] [D loss: 1.669703, acc: 64%] [G loss: 1.929105]\n",
      "[Epoch 98/200] [Batch 20/340] [D loss: 1.617072, acc: 60%] [G loss: 2.503563]\n",
      "[Epoch 98/200] [Batch 40/340] [D loss: 1.687805, acc: 58%] [G loss: 2.193117]\n",
      "[Epoch 98/200] [Batch 60/340] [D loss: 1.657191, acc: 61%] [G loss: 2.228523]\n",
      "[Epoch 98/200] [Batch 80/340] [D loss: 1.676596, acc: 63%] [G loss: 2.214747]\n",
      "[Epoch 98/200] [Batch 100/340] [D loss: 1.653350, acc: 60%] [G loss: 2.104579]\n",
      "[Epoch 98/200] [Batch 120/340] [D loss: 1.696545, acc: 62%] [G loss: 1.791899]\n",
      "[Epoch 98/200] [Batch 140/340] [D loss: 1.689393, acc: 62%] [G loss: 2.112174]\n",
      "[Epoch 98/200] [Batch 160/340] [D loss: 1.674840, acc: 59%] [G loss: 2.125916]\n",
      "[Epoch 98/200] [Batch 180/340] [D loss: 1.631210, acc: 60%] [G loss: 1.861031]\n",
      "[Epoch 98/200] [Batch 200/340] [D loss: 1.646374, acc: 62%] [G loss: 2.343905]\n",
      "[Epoch 98/200] [Batch 220/340] [D loss: 1.694106, acc: 64%] [G loss: 1.966991]\n",
      "[Epoch 98/200] [Batch 240/340] [D loss: 1.730047, acc: 59%] [G loss: 2.047991]\n",
      "[Epoch 98/200] [Batch 260/340] [D loss: 1.694820, acc: 61%] [G loss: 1.931988]\n",
      "[Epoch 98/200] [Batch 280/340] [D loss: 1.636666, acc: 62%] [G loss: 2.192281]\n",
      "[Epoch 98/200] [Batch 300/340] [D loss: 1.681533, acc: 62%] [G loss: 2.369713]\n",
      "[Epoch 98/200] [Batch 320/340] [D loss: 1.719801, acc: 60%] [G loss: 2.064343]\n",
      "[Epoch 99/200] [Batch 0/340] [D loss: 1.684364, acc: 64%] [G loss: 2.108559]\n",
      "[Epoch 99/200] [Batch 20/340] [D loss: 1.669401, acc: 59%] [G loss: 2.054480]\n",
      "[Epoch 99/200] [Batch 40/340] [D loss: 1.662746, acc: 59%] [G loss: 1.989383]\n",
      "[Epoch 99/200] [Batch 60/340] [D loss: 1.633828, acc: 61%] [G loss: 2.112007]\n",
      "[Epoch 99/200] [Batch 80/340] [D loss: 1.716904, acc: 59%] [G loss: 2.472659]\n",
      "[Epoch 99/200] [Batch 100/340] [D loss: 1.713150, acc: 60%] [G loss: 2.068841]\n",
      "[Epoch 99/200] [Batch 120/340] [D loss: 1.652086, acc: 57%] [G loss: 2.114610]\n",
      "[Epoch 99/200] [Batch 140/340] [D loss: 1.752411, acc: 60%] [G loss: 2.221826]\n",
      "[Epoch 99/200] [Batch 160/340] [D loss: 1.706046, acc: 59%] [G loss: 2.148688]\n",
      "[Epoch 99/200] [Batch 180/340] [D loss: 1.716493, acc: 61%] [G loss: 1.914850]\n",
      "[Epoch 99/200] [Batch 200/340] [D loss: 1.660008, acc: 61%] [G loss: 2.042134]\n",
      "[Epoch 99/200] [Batch 220/340] [D loss: 1.727448, acc: 60%] [G loss: 1.994011]\n",
      "[Epoch 99/200] [Batch 240/340] [D loss: 1.723079, acc: 58%] [G loss: 2.181416]\n",
      "[Epoch 99/200] [Batch 260/340] [D loss: 1.679866, acc: 57%] [G loss: 1.974842]\n",
      "[Epoch 99/200] [Batch 280/340] [D loss: 1.648596, acc: 63%] [G loss: 2.235006]\n",
      "[Epoch 99/200] [Batch 300/340] [D loss: 1.691296, acc: 64%] [G loss: 1.977503]\n",
      "[Epoch 99/200] [Batch 320/340] [D loss: 1.717765, acc: 63%] [G loss: 1.882440]\n",
      "[Epoch 100/200] [Batch 0/340] [D loss: 1.674899, acc: 62%] [G loss: 1.985186]\n",
      "[Epoch 100/200] [Batch 20/340] [D loss: 1.687354, acc: 61%] [G loss: 2.116201]\n",
      "[Epoch 100/200] [Batch 40/340] [D loss: 1.714888, acc: 64%] [G loss: 2.044090]\n",
      "[Epoch 100/200] [Batch 60/340] [D loss: 1.656045, acc: 58%] [G loss: 2.124499]\n",
      "[Epoch 100/200] [Batch 80/340] [D loss: 1.651847, acc: 63%] [G loss: 1.962893]\n",
      "[Epoch 100/200] [Batch 100/340] [D loss: 1.688625, acc: 61%] [G loss: 2.020315]\n",
      "[Epoch 100/200] [Batch 120/340] [D loss: 1.654340, acc: 62%] [G loss: 2.018921]\n",
      "[Epoch 100/200] [Batch 140/340] [D loss: 1.619832, acc: 62%] [G loss: 2.359122]\n",
      "[Epoch 100/200] [Batch 160/340] [D loss: 1.689884, acc: 60%] [G loss: 2.064499]\n",
      "[Epoch 100/200] [Batch 180/340] [D loss: 1.692059, acc: 63%] [G loss: 2.363926]\n",
      "[Epoch 100/200] [Batch 200/340] [D loss: 1.629271, acc: 61%] [G loss: 1.966366]\n",
      "[Epoch 100/200] [Batch 220/340] [D loss: 1.657030, acc: 63%] [G loss: 1.903984]\n",
      "[Epoch 100/200] [Batch 240/340] [D loss: 1.637591, acc: 62%] [G loss: 1.991321]\n",
      "[Epoch 100/200] [Batch 260/340] [D loss: 1.668807, acc: 63%] [G loss: 2.049640]\n",
      "[Epoch 100/200] [Batch 280/340] [D loss: 1.709847, acc: 63%] [G loss: 2.402439]\n",
      "[Epoch 100/200] [Batch 300/340] [D loss: 1.642080, acc: 68%] [G loss: 2.203363]\n",
      "[Epoch 100/200] [Batch 320/340] [D loss: 1.684996, acc: 61%] [G loss: 2.338001]\n",
      "[Epoch 101/200] [Batch 0/340] [D loss: 1.745570, acc: 61%] [G loss: 2.063881]\n",
      "[Epoch 101/200] [Batch 20/340] [D loss: 1.692222, acc: 63%] [G loss: 2.005607]\n",
      "[Epoch 101/200] [Batch 40/340] [D loss: 1.704874, acc: 64%] [G loss: 1.984078]\n",
      "[Epoch 101/200] [Batch 60/340] [D loss: 1.665976, acc: 62%] [G loss: 2.158328]\n",
      "[Epoch 101/200] [Batch 80/340] [D loss: 1.623418, acc: 58%] [G loss: 2.174288]\n",
      "[Epoch 101/200] [Batch 100/340] [D loss: 1.621668, acc: 62%] [G loss: 1.984717]\n",
      "[Epoch 101/200] [Batch 120/340] [D loss: 1.660379, acc: 63%] [G loss: 2.125061]\n",
      "[Epoch 101/200] [Batch 140/340] [D loss: 1.691919, acc: 61%] [G loss: 2.140897]\n",
      "[Epoch 101/200] [Batch 160/340] [D loss: 1.671881, acc: 62%] [G loss: 2.045935]\n",
      "[Epoch 101/200] [Batch 180/340] [D loss: 1.650174, acc: 60%] [G loss: 2.044139]\n",
      "[Epoch 101/200] [Batch 200/340] [D loss: 1.658073, acc: 64%] [G loss: 1.898852]\n",
      "[Epoch 101/200] [Batch 220/340] [D loss: 1.653831, acc: 62%] [G loss: 1.995064]\n",
      "[Epoch 101/200] [Batch 240/340] [D loss: 1.673245, acc: 63%] [G loss: 1.778714]\n",
      "[Epoch 101/200] [Batch 260/340] [D loss: 1.671575, acc: 61%] [G loss: 1.991148]\n",
      "[Epoch 101/200] [Batch 280/340] [D loss: 1.686499, acc: 61%] [G loss: 1.909026]\n",
      "[Epoch 101/200] [Batch 300/340] [D loss: 1.636635, acc: 58%] [G loss: 2.096953]\n",
      "[Epoch 101/200] [Batch 320/340] [D loss: 1.628190, acc: 63%] [G loss: 1.863831]\n",
      "[Epoch 102/200] [Batch 0/340] [D loss: 1.731712, acc: 61%] [G loss: 2.059128]\n",
      "[Epoch 102/200] [Batch 20/340] [D loss: 1.686115, acc: 58%] [G loss: 1.876410]\n",
      "[Epoch 102/200] [Batch 40/340] [D loss: 1.680215, acc: 62%] [G loss: 1.957108]\n",
      "[Epoch 102/200] [Batch 60/340] [D loss: 1.701667, acc: 60%] [G loss: 1.946108]\n",
      "[Epoch 102/200] [Batch 80/340] [D loss: 1.707232, acc: 62%] [G loss: 1.994314]\n",
      "[Epoch 102/200] [Batch 100/340] [D loss: 1.719499, acc: 60%] [G loss: 1.957066]\n",
      "[Epoch 102/200] [Batch 120/340] [D loss: 1.623056, acc: 61%] [G loss: 2.100174]\n",
      "[Epoch 102/200] [Batch 140/340] [D loss: 1.620546, acc: 64%] [G loss: 2.013221]\n",
      "[Epoch 102/200] [Batch 160/340] [D loss: 1.732303, acc: 61%] [G loss: 2.213329]\n",
      "[Epoch 102/200] [Batch 180/340] [D loss: 1.679654, acc: 60%] [G loss: 1.967614]\n",
      "[Epoch 102/200] [Batch 200/340] [D loss: 1.599842, acc: 63%] [G loss: 2.202012]\n",
      "[Epoch 102/200] [Batch 220/340] [D loss: 1.623263, acc: 61%] [G loss: 2.296846]\n",
      "[Epoch 102/200] [Batch 240/340] [D loss: 1.644030, acc: 60%] [G loss: 2.106150]\n",
      "[Epoch 102/200] [Batch 260/340] [D loss: 1.668138, acc: 65%] [G loss: 1.972630]\n",
      "[Epoch 102/200] [Batch 280/340] [D loss: 1.683104, acc: 61%] [G loss: 2.093776]\n",
      "[Epoch 102/200] [Batch 300/340] [D loss: 1.681354, acc: 60%] [G loss: 2.244268]\n",
      "[Epoch 102/200] [Batch 320/340] [D loss: 1.629869, acc: 65%] [G loss: 1.970518]\n",
      "[Epoch 103/200] [Batch 0/340] [D loss: 1.674942, acc: 64%] [G loss: 2.019758]\n",
      "[Epoch 103/200] [Batch 20/340] [D loss: 1.656040, acc: 62%] [G loss: 2.343432]\n",
      "[Epoch 103/200] [Batch 40/340] [D loss: 1.646766, acc: 62%] [G loss: 1.969069]\n",
      "[Epoch 103/200] [Batch 60/340] [D loss: 1.597206, acc: 66%] [G loss: 1.958904]\n",
      "[Epoch 103/200] [Batch 80/340] [D loss: 1.626929, acc: 62%] [G loss: 1.959435]\n",
      "[Epoch 103/200] [Batch 100/340] [D loss: 1.726439, acc: 64%] [G loss: 2.120239]\n",
      "[Epoch 103/200] [Batch 120/340] [D loss: 1.685796, acc: 64%] [G loss: 2.129668]\n",
      "[Epoch 103/200] [Batch 140/340] [D loss: 1.643317, acc: 58%] [G loss: 2.295393]\n",
      "[Epoch 103/200] [Batch 160/340] [D loss: 1.695739, acc: 61%] [G loss: 2.107461]\n",
      "[Epoch 103/200] [Batch 180/340] [D loss: 1.619704, acc: 61%] [G loss: 2.020474]\n",
      "[Epoch 103/200] [Batch 200/340] [D loss: 1.715945, acc: 60%] [G loss: 1.799587]\n",
      "[Epoch 103/200] [Batch 220/340] [D loss: 1.627863, acc: 64%] [G loss: 2.016317]\n",
      "[Epoch 103/200] [Batch 240/340] [D loss: 1.662470, acc: 62%] [G loss: 1.965386]\n",
      "[Epoch 103/200] [Batch 260/340] [D loss: 1.734155, acc: 61%] [G loss: 2.707624]\n",
      "[Epoch 103/200] [Batch 280/340] [D loss: 1.622784, acc: 64%] [G loss: 1.976651]\n",
      "[Epoch 103/200] [Batch 300/340] [D loss: 1.704949, acc: 63%] [G loss: 2.293953]\n",
      "[Epoch 103/200] [Batch 320/340] [D loss: 1.680122, acc: 64%] [G loss: 2.125757]\n",
      "[Epoch 104/200] [Batch 0/340] [D loss: 1.649194, acc: 63%] [G loss: 1.873350]\n",
      "[Epoch 104/200] [Batch 20/340] [D loss: 1.733085, acc: 62%] [G loss: 1.900541]\n",
      "[Epoch 104/200] [Batch 40/340] [D loss: 1.640601, acc: 63%] [G loss: 2.206413]\n",
      "[Epoch 104/200] [Batch 60/340] [D loss: 1.747058, acc: 64%] [G loss: 1.975988]\n",
      "[Epoch 104/200] [Batch 80/340] [D loss: 1.650477, acc: 60%] [G loss: 2.168293]\n",
      "[Epoch 104/200] [Batch 100/340] [D loss: 1.624501, acc: 65%] [G loss: 1.961931]\n",
      "[Epoch 104/200] [Batch 120/340] [D loss: 1.634925, acc: 62%] [G loss: 1.958831]\n",
      "[Epoch 104/200] [Batch 140/340] [D loss: 1.611401, acc: 63%] [G loss: 2.193818]\n",
      "[Epoch 104/200] [Batch 160/340] [D loss: 1.667085, acc: 61%] [G loss: 1.994022]\n",
      "[Epoch 104/200] [Batch 180/340] [D loss: 1.695555, acc: 64%] [G loss: 2.185955]\n",
      "[Epoch 104/200] [Batch 200/340] [D loss: 1.654786, acc: 60%] [G loss: 2.206409]\n",
      "[Epoch 104/200] [Batch 220/340] [D loss: 1.626351, acc: 63%] [G loss: 1.965468]\n",
      "[Epoch 104/200] [Batch 240/340] [D loss: 1.636510, acc: 65%] [G loss: 1.961321]\n",
      "[Epoch 104/200] [Batch 260/340] [D loss: 1.583202, acc: 62%] [G loss: 2.121285]\n",
      "[Epoch 104/200] [Batch 280/340] [D loss: 1.679586, acc: 64%] [G loss: 2.440549]\n",
      "[Epoch 104/200] [Batch 300/340] [D loss: 1.714899, acc: 62%] [G loss: 2.124963]\n",
      "[Epoch 104/200] [Batch 320/340] [D loss: 1.700228, acc: 63%] [G loss: 2.042755]\n",
      "[Epoch 105/200] [Batch 0/340] [D loss: 1.667300, acc: 58%] [G loss: 1.867526]\n",
      "[Epoch 105/200] [Batch 20/340] [D loss: 1.697609, acc: 61%] [G loss: 2.149872]\n",
      "[Epoch 105/200] [Batch 40/340] [D loss: 1.652859, acc: 62%] [G loss: 1.785667]\n",
      "[Epoch 105/200] [Batch 60/340] [D loss: 1.680323, acc: 62%] [G loss: 1.872659]\n",
      "[Epoch 105/200] [Batch 80/340] [D loss: 1.632017, acc: 60%] [G loss: 2.098289]\n",
      "[Epoch 105/200] [Batch 100/340] [D loss: 1.672286, acc: 59%] [G loss: 2.127809]\n",
      "[Epoch 105/200] [Batch 120/340] [D loss: 1.607375, acc: 63%] [G loss: 1.988693]\n",
      "[Epoch 105/200] [Batch 140/340] [D loss: 1.622769, acc: 60%] [G loss: 2.222837]\n",
      "[Epoch 105/200] [Batch 160/340] [D loss: 1.643967, acc: 60%] [G loss: 2.081009]\n",
      "[Epoch 105/200] [Batch 180/340] [D loss: 1.617497, acc: 66%] [G loss: 2.264895]\n",
      "[Epoch 105/200] [Batch 200/340] [D loss: 1.711394, acc: 58%] [G loss: 2.135080]\n",
      "[Epoch 105/200] [Batch 220/340] [D loss: 1.659044, acc: 58%] [G loss: 2.269370]\n",
      "[Epoch 105/200] [Batch 240/340] [D loss: 1.671645, acc: 61%] [G loss: 2.045811]\n",
      "[Epoch 105/200] [Batch 260/340] [D loss: 1.670005, acc: 63%] [G loss: 2.165540]\n",
      "[Epoch 105/200] [Batch 280/340] [D loss: 1.691272, acc: 63%] [G loss: 2.110893]\n",
      "[Epoch 105/200] [Batch 300/340] [D loss: 1.640195, acc: 59%] [G loss: 1.938184]\n",
      "[Epoch 105/200] [Batch 320/340] [D loss: 1.648930, acc: 62%] [G loss: 1.901443]\n",
      "[Epoch 106/200] [Batch 0/340] [D loss: 1.714233, acc: 55%] [G loss: 2.080890]\n",
      "[Epoch 106/200] [Batch 20/340] [D loss: 1.721577, acc: 62%] [G loss: 1.938081]\n",
      "[Epoch 106/200] [Batch 40/340] [D loss: 1.710457, acc: 61%] [G loss: 1.941782]\n",
      "[Epoch 106/200] [Batch 60/340] [D loss: 1.636735, acc: 61%] [G loss: 1.929604]\n",
      "[Epoch 106/200] [Batch 80/340] [D loss: 1.677559, acc: 63%] [G loss: 1.969300]\n",
      "[Epoch 106/200] [Batch 100/340] [D loss: 1.715870, acc: 64%] [G loss: 2.114302]\n",
      "[Epoch 106/200] [Batch 120/340] [D loss: 1.656895, acc: 64%] [G loss: 2.057196]\n",
      "[Epoch 106/200] [Batch 140/340] [D loss: 1.689641, acc: 62%] [G loss: 1.842332]\n",
      "[Epoch 106/200] [Batch 160/340] [D loss: 1.700235, acc: 60%] [G loss: 2.080841]\n",
      "[Epoch 106/200] [Batch 180/340] [D loss: 1.595470, acc: 64%] [G loss: 2.314406]\n",
      "[Epoch 106/200] [Batch 200/340] [D loss: 1.642610, acc: 61%] [G loss: 1.836760]\n",
      "[Epoch 106/200] [Batch 220/340] [D loss: 1.710786, acc: 60%] [G loss: 1.815662]\n",
      "[Epoch 106/200] [Batch 240/340] [D loss: 1.661543, acc: 67%] [G loss: 2.060796]\n",
      "[Epoch 106/200] [Batch 260/340] [D loss: 1.631367, acc: 61%] [G loss: 2.496232]\n",
      "[Epoch 106/200] [Batch 280/340] [D loss: 1.660182, acc: 63%] [G loss: 1.919742]\n",
      "[Epoch 106/200] [Batch 300/340] [D loss: 1.614739, acc: 63%] [G loss: 2.249541]\n",
      "[Epoch 106/200] [Batch 320/340] [D loss: 1.703434, acc: 65%] [G loss: 2.062472]\n",
      "[Epoch 107/200] [Batch 0/340] [D loss: 1.685956, acc: 64%] [G loss: 2.128606]\n",
      "[Epoch 107/200] [Batch 20/340] [D loss: 1.720723, acc: 60%] [G loss: 1.946954]\n",
      "[Epoch 107/200] [Batch 40/340] [D loss: 1.651588, acc: 62%] [G loss: 2.137197]\n",
      "[Epoch 107/200] [Batch 60/340] [D loss: 1.733025, acc: 58%] [G loss: 2.154747]\n",
      "[Epoch 107/200] [Batch 80/340] [D loss: 1.714887, acc: 64%] [G loss: 1.971752]\n",
      "[Epoch 107/200] [Batch 100/340] [D loss: 1.639326, acc: 66%] [G loss: 2.202367]\n",
      "[Epoch 107/200] [Batch 120/340] [D loss: 1.659364, acc: 64%] [G loss: 2.030348]\n",
      "[Epoch 107/200] [Batch 140/340] [D loss: 1.689230, acc: 62%] [G loss: 2.312598]\n",
      "[Epoch 107/200] [Batch 160/340] [D loss: 1.613755, acc: 62%] [G loss: 1.984189]\n",
      "[Epoch 107/200] [Batch 180/340] [D loss: 1.684761, acc: 61%] [G loss: 2.185817]\n",
      "[Epoch 107/200] [Batch 200/340] [D loss: 1.674317, acc: 62%] [G loss: 1.939540]\n",
      "[Epoch 107/200] [Batch 220/340] [D loss: 1.683172, acc: 62%] [G loss: 2.045216]\n",
      "[Epoch 107/200] [Batch 240/340] [D loss: 1.641770, acc: 61%] [G loss: 2.028431]\n",
      "[Epoch 107/200] [Batch 260/340] [D loss: 1.657644, acc: 63%] [G loss: 1.988178]\n",
      "[Epoch 107/200] [Batch 280/340] [D loss: 1.654755, acc: 64%] [G loss: 1.922188]\n",
      "[Epoch 107/200] [Batch 300/340] [D loss: 1.649639, acc: 64%] [G loss: 2.113712]\n",
      "[Epoch 107/200] [Batch 320/340] [D loss: 1.651694, acc: 62%] [G loss: 2.086655]\n",
      "[Epoch 108/200] [Batch 0/340] [D loss: 1.693757, acc: 64%] [G loss: 2.031519]\n",
      "[Epoch 108/200] [Batch 20/340] [D loss: 1.689738, acc: 62%] [G loss: 1.956777]\n",
      "[Epoch 108/200] [Batch 40/340] [D loss: 1.643120, acc: 63%] [G loss: 2.202924]\n",
      "[Epoch 108/200] [Batch 60/340] [D loss: 1.625637, acc: 63%] [G loss: 1.856786]\n",
      "[Epoch 108/200] [Batch 80/340] [D loss: 1.697474, acc: 63%] [G loss: 1.888041]\n",
      "[Epoch 108/200] [Batch 100/340] [D loss: 1.723385, acc: 61%] [G loss: 2.006439]\n",
      "[Epoch 108/200] [Batch 120/340] [D loss: 1.673696, acc: 61%] [G loss: 1.978927]\n",
      "[Epoch 108/200] [Batch 140/340] [D loss: 1.690266, acc: 62%] [G loss: 1.911919]\n",
      "[Epoch 108/200] [Batch 160/340] [D loss: 1.698821, acc: 63%] [G loss: 2.300164]\n",
      "[Epoch 108/200] [Batch 180/340] [D loss: 1.703900, acc: 60%] [G loss: 2.031312]\n",
      "[Epoch 108/200] [Batch 200/340] [D loss: 1.642153, acc: 66%] [G loss: 1.953728]\n",
      "[Epoch 108/200] [Batch 220/340] [D loss: 1.652061, acc: 63%] [G loss: 2.168090]\n",
      "[Epoch 108/200] [Batch 240/340] [D loss: 1.674406, acc: 63%] [G loss: 1.837532]\n",
      "[Epoch 108/200] [Batch 260/340] [D loss: 1.655737, acc: 60%] [G loss: 2.318640]\n",
      "[Epoch 108/200] [Batch 280/340] [D loss: 1.665983, acc: 62%] [G loss: 2.047319]\n",
      "[Epoch 108/200] [Batch 300/340] [D loss: 1.669587, acc: 61%] [G loss: 2.096651]\n",
      "[Epoch 108/200] [Batch 320/340] [D loss: 1.625624, acc: 65%] [G loss: 2.080344]\n",
      "[Epoch 109/200] [Batch 0/340] [D loss: 1.735988, acc: 63%] [G loss: 1.838908]\n",
      "[Epoch 109/200] [Batch 20/340] [D loss: 1.639851, acc: 61%] [G loss: 2.071834]\n",
      "[Epoch 109/200] [Batch 40/340] [D loss: 1.678917, acc: 63%] [G loss: 1.822718]\n",
      "[Epoch 109/200] [Batch 60/340] [D loss: 1.624397, acc: 64%] [G loss: 1.967080]\n",
      "[Epoch 109/200] [Batch 80/340] [D loss: 1.635511, acc: 62%] [G loss: 1.890581]\n",
      "[Epoch 109/200] [Batch 100/340] [D loss: 1.669555, acc: 61%] [G loss: 1.987187]\n",
      "[Epoch 109/200] [Batch 120/340] [D loss: 1.663080, acc: 65%] [G loss: 2.014374]\n",
      "[Epoch 109/200] [Batch 140/340] [D loss: 1.702096, acc: 59%] [G loss: 2.041086]\n",
      "[Epoch 109/200] [Batch 160/340] [D loss: 1.681355, acc: 65%] [G loss: 1.873304]\n",
      "[Epoch 109/200] [Batch 180/340] [D loss: 1.694732, acc: 63%] [G loss: 1.939023]\n",
      "[Epoch 109/200] [Batch 200/340] [D loss: 1.640785, acc: 63%] [G loss: 1.890175]\n",
      "[Epoch 109/200] [Batch 220/340] [D loss: 1.684581, acc: 59%] [G loss: 1.936759]\n",
      "[Epoch 109/200] [Batch 240/340] [D loss: 1.639409, acc: 61%] [G loss: 1.952920]\n",
      "[Epoch 109/200] [Batch 260/340] [D loss: 1.669340, acc: 67%] [G loss: 2.102456]\n",
      "[Epoch 109/200] [Batch 280/340] [D loss: 1.639206, acc: 62%] [G loss: 2.081560]\n",
      "[Epoch 109/200] [Batch 300/340] [D loss: 1.658098, acc: 62%] [G loss: 1.900723]\n",
      "[Epoch 109/200] [Batch 320/340] [D loss: 1.759316, acc: 62%] [G loss: 2.216812]\n",
      "[Epoch 110/200] [Batch 0/340] [D loss: 1.632131, acc: 64%] [G loss: 1.964584]\n",
      "[Epoch 110/200] [Batch 20/340] [D loss: 1.744036, acc: 62%] [G loss: 2.072737]\n",
      "[Epoch 110/200] [Batch 40/340] [D loss: 1.760115, acc: 61%] [G loss: 2.201467]\n",
      "[Epoch 110/200] [Batch 60/340] [D loss: 1.700042, acc: 63%] [G loss: 1.937482]\n",
      "[Epoch 110/200] [Batch 80/340] [D loss: 1.702229, acc: 61%] [G loss: 2.166812]\n",
      "[Epoch 110/200] [Batch 100/340] [D loss: 1.620106, acc: 62%] [G loss: 2.024045]\n",
      "[Epoch 110/200] [Batch 120/340] [D loss: 1.750917, acc: 63%] [G loss: 2.089687]\n",
      "[Epoch 110/200] [Batch 140/340] [D loss: 1.696776, acc: 65%] [G loss: 2.228593]\n",
      "[Epoch 110/200] [Batch 160/340] [D loss: 1.704170, acc: 57%] [G loss: 2.184430]\n",
      "[Epoch 110/200] [Batch 180/340] [D loss: 1.626791, acc: 61%] [G loss: 1.885124]\n",
      "[Epoch 110/200] [Batch 200/340] [D loss: 1.667254, acc: 63%] [G loss: 2.214254]\n",
      "[Epoch 110/200] [Batch 220/340] [D loss: 1.693752, acc: 58%] [G loss: 1.927412]\n",
      "[Epoch 110/200] [Batch 240/340] [D loss: 1.719512, acc: 61%] [G loss: 2.319010]\n",
      "[Epoch 110/200] [Batch 260/340] [D loss: 1.704255, acc: 64%] [G loss: 2.102149]\n",
      "[Epoch 110/200] [Batch 280/340] [D loss: 1.703039, acc: 59%] [G loss: 2.008217]\n",
      "[Epoch 110/200] [Batch 300/340] [D loss: 1.703561, acc: 64%] [G loss: 1.971735]\n",
      "[Epoch 110/200] [Batch 320/340] [D loss: 1.641679, acc: 65%] [G loss: 2.137393]\n",
      "[Epoch 111/200] [Batch 0/340] [D loss: 1.633078, acc: 66%] [G loss: 2.016745]\n",
      "[Epoch 111/200] [Batch 20/340] [D loss: 1.717001, acc: 63%] [G loss: 2.153198]\n",
      "[Epoch 111/200] [Batch 40/340] [D loss: 1.667050, acc: 63%] [G loss: 1.903237]\n",
      "[Epoch 111/200] [Batch 60/340] [D loss: 1.676551, acc: 62%] [G loss: 2.011113]\n",
      "[Epoch 111/200] [Batch 80/340] [D loss: 1.705630, acc: 64%] [G loss: 1.939041]\n",
      "[Epoch 111/200] [Batch 100/340] [D loss: 1.684172, acc: 61%] [G loss: 1.884448]\n",
      "[Epoch 111/200] [Batch 120/340] [D loss: 1.671529, acc: 64%] [G loss: 2.121650]\n",
      "[Epoch 111/200] [Batch 140/340] [D loss: 1.646769, acc: 67%] [G loss: 1.928180]\n",
      "[Epoch 111/200] [Batch 160/340] [D loss: 1.641380, acc: 62%] [G loss: 1.939150]\n",
      "[Epoch 111/200] [Batch 180/340] [D loss: 1.692793, acc: 62%] [G loss: 1.959452]\n",
      "[Epoch 111/200] [Batch 200/340] [D loss: 1.670054, acc: 67%] [G loss: 2.048281]\n",
      "[Epoch 111/200] [Batch 220/340] [D loss: 1.697548, acc: 63%] [G loss: 1.885043]\n",
      "[Epoch 111/200] [Batch 240/340] [D loss: 1.643653, acc: 60%] [G loss: 2.114872]\n",
      "[Epoch 111/200] [Batch 260/340] [D loss: 1.713992, acc: 67%] [G loss: 2.188792]\n",
      "[Epoch 111/200] [Batch 280/340] [D loss: 1.657530, acc: 64%] [G loss: 1.854956]\n",
      "[Epoch 111/200] [Batch 300/340] [D loss: 1.665699, acc: 59%] [G loss: 2.199830]\n",
      "[Epoch 111/200] [Batch 320/340] [D loss: 1.694049, acc: 63%] [G loss: 1.987181]\n",
      "[Epoch 112/200] [Batch 0/340] [D loss: 1.615965, acc: 67%] [G loss: 2.171434]\n",
      "[Epoch 112/200] [Batch 20/340] [D loss: 1.661094, acc: 64%] [G loss: 1.826696]\n",
      "[Epoch 112/200] [Batch 40/340] [D loss: 1.626113, acc: 63%] [G loss: 1.963251]\n",
      "[Epoch 112/200] [Batch 60/340] [D loss: 1.608519, acc: 65%] [G loss: 2.176398]\n",
      "[Epoch 112/200] [Batch 80/340] [D loss: 1.611470, acc: 65%] [G loss: 2.270130]\n",
      "[Epoch 112/200] [Batch 100/340] [D loss: 1.729231, acc: 62%] [G loss: 2.179456]\n",
      "[Epoch 112/200] [Batch 120/340] [D loss: 1.748997, acc: 58%] [G loss: 2.307879]\n",
      "[Epoch 112/200] [Batch 140/340] [D loss: 1.658034, acc: 64%] [G loss: 2.066011]\n",
      "[Epoch 112/200] [Batch 160/340] [D loss: 1.687399, acc: 60%] [G loss: 2.017169]\n",
      "[Epoch 112/200] [Batch 180/340] [D loss: 1.608414, acc: 63%] [G loss: 2.241206]\n",
      "[Epoch 112/200] [Batch 200/340] [D loss: 1.744552, acc: 58%] [G loss: 1.938198]\n",
      "[Epoch 112/200] [Batch 220/340] [D loss: 1.642939, acc: 66%] [G loss: 2.115556]\n",
      "[Epoch 112/200] [Batch 240/340] [D loss: 1.675120, acc: 63%] [G loss: 2.033946]\n",
      "[Epoch 112/200] [Batch 260/340] [D loss: 1.588840, acc: 67%] [G loss: 2.093993]\n",
      "[Epoch 112/200] [Batch 280/340] [D loss: 1.691302, acc: 60%] [G loss: 1.958411]\n",
      "[Epoch 112/200] [Batch 300/340] [D loss: 1.643452, acc: 63%] [G loss: 1.994049]\n",
      "[Epoch 112/200] [Batch 320/340] [D loss: 1.694225, acc: 62%] [G loss: 1.893623]\n",
      "[Epoch 113/200] [Batch 0/340] [D loss: 1.670519, acc: 65%] [G loss: 1.806790]\n",
      "[Epoch 113/200] [Batch 20/340] [D loss: 1.777541, acc: 64%] [G loss: 1.856224]\n",
      "[Epoch 113/200] [Batch 40/340] [D loss: 1.622136, acc: 64%] [G loss: 1.966252]\n",
      "[Epoch 113/200] [Batch 60/340] [D loss: 1.653589, acc: 64%] [G loss: 1.995048]\n",
      "[Epoch 113/200] [Batch 80/340] [D loss: 1.711090, acc: 63%] [G loss: 2.170846]\n",
      "[Epoch 113/200] [Batch 100/340] [D loss: 1.665853, acc: 60%] [G loss: 2.066237]\n",
      "[Epoch 113/200] [Batch 120/340] [D loss: 1.680800, acc: 63%] [G loss: 2.039989]\n",
      "[Epoch 113/200] [Batch 140/340] [D loss: 1.685371, acc: 60%] [G loss: 2.060311]\n",
      "[Epoch 113/200] [Batch 160/340] [D loss: 1.680801, acc: 59%] [G loss: 1.897343]\n",
      "[Epoch 113/200] [Batch 180/340] [D loss: 1.645720, acc: 61%] [G loss: 1.995603]\n",
      "[Epoch 113/200] [Batch 200/340] [D loss: 1.693523, acc: 63%] [G loss: 2.054399]\n",
      "[Epoch 113/200] [Batch 220/340] [D loss: 1.697652, acc: 63%] [G loss: 1.873407]\n",
      "[Epoch 113/200] [Batch 240/340] [D loss: 1.710069, acc: 59%] [G loss: 2.554648]\n",
      "[Epoch 113/200] [Batch 260/340] [D loss: 1.739866, acc: 61%] [G loss: 2.024891]\n",
      "[Epoch 113/200] [Batch 280/340] [D loss: 1.638092, acc: 66%] [G loss: 2.164820]\n",
      "[Epoch 113/200] [Batch 300/340] [D loss: 1.641795, acc: 65%] [G loss: 2.220581]\n",
      "[Epoch 113/200] [Batch 320/340] [D loss: 1.622484, acc: 63%] [G loss: 2.127297]\n",
      "[Epoch 114/200] [Batch 0/340] [D loss: 1.665366, acc: 63%] [G loss: 1.859015]\n",
      "[Epoch 114/200] [Batch 20/340] [D loss: 1.712581, acc: 62%] [G loss: 2.063034]\n",
      "[Epoch 114/200] [Batch 40/340] [D loss: 1.675833, acc: 62%] [G loss: 2.169985]\n",
      "[Epoch 114/200] [Batch 60/340] [D loss: 1.704435, acc: 62%] [G loss: 1.989629]\n",
      "[Epoch 114/200] [Batch 80/340] [D loss: 1.634022, acc: 63%] [G loss: 2.087786]\n",
      "[Epoch 114/200] [Batch 100/340] [D loss: 1.588327, acc: 65%] [G loss: 2.109572]\n",
      "[Epoch 114/200] [Batch 120/340] [D loss: 1.710536, acc: 64%] [G loss: 2.368454]\n",
      "[Epoch 114/200] [Batch 140/340] [D loss: 1.639866, acc: 62%] [G loss: 1.930162]\n",
      "[Epoch 114/200] [Batch 160/340] [D loss: 1.717276, acc: 66%] [G loss: 1.970167]\n",
      "[Epoch 114/200] [Batch 180/340] [D loss: 1.606768, acc: 63%] [G loss: 1.935726]\n",
      "[Epoch 114/200] [Batch 200/340] [D loss: 1.648889, acc: 60%] [G loss: 2.187126]\n",
      "[Epoch 114/200] [Batch 220/340] [D loss: 1.662482, acc: 65%] [G loss: 1.826915]\n",
      "[Epoch 114/200] [Batch 240/340] [D loss: 1.598002, acc: 63%] [G loss: 2.123912]\n",
      "[Epoch 114/200] [Batch 260/340] [D loss: 1.666551, acc: 61%] [G loss: 2.082195]\n",
      "[Epoch 114/200] [Batch 280/340] [D loss: 1.662695, acc: 64%] [G loss: 2.084538]\n",
      "[Epoch 114/200] [Batch 300/340] [D loss: 1.654015, acc: 65%] [G loss: 2.193329]\n",
      "[Epoch 114/200] [Batch 320/340] [D loss: 1.765594, acc: 62%] [G loss: 1.947152]\n",
      "[Epoch 115/200] [Batch 0/340] [D loss: 1.675662, acc: 65%] [G loss: 1.811074]\n",
      "[Epoch 115/200] [Batch 20/340] [D loss: 1.698444, acc: 61%] [G loss: 2.432428]\n",
      "[Epoch 115/200] [Batch 40/340] [D loss: 1.697990, acc: 61%] [G loss: 2.095371]\n",
      "[Epoch 115/200] [Batch 60/340] [D loss: 1.626373, acc: 64%] [G loss: 2.049438]\n",
      "[Epoch 115/200] [Batch 80/340] [D loss: 1.705694, acc: 64%] [G loss: 1.888830]\n",
      "[Epoch 115/200] [Batch 100/340] [D loss: 1.664238, acc: 63%] [G loss: 1.824951]\n",
      "[Epoch 115/200] [Batch 120/340] [D loss: 1.645369, acc: 66%] [G loss: 1.968078]\n",
      "[Epoch 115/200] [Batch 140/340] [D loss: 1.607784, acc: 66%] [G loss: 1.887851]\n",
      "[Epoch 115/200] [Batch 160/340] [D loss: 1.674392, acc: 61%] [G loss: 2.152102]\n",
      "[Epoch 115/200] [Batch 180/340] [D loss: 1.715013, acc: 64%] [G loss: 2.317403]\n",
      "[Epoch 115/200] [Batch 200/340] [D loss: 1.669579, acc: 65%] [G loss: 2.018421]\n",
      "[Epoch 115/200] [Batch 220/340] [D loss: 1.708826, acc: 66%] [G loss: 1.896482]\n",
      "[Epoch 115/200] [Batch 240/340] [D loss: 1.671351, acc: 63%] [G loss: 2.091921]\n",
      "[Epoch 115/200] [Batch 260/340] [D loss: 1.622025, acc: 64%] [G loss: 2.182521]\n",
      "[Epoch 115/200] [Batch 280/340] [D loss: 1.620437, acc: 66%] [G loss: 1.777781]\n",
      "[Epoch 115/200] [Batch 300/340] [D loss: 1.662714, acc: 66%] [G loss: 2.216463]\n",
      "[Epoch 115/200] [Batch 320/340] [D loss: 1.684567, acc: 66%] [G loss: 1.879152]\n",
      "[Epoch 116/200] [Batch 0/340] [D loss: 1.679870, acc: 65%] [G loss: 1.861855]\n",
      "[Epoch 116/200] [Batch 20/340] [D loss: 1.683315, acc: 63%] [G loss: 2.058352]\n",
      "[Epoch 116/200] [Batch 40/340] [D loss: 1.704229, acc: 64%] [G loss: 1.940403]\n",
      "[Epoch 116/200] [Batch 60/340] [D loss: 1.678753, acc: 63%] [G loss: 1.805849]\n",
      "[Epoch 116/200] [Batch 80/340] [D loss: 1.661731, acc: 64%] [G loss: 2.164214]\n",
      "[Epoch 116/200] [Batch 100/340] [D loss: 1.665880, acc: 65%] [G loss: 1.845828]\n",
      "[Epoch 116/200] [Batch 120/340] [D loss: 1.691726, acc: 60%] [G loss: 2.190842]\n",
      "[Epoch 116/200] [Batch 140/340] [D loss: 1.701631, acc: 63%] [G loss: 1.948850]\n",
      "[Epoch 116/200] [Batch 160/340] [D loss: 1.677281, acc: 63%] [G loss: 2.173032]\n",
      "[Epoch 116/200] [Batch 180/340] [D loss: 1.710013, acc: 61%] [G loss: 1.846950]\n",
      "[Epoch 116/200] [Batch 200/340] [D loss: 1.691279, acc: 66%] [G loss: 2.225499]\n",
      "[Epoch 116/200] [Batch 220/340] [D loss: 1.648737, acc: 65%] [G loss: 2.089680]\n",
      "[Epoch 116/200] [Batch 240/340] [D loss: 1.710156, acc: 62%] [G loss: 2.498848]\n",
      "[Epoch 116/200] [Batch 260/340] [D loss: 1.743104, acc: 64%] [G loss: 2.095108]\n",
      "[Epoch 116/200] [Batch 280/340] [D loss: 1.649950, acc: 64%] [G loss: 2.014035]\n",
      "[Epoch 116/200] [Batch 300/340] [D loss: 1.697567, acc: 65%] [G loss: 1.950382]\n",
      "[Epoch 116/200] [Batch 320/340] [D loss: 1.673075, acc: 63%] [G loss: 1.940125]\n",
      "[Epoch 117/200] [Batch 0/340] [D loss: 1.689520, acc: 65%] [G loss: 1.945387]\n",
      "[Epoch 117/200] [Batch 20/340] [D loss: 1.756590, acc: 63%] [G loss: 2.344860]\n",
      "[Epoch 117/200] [Batch 40/340] [D loss: 1.583519, acc: 64%] [G loss: 2.259634]\n",
      "[Epoch 117/200] [Batch 60/340] [D loss: 1.627840, acc: 63%] [G loss: 1.783860]\n",
      "[Epoch 117/200] [Batch 80/340] [D loss: 1.653030, acc: 64%] [G loss: 1.998911]\n",
      "[Epoch 117/200] [Batch 100/340] [D loss: 1.614676, acc: 65%] [G loss: 1.884972]\n",
      "[Epoch 117/200] [Batch 120/340] [D loss: 1.719457, acc: 68%] [G loss: 2.174419]\n",
      "[Epoch 117/200] [Batch 140/340] [D loss: 1.631582, acc: 65%] [G loss: 2.293852]\n",
      "[Epoch 117/200] [Batch 160/340] [D loss: 1.650993, acc: 65%] [G loss: 2.118648]\n",
      "[Epoch 117/200] [Batch 180/340] [D loss: 1.679635, acc: 67%] [G loss: 2.328162]\n",
      "[Epoch 117/200] [Batch 200/340] [D loss: 1.644140, acc: 66%] [G loss: 2.371352]\n",
      "[Epoch 117/200] [Batch 220/340] [D loss: 1.682450, acc: 66%] [G loss: 1.976495]\n",
      "[Epoch 117/200] [Batch 240/340] [D loss: 1.648714, acc: 64%] [G loss: 2.081186]\n",
      "[Epoch 117/200] [Batch 260/340] [D loss: 1.665448, acc: 65%] [G loss: 2.122216]\n",
      "[Epoch 117/200] [Batch 280/340] [D loss: 1.702015, acc: 64%] [G loss: 1.973607]\n",
      "[Epoch 117/200] [Batch 300/340] [D loss: 1.678995, acc: 62%] [G loss: 2.041648]\n",
      "[Epoch 117/200] [Batch 320/340] [D loss: 1.718357, acc: 64%] [G loss: 2.043042]\n",
      "[Epoch 118/200] [Batch 0/340] [D loss: 1.643155, acc: 68%] [G loss: 2.210852]\n",
      "[Epoch 118/200] [Batch 20/340] [D loss: 1.710887, acc: 65%] [G loss: 2.233814]\n",
      "[Epoch 118/200] [Batch 40/340] [D loss: 1.677129, acc: 67%] [G loss: 2.157428]\n",
      "[Epoch 118/200] [Batch 60/340] [D loss: 1.679955, acc: 61%] [G loss: 1.866683]\n",
      "[Epoch 118/200] [Batch 80/340] [D loss: 1.645467, acc: 64%] [G loss: 1.957086]\n",
      "[Epoch 118/200] [Batch 100/340] [D loss: 1.676408, acc: 64%] [G loss: 1.904128]\n",
      "[Epoch 118/200] [Batch 120/340] [D loss: 1.666256, acc: 64%] [G loss: 2.002393]\n",
      "[Epoch 118/200] [Batch 140/340] [D loss: 1.676141, acc: 64%] [G loss: 1.891402]\n",
      "[Epoch 118/200] [Batch 160/340] [D loss: 1.739521, acc: 64%] [G loss: 1.989384]\n",
      "[Epoch 118/200] [Batch 180/340] [D loss: 1.673465, acc: 66%] [G loss: 1.835027]\n",
      "[Epoch 118/200] [Batch 200/340] [D loss: 1.607382, acc: 65%] [G loss: 2.017329]\n",
      "[Epoch 118/200] [Batch 220/340] [D loss: 1.684530, acc: 62%] [G loss: 2.071221]\n",
      "[Epoch 118/200] [Batch 240/340] [D loss: 1.713713, acc: 65%] [G loss: 2.207168]\n",
      "[Epoch 118/200] [Batch 260/340] [D loss: 1.685261, acc: 63%] [G loss: 2.213323]\n",
      "[Epoch 118/200] [Batch 280/340] [D loss: 1.678399, acc: 62%] [G loss: 1.911964]\n",
      "[Epoch 118/200] [Batch 300/340] [D loss: 1.726518, acc: 65%] [G loss: 1.857757]\n",
      "[Epoch 118/200] [Batch 320/340] [D loss: 1.650747, acc: 65%] [G loss: 1.980210]\n",
      "[Epoch 119/200] [Batch 0/340] [D loss: 1.639965, acc: 63%] [G loss: 1.820603]\n",
      "[Epoch 119/200] [Batch 20/340] [D loss: 1.687760, acc: 60%] [G loss: 2.029368]\n",
      "[Epoch 119/200] [Batch 40/340] [D loss: 1.673506, acc: 63%] [G loss: 2.203263]\n",
      "[Epoch 119/200] [Batch 60/340] [D loss: 1.694135, acc: 66%] [G loss: 1.874830]\n",
      "[Epoch 119/200] [Batch 80/340] [D loss: 1.644612, acc: 65%] [G loss: 2.010304]\n",
      "[Epoch 119/200] [Batch 100/340] [D loss: 1.628064, acc: 64%] [G loss: 1.849005]\n",
      "[Epoch 119/200] [Batch 120/340] [D loss: 1.612871, acc: 64%] [G loss: 2.057980]\n",
      "[Epoch 119/200] [Batch 140/340] [D loss: 1.667943, acc: 64%] [G loss: 2.034744]\n",
      "[Epoch 119/200] [Batch 160/340] [D loss: 1.682438, acc: 66%] [G loss: 2.006074]\n",
      "[Epoch 119/200] [Batch 180/340] [D loss: 1.679617, acc: 68%] [G loss: 1.684194]\n",
      "[Epoch 119/200] [Batch 200/340] [D loss: 1.653068, acc: 62%] [G loss: 2.008407]\n",
      "[Epoch 119/200] [Batch 220/340] [D loss: 1.646580, acc: 63%] [G loss: 2.201961]\n",
      "[Epoch 119/200] [Batch 240/340] [D loss: 1.618430, acc: 66%] [G loss: 2.082139]\n",
      "[Epoch 119/200] [Batch 260/340] [D loss: 1.634047, acc: 63%] [G loss: 1.930936]\n",
      "[Epoch 119/200] [Batch 280/340] [D loss: 1.634037, acc: 63%] [G loss: 1.998272]\n",
      "[Epoch 119/200] [Batch 300/340] [D loss: 1.689019, acc: 67%] [G loss: 2.298451]\n",
      "[Epoch 119/200] [Batch 320/340] [D loss: 1.641940, acc: 67%] [G loss: 2.070292]\n",
      "[Epoch 120/200] [Batch 0/340] [D loss: 1.621017, acc: 58%] [G loss: 2.147691]\n",
      "[Epoch 120/200] [Batch 20/340] [D loss: 1.737162, acc: 59%] [G loss: 1.714876]\n",
      "[Epoch 120/200] [Batch 40/340] [D loss: 1.691963, acc: 65%] [G loss: 1.952268]\n",
      "[Epoch 120/200] [Batch 60/340] [D loss: 1.624319, acc: 69%] [G loss: 2.029407]\n",
      "[Epoch 120/200] [Batch 80/340] [D loss: 1.727160, acc: 61%] [G loss: 1.816770]\n",
      "[Epoch 120/200] [Batch 100/340] [D loss: 1.663719, acc: 60%] [G loss: 1.945625]\n",
      "[Epoch 120/200] [Batch 120/340] [D loss: 1.692572, acc: 65%] [G loss: 1.826225]\n",
      "[Epoch 120/200] [Batch 140/340] [D loss: 1.709209, acc: 66%] [G loss: 2.069227]\n",
      "[Epoch 120/200] [Batch 160/340] [D loss: 1.752984, acc: 61%] [G loss: 1.920578]\n",
      "[Epoch 120/200] [Batch 180/340] [D loss: 1.737135, acc: 63%] [G loss: 2.008709]\n",
      "[Epoch 120/200] [Batch 200/340] [D loss: 1.641858, acc: 64%] [G loss: 2.016682]\n",
      "[Epoch 120/200] [Batch 220/340] [D loss: 1.633707, acc: 59%] [G loss: 2.162372]\n",
      "[Epoch 120/200] [Batch 240/340] [D loss: 1.703949, acc: 66%] [G loss: 1.709962]\n",
      "[Epoch 120/200] [Batch 260/340] [D loss: 1.638018, acc: 67%] [G loss: 1.993826]\n",
      "[Epoch 120/200] [Batch 280/340] [D loss: 1.659428, acc: 63%] [G loss: 2.037201]\n",
      "[Epoch 120/200] [Batch 300/340] [D loss: 1.659632, acc: 62%] [G loss: 1.958126]\n",
      "[Epoch 120/200] [Batch 320/340] [D loss: 1.700973, acc: 64%] [G loss: 2.042413]\n",
      "[Epoch 121/200] [Batch 0/340] [D loss: 1.625590, acc: 65%] [G loss: 1.984991]\n",
      "[Epoch 121/200] [Batch 20/340] [D loss: 1.706218, acc: 66%] [G loss: 2.183183]\n",
      "[Epoch 121/200] [Batch 40/340] [D loss: 1.687390, acc: 61%] [G loss: 1.859034]\n",
      "[Epoch 121/200] [Batch 60/340] [D loss: 1.653269, acc: 64%] [G loss: 2.260514]\n",
      "[Epoch 121/200] [Batch 80/340] [D loss: 1.672505, acc: 60%] [G loss: 2.146205]\n",
      "[Epoch 121/200] [Batch 100/340] [D loss: 1.645514, acc: 67%] [G loss: 1.994484]\n",
      "[Epoch 121/200] [Batch 120/340] [D loss: 1.681943, acc: 64%] [G loss: 1.824079]\n",
      "[Epoch 121/200] [Batch 140/340] [D loss: 1.651281, acc: 65%] [G loss: 1.862700]\n",
      "[Epoch 121/200] [Batch 160/340] [D loss: 1.631486, acc: 66%] [G loss: 2.114790]\n",
      "[Epoch 121/200] [Batch 180/340] [D loss: 1.707612, acc: 64%] [G loss: 2.056466]\n",
      "[Epoch 121/200] [Batch 200/340] [D loss: 1.626114, acc: 66%] [G loss: 1.976980]\n",
      "[Epoch 121/200] [Batch 220/340] [D loss: 1.670646, acc: 67%] [G loss: 2.010786]\n",
      "[Epoch 121/200] [Batch 240/340] [D loss: 1.627709, acc: 66%] [G loss: 1.929276]\n",
      "[Epoch 121/200] [Batch 260/340] [D loss: 1.669702, acc: 67%] [G loss: 1.972352]\n",
      "[Epoch 121/200] [Batch 280/340] [D loss: 1.689754, acc: 66%] [G loss: 2.041790]\n",
      "[Epoch 121/200] [Batch 300/340] [D loss: 1.654908, acc: 63%] [G loss: 2.176414]\n",
      "[Epoch 121/200] [Batch 320/340] [D loss: 1.667192, acc: 66%] [G loss: 2.001152]\n",
      "[Epoch 122/200] [Batch 0/340] [D loss: 1.625651, acc: 63%] [G loss: 2.180666]\n",
      "[Epoch 122/200] [Batch 20/340] [D loss: 1.690557, acc: 64%] [G loss: 1.947995]\n",
      "[Epoch 122/200] [Batch 40/340] [D loss: 1.652579, acc: 64%] [G loss: 1.965164]\n",
      "[Epoch 122/200] [Batch 60/340] [D loss: 1.702831, acc: 61%] [G loss: 2.083153]\n",
      "[Epoch 122/200] [Batch 80/340] [D loss: 1.617752, acc: 64%] [G loss: 1.986682]\n",
      "[Epoch 122/200] [Batch 100/340] [D loss: 1.603120, acc: 66%] [G loss: 1.727012]\n",
      "[Epoch 122/200] [Batch 120/340] [D loss: 1.610836, acc: 68%] [G loss: 1.781405]\n",
      "[Epoch 122/200] [Batch 140/340] [D loss: 1.586913, acc: 67%] [G loss: 2.106138]\n",
      "[Epoch 122/200] [Batch 160/340] [D loss: 1.736411, acc: 65%] [G loss: 1.999475]\n",
      "[Epoch 122/200] [Batch 180/340] [D loss: 1.648520, acc: 68%] [G loss: 2.030712]\n",
      "[Epoch 122/200] [Batch 200/340] [D loss: 1.669802, acc: 64%] [G loss: 2.079033]\n",
      "[Epoch 122/200] [Batch 220/340] [D loss: 1.675726, acc: 59%] [G loss: 2.015511]\n",
      "[Epoch 122/200] [Batch 240/340] [D loss: 1.569336, acc: 64%] [G loss: 2.031519]\n",
      "[Epoch 122/200] [Batch 260/340] [D loss: 1.646654, acc: 68%] [G loss: 1.945230]\n",
      "[Epoch 122/200] [Batch 280/340] [D loss: 1.697374, acc: 66%] [G loss: 2.104923]\n",
      "[Epoch 122/200] [Batch 300/340] [D loss: 1.695577, acc: 64%] [G loss: 1.952848]\n",
      "[Epoch 122/200] [Batch 320/340] [D loss: 1.651660, acc: 65%] [G loss: 2.072827]\n",
      "[Epoch 123/200] [Batch 0/340] [D loss: 1.705245, acc: 69%] [G loss: 1.838621]\n",
      "[Epoch 123/200] [Batch 20/340] [D loss: 1.699926, acc: 68%] [G loss: 2.353047]\n",
      "[Epoch 123/200] [Batch 40/340] [D loss: 1.582657, acc: 69%] [G loss: 2.115754]\n",
      "[Epoch 123/200] [Batch 60/340] [D loss: 1.660941, acc: 64%] [G loss: 1.863554]\n",
      "[Epoch 123/200] [Batch 80/340] [D loss: 1.615362, acc: 66%] [G loss: 2.068321]\n",
      "[Epoch 123/200] [Batch 100/340] [D loss: 1.633134, acc: 65%] [G loss: 1.990880]\n",
      "[Epoch 123/200] [Batch 120/340] [D loss: 1.654738, acc: 65%] [G loss: 2.182821]\n",
      "[Epoch 123/200] [Batch 140/340] [D loss: 1.697230, acc: 63%] [G loss: 2.069932]\n",
      "[Epoch 123/200] [Batch 160/340] [D loss: 1.649914, acc: 69%] [G loss: 2.079578]\n",
      "[Epoch 123/200] [Batch 180/340] [D loss: 1.629250, acc: 67%] [G loss: 1.974452]\n",
      "[Epoch 123/200] [Batch 200/340] [D loss: 1.638232, acc: 65%] [G loss: 2.064396]\n",
      "[Epoch 123/200] [Batch 220/340] [D loss: 1.661462, acc: 64%] [G loss: 2.082008]\n",
      "[Epoch 123/200] [Batch 240/340] [D loss: 1.754522, acc: 64%] [G loss: 2.196742]\n",
      "[Epoch 123/200] [Batch 260/340] [D loss: 1.670332, acc: 66%] [G loss: 1.888701]\n",
      "[Epoch 123/200] [Batch 280/340] [D loss: 1.725352, acc: 64%] [G loss: 2.239902]\n",
      "[Epoch 123/200] [Batch 300/340] [D loss: 1.711689, acc: 64%] [G loss: 1.818172]\n",
      "[Epoch 123/200] [Batch 320/340] [D loss: 1.619253, acc: 65%] [G loss: 1.976453]\n",
      "[Epoch 124/200] [Batch 0/340] [D loss: 1.657729, acc: 64%] [G loss: 1.921623]\n",
      "[Epoch 124/200] [Batch 20/340] [D loss: 1.647652, acc: 66%] [G loss: 1.970948]\n",
      "[Epoch 124/200] [Batch 40/340] [D loss: 1.647917, acc: 65%] [G loss: 2.045330]\n",
      "[Epoch 124/200] [Batch 60/340] [D loss: 1.678595, acc: 67%] [G loss: 2.395390]\n",
      "[Epoch 124/200] [Batch 80/340] [D loss: 1.660157, acc: 65%] [G loss: 2.071145]\n",
      "[Epoch 124/200] [Batch 100/340] [D loss: 1.579132, acc: 64%] [G loss: 2.135668]\n",
      "[Epoch 124/200] [Batch 120/340] [D loss: 1.672731, acc: 64%] [G loss: 1.886060]\n",
      "[Epoch 124/200] [Batch 140/340] [D loss: 1.701775, acc: 67%] [G loss: 2.147719]\n",
      "[Epoch 124/200] [Batch 160/340] [D loss: 1.612419, acc: 65%] [G loss: 1.990793]\n",
      "[Epoch 124/200] [Batch 180/340] [D loss: 1.642323, acc: 65%] [G loss: 2.299351]\n",
      "[Epoch 124/200] [Batch 200/340] [D loss: 1.607220, acc: 66%] [G loss: 1.860493]\n",
      "[Epoch 124/200] [Batch 220/340] [D loss: 1.726326, acc: 65%] [G loss: 2.287722]\n",
      "[Epoch 124/200] [Batch 240/340] [D loss: 1.659139, acc: 67%] [G loss: 2.019480]\n",
      "[Epoch 124/200] [Batch 260/340] [D loss: 1.682386, acc: 64%] [G loss: 1.956316]\n",
      "[Epoch 124/200] [Batch 280/340] [D loss: 1.693959, acc: 63%] [G loss: 1.892249]\n",
      "[Epoch 124/200] [Batch 300/340] [D loss: 1.592125, acc: 68%] [G loss: 2.192862]\n",
      "[Epoch 124/200] [Batch 320/340] [D loss: 1.717793, acc: 65%] [G loss: 2.180576]\n",
      "[Epoch 125/200] [Batch 0/340] [D loss: 1.706201, acc: 63%] [G loss: 2.244655]\n",
      "[Epoch 125/200] [Batch 20/340] [D loss: 1.648700, acc: 67%] [G loss: 1.771283]\n",
      "[Epoch 125/200] [Batch 40/340] [D loss: 1.649226, acc: 68%] [G loss: 1.867773]\n",
      "[Epoch 125/200] [Batch 60/340] [D loss: 1.640630, acc: 65%] [G loss: 2.000133]\n",
      "[Epoch 125/200] [Batch 80/340] [D loss: 1.671991, acc: 69%] [G loss: 1.826000]\n",
      "[Epoch 125/200] [Batch 100/340] [D loss: 1.695168, acc: 64%] [G loss: 1.961981]\n",
      "[Epoch 125/200] [Batch 120/340] [D loss: 1.606208, acc: 70%] [G loss: 2.043550]\n",
      "[Epoch 125/200] [Batch 140/340] [D loss: 1.671556, acc: 66%] [G loss: 1.726483]\n",
      "[Epoch 125/200] [Batch 160/340] [D loss: 1.708219, acc: 62%] [G loss: 1.970057]\n",
      "[Epoch 125/200] [Batch 180/340] [D loss: 1.701601, acc: 59%] [G loss: 2.118644]\n",
      "[Epoch 125/200] [Batch 200/340] [D loss: 1.694508, acc: 62%] [G loss: 1.854737]\n",
      "[Epoch 125/200] [Batch 220/340] [D loss: 1.705356, acc: 65%] [G loss: 1.906607]\n",
      "[Epoch 125/200] [Batch 240/340] [D loss: 1.674443, acc: 65%] [G loss: 2.030349]\n",
      "[Epoch 125/200] [Batch 260/340] [D loss: 1.670085, acc: 65%] [G loss: 1.914941]\n",
      "[Epoch 125/200] [Batch 280/340] [D loss: 1.584148, acc: 65%] [G loss: 1.960044]\n",
      "[Epoch 125/200] [Batch 300/340] [D loss: 1.579675, acc: 66%] [G loss: 2.115684]\n",
      "[Epoch 125/200] [Batch 320/340] [D loss: 1.716091, acc: 65%] [G loss: 2.258254]\n",
      "[Epoch 126/200] [Batch 0/340] [D loss: 1.675550, acc: 66%] [G loss: 1.932168]\n",
      "[Epoch 126/200] [Batch 20/340] [D loss: 1.671003, acc: 65%] [G loss: 1.878983]\n",
      "[Epoch 126/200] [Batch 40/340] [D loss: 1.685759, acc: 65%] [G loss: 1.740409]\n",
      "[Epoch 126/200] [Batch 60/340] [D loss: 1.726862, acc: 64%] [G loss: 2.210052]\n",
      "[Epoch 126/200] [Batch 80/340] [D loss: 1.738049, acc: 61%] [G loss: 2.251649]\n",
      "[Epoch 126/200] [Batch 100/340] [D loss: 1.683152, acc: 65%] [G loss: 1.748420]\n",
      "[Epoch 126/200] [Batch 120/340] [D loss: 1.696492, acc: 61%] [G loss: 1.965131]\n",
      "[Epoch 126/200] [Batch 140/340] [D loss: 1.628305, acc: 67%] [G loss: 2.039436]\n",
      "[Epoch 126/200] [Batch 160/340] [D loss: 1.685946, acc: 65%] [G loss: 2.059326]\n",
      "[Epoch 126/200] [Batch 180/340] [D loss: 1.683654, acc: 65%] [G loss: 1.977464]\n",
      "[Epoch 126/200] [Batch 200/340] [D loss: 1.681696, acc: 66%] [G loss: 1.796839]\n",
      "[Epoch 126/200] [Batch 220/340] [D loss: 1.686808, acc: 66%] [G loss: 1.860322]\n",
      "[Epoch 126/200] [Batch 240/340] [D loss: 1.641924, acc: 66%] [G loss: 2.013963]\n",
      "[Epoch 126/200] [Batch 260/340] [D loss: 1.638771, acc: 68%] [G loss: 2.123170]\n",
      "[Epoch 126/200] [Batch 280/340] [D loss: 1.674765, acc: 64%] [G loss: 1.899454]\n",
      "[Epoch 126/200] [Batch 300/340] [D loss: 1.683522, acc: 64%] [G loss: 1.992887]\n",
      "[Epoch 126/200] [Batch 320/340] [D loss: 1.704011, acc: 64%] [G loss: 1.972049]\n",
      "[Epoch 127/200] [Batch 0/340] [D loss: 1.742382, acc: 64%] [G loss: 2.050189]\n",
      "[Epoch 127/200] [Batch 20/340] [D loss: 1.700266, acc: 68%] [G loss: 1.850338]\n",
      "[Epoch 127/200] [Batch 40/340] [D loss: 1.684438, acc: 64%] [G loss: 2.064251]\n",
      "[Epoch 127/200] [Batch 60/340] [D loss: 1.692696, acc: 65%] [G loss: 2.050477]\n",
      "[Epoch 127/200] [Batch 80/340] [D loss: 1.672671, acc: 65%] [G loss: 1.928077]\n",
      "[Epoch 127/200] [Batch 100/340] [D loss: 1.646808, acc: 63%] [G loss: 2.257162]\n",
      "[Epoch 127/200] [Batch 120/340] [D loss: 1.693955, acc: 68%] [G loss: 2.036142]\n",
      "[Epoch 127/200] [Batch 140/340] [D loss: 1.626730, acc: 70%] [G loss: 1.866416]\n",
      "[Epoch 127/200] [Batch 160/340] [D loss: 1.642364, acc: 67%] [G loss: 2.264572]\n",
      "[Epoch 127/200] [Batch 180/340] [D loss: 1.625559, acc: 66%] [G loss: 2.087654]\n",
      "[Epoch 127/200] [Batch 200/340] [D loss: 1.650042, acc: 61%] [G loss: 1.871736]\n",
      "[Epoch 127/200] [Batch 220/340] [D loss: 1.683972, acc: 64%] [G loss: 2.193514]\n",
      "[Epoch 127/200] [Batch 240/340] [D loss: 1.632690, acc: 65%] [G loss: 1.877853]\n",
      "[Epoch 127/200] [Batch 260/340] [D loss: 1.726360, acc: 63%] [G loss: 2.038872]\n",
      "[Epoch 127/200] [Batch 280/340] [D loss: 1.609996, acc: 68%] [G loss: 1.961736]\n",
      "[Epoch 127/200] [Batch 300/340] [D loss: 1.688058, acc: 63%] [G loss: 1.932121]\n",
      "[Epoch 127/200] [Batch 320/340] [D loss: 1.649223, acc: 62%] [G loss: 2.189712]\n",
      "[Epoch 128/200] [Batch 0/340] [D loss: 1.648197, acc: 64%] [G loss: 2.250849]\n",
      "[Epoch 128/200] [Batch 20/340] [D loss: 1.619844, acc: 65%] [G loss: 1.959813]\n",
      "[Epoch 128/200] [Batch 40/340] [D loss: 1.691352, acc: 64%] [G loss: 2.004857]\n",
      "[Epoch 128/200] [Batch 60/340] [D loss: 1.632548, acc: 66%] [G loss: 1.799720]\n",
      "[Epoch 128/200] [Batch 80/340] [D loss: 1.613448, acc: 67%] [G loss: 1.986932]\n",
      "[Epoch 128/200] [Batch 100/340] [D loss: 1.723792, acc: 67%] [G loss: 2.176494]\n",
      "[Epoch 128/200] [Batch 120/340] [D loss: 1.759120, acc: 64%] [G loss: 1.897050]\n",
      "[Epoch 128/200] [Batch 140/340] [D loss: 1.676884, acc: 65%] [G loss: 2.183878]\n",
      "[Epoch 128/200] [Batch 160/340] [D loss: 1.634028, acc: 68%] [G loss: 2.064085]\n",
      "[Epoch 128/200] [Batch 180/340] [D loss: 1.657256, acc: 65%] [G loss: 1.979385]\n",
      "[Epoch 128/200] [Batch 200/340] [D loss: 1.588157, acc: 65%] [G loss: 2.123814]\n",
      "[Epoch 128/200] [Batch 220/340] [D loss: 1.703216, acc: 66%] [G loss: 2.245502]\n",
      "[Epoch 128/200] [Batch 240/340] [D loss: 1.649818, acc: 68%] [G loss: 2.106112]\n",
      "[Epoch 128/200] [Batch 260/340] [D loss: 1.650322, acc: 64%] [G loss: 2.081484]\n",
      "[Epoch 128/200] [Batch 280/340] [D loss: 1.617114, acc: 65%] [G loss: 2.101552]\n",
      "[Epoch 128/200] [Batch 300/340] [D loss: 1.699762, acc: 65%] [G loss: 1.860870]\n",
      "[Epoch 128/200] [Batch 320/340] [D loss: 1.685324, acc: 65%] [G loss: 1.881306]\n",
      "[Epoch 129/200] [Batch 0/340] [D loss: 1.635417, acc: 65%] [G loss: 1.803873]\n",
      "[Epoch 129/200] [Batch 20/340] [D loss: 1.670828, acc: 65%] [G loss: 1.883710]\n",
      "[Epoch 129/200] [Batch 40/340] [D loss: 1.670987, acc: 62%] [G loss: 2.223970]\n",
      "[Epoch 129/200] [Batch 60/340] [D loss: 1.680364, acc: 70%] [G loss: 1.901912]\n",
      "[Epoch 129/200] [Batch 80/340] [D loss: 1.611571, acc: 63%] [G loss: 2.032141]\n",
      "[Epoch 129/200] [Batch 100/340] [D loss: 1.729396, acc: 64%] [G loss: 1.902063]\n",
      "[Epoch 129/200] [Batch 120/340] [D loss: 1.645491, acc: 67%] [G loss: 2.155697]\n",
      "[Epoch 129/200] [Batch 140/340] [D loss: 1.668243, acc: 66%] [G loss: 1.777248]\n",
      "[Epoch 129/200] [Batch 160/340] [D loss: 1.661158, acc: 66%] [G loss: 2.096287]\n",
      "[Epoch 129/200] [Batch 180/340] [D loss: 1.726712, acc: 65%] [G loss: 2.128376]\n",
      "[Epoch 129/200] [Batch 200/340] [D loss: 1.638507, acc: 62%] [G loss: 2.109387]\n",
      "[Epoch 129/200] [Batch 220/340] [D loss: 1.715071, acc: 63%] [G loss: 1.923498]\n",
      "[Epoch 129/200] [Batch 240/340] [D loss: 1.740369, acc: 67%] [G loss: 1.841231]\n",
      "[Epoch 129/200] [Batch 260/340] [D loss: 1.660930, acc: 65%] [G loss: 1.979555]\n",
      "[Epoch 129/200] [Batch 280/340] [D loss: 1.646144, acc: 65%] [G loss: 1.989791]\n",
      "[Epoch 129/200] [Batch 300/340] [D loss: 1.671012, acc: 64%] [G loss: 1.750918]\n",
      "[Epoch 129/200] [Batch 320/340] [D loss: 1.663507, acc: 66%] [G loss: 2.021328]\n",
      "[Epoch 130/200] [Batch 0/340] [D loss: 1.698289, acc: 65%] [G loss: 1.942218]\n",
      "[Epoch 130/200] [Batch 20/340] [D loss: 1.636320, acc: 68%] [G loss: 2.181950]\n",
      "[Epoch 130/200] [Batch 40/340] [D loss: 1.625170, acc: 68%] [G loss: 1.993523]\n",
      "[Epoch 130/200] [Batch 60/340] [D loss: 1.660761, acc: 65%] [G loss: 1.972568]\n",
      "[Epoch 130/200] [Batch 80/340] [D loss: 1.763039, acc: 63%] [G loss: 1.902610]\n",
      "[Epoch 130/200] [Batch 100/340] [D loss: 1.693016, acc: 65%] [G loss: 1.953002]\n",
      "[Epoch 130/200] [Batch 120/340] [D loss: 1.772290, acc: 63%] [G loss: 1.840914]\n",
      "[Epoch 130/200] [Batch 140/340] [D loss: 1.612521, acc: 66%] [G loss: 1.787127]\n",
      "[Epoch 130/200] [Batch 160/340] [D loss: 1.619465, acc: 66%] [G loss: 2.211422]\n",
      "[Epoch 130/200] [Batch 180/340] [D loss: 1.653774, acc: 66%] [G loss: 2.033670]\n",
      "[Epoch 130/200] [Batch 200/340] [D loss: 1.631510, acc: 68%] [G loss: 2.377415]\n",
      "[Epoch 130/200] [Batch 220/340] [D loss: 1.620130, acc: 69%] [G loss: 2.029639]\n",
      "[Epoch 130/200] [Batch 240/340] [D loss: 1.720800, acc: 66%] [G loss: 2.168395]\n",
      "[Epoch 130/200] [Batch 260/340] [D loss: 1.710276, acc: 66%] [G loss: 1.861273]\n",
      "[Epoch 130/200] [Batch 280/340] [D loss: 1.642275, acc: 69%] [G loss: 2.016192]\n",
      "[Epoch 130/200] [Batch 300/340] [D loss: 1.614954, acc: 67%] [G loss: 1.814552]\n",
      "[Epoch 130/200] [Batch 320/340] [D loss: 1.589022, acc: 68%] [G loss: 1.857054]\n",
      "[Epoch 131/200] [Batch 0/340] [D loss: 1.624014, acc: 67%] [G loss: 2.224078]\n",
      "[Epoch 131/200] [Batch 20/340] [D loss: 1.685511, acc: 67%] [G loss: 1.945596]\n",
      "[Epoch 131/200] [Batch 40/340] [D loss: 1.710423, acc: 66%] [G loss: 1.841953]\n",
      "[Epoch 131/200] [Batch 60/340] [D loss: 1.665191, acc: 67%] [G loss: 2.240075]\n",
      "[Epoch 131/200] [Batch 80/340] [D loss: 1.629233, acc: 63%] [G loss: 1.899403]\n",
      "[Epoch 131/200] [Batch 100/340] [D loss: 1.590943, acc: 66%] [G loss: 2.041058]\n",
      "[Epoch 131/200] [Batch 120/340] [D loss: 1.601694, acc: 70%] [G loss: 1.945929]\n",
      "[Epoch 131/200] [Batch 140/340] [D loss: 1.705928, acc: 66%] [G loss: 2.095260]\n",
      "[Epoch 131/200] [Batch 160/340] [D loss: 1.682359, acc: 69%] [G loss: 2.048748]\n",
      "[Epoch 131/200] [Batch 180/340] [D loss: 1.702475, acc: 64%] [G loss: 2.130592]\n",
      "[Epoch 131/200] [Batch 200/340] [D loss: 1.681744, acc: 69%] [G loss: 1.800785]\n",
      "[Epoch 131/200] [Batch 220/340] [D loss: 1.668366, acc: 68%] [G loss: 1.833710]\n",
      "[Epoch 131/200] [Batch 240/340] [D loss: 1.651121, acc: 66%] [G loss: 2.114336]\n",
      "[Epoch 131/200] [Batch 260/340] [D loss: 1.665041, acc: 61%] [G loss: 1.886770]\n",
      "[Epoch 131/200] [Batch 280/340] [D loss: 1.711443, acc: 67%] [G loss: 1.780898]\n",
      "[Epoch 131/200] [Batch 300/340] [D loss: 1.644930, acc: 67%] [G loss: 1.880983]\n",
      "[Epoch 131/200] [Batch 320/340] [D loss: 1.689612, acc: 67%] [G loss: 2.072463]\n",
      "[Epoch 132/200] [Batch 0/340] [D loss: 1.760777, acc: 66%] [G loss: 1.905273]\n",
      "[Epoch 132/200] [Batch 20/340] [D loss: 1.637882, acc: 68%] [G loss: 1.941102]\n",
      "[Epoch 132/200] [Batch 40/340] [D loss: 1.673931, acc: 66%] [G loss: 1.903174]\n",
      "[Epoch 132/200] [Batch 60/340] [D loss: 1.701242, acc: 65%] [G loss: 1.962858]\n",
      "[Epoch 132/200] [Batch 80/340] [D loss: 1.642587, acc: 67%] [G loss: 2.033416]\n",
      "[Epoch 132/200] [Batch 100/340] [D loss: 1.672428, acc: 67%] [G loss: 1.788092]\n",
      "[Epoch 132/200] [Batch 120/340] [D loss: 1.726722, acc: 66%] [G loss: 1.857558]\n",
      "[Epoch 132/200] [Batch 140/340] [D loss: 1.630765, acc: 65%] [G loss: 2.053223]\n",
      "[Epoch 132/200] [Batch 160/340] [D loss: 1.651731, acc: 67%] [G loss: 1.850890]\n",
      "[Epoch 132/200] [Batch 180/340] [D loss: 1.621656, acc: 65%] [G loss: 1.949571]\n",
      "[Epoch 132/200] [Batch 200/340] [D loss: 1.676603, acc: 66%] [G loss: 1.822758]\n",
      "[Epoch 132/200] [Batch 220/340] [D loss: 1.638219, acc: 68%] [G loss: 2.166266]\n",
      "[Epoch 132/200] [Batch 240/340] [D loss: 1.711422, acc: 65%] [G loss: 1.930574]\n",
      "[Epoch 132/200] [Batch 260/340] [D loss: 1.624836, acc: 68%] [G loss: 2.052425]\n",
      "[Epoch 132/200] [Batch 280/340] [D loss: 1.612172, acc: 66%] [G loss: 2.162009]\n",
      "[Epoch 132/200] [Batch 300/340] [D loss: 1.679966, acc: 65%] [G loss: 2.013075]\n",
      "[Epoch 132/200] [Batch 320/340] [D loss: 1.650498, acc: 61%] [G loss: 1.817054]\n",
      "[Epoch 133/200] [Batch 0/340] [D loss: 1.646436, acc: 66%] [G loss: 2.217476]\n",
      "[Epoch 133/200] [Batch 20/340] [D loss: 1.625070, acc: 69%] [G loss: 2.200854]\n",
      "[Epoch 133/200] [Batch 40/340] [D loss: 1.633426, acc: 64%] [G loss: 2.074588]\n",
      "[Epoch 133/200] [Batch 60/340] [D loss: 1.683392, acc: 66%] [G loss: 1.902884]\n",
      "[Epoch 133/200] [Batch 80/340] [D loss: 1.630296, acc: 66%] [G loss: 2.185933]\n",
      "[Epoch 133/200] [Batch 100/340] [D loss: 1.677758, acc: 69%] [G loss: 1.815714]\n",
      "[Epoch 133/200] [Batch 120/340] [D loss: 1.629870, acc: 67%] [G loss: 1.867128]\n",
      "[Epoch 133/200] [Batch 140/340] [D loss: 1.625742, acc: 68%] [G loss: 2.125062]\n",
      "[Epoch 133/200] [Batch 160/340] [D loss: 1.610052, acc: 67%] [G loss: 1.871399]\n",
      "[Epoch 133/200] [Batch 180/340] [D loss: 1.720930, acc: 65%] [G loss: 2.061019]\n",
      "[Epoch 133/200] [Batch 200/340] [D loss: 1.671201, acc: 65%] [G loss: 2.045124]\n",
      "[Epoch 133/200] [Batch 220/340] [D loss: 1.666651, acc: 66%] [G loss: 2.137223]\n",
      "[Epoch 133/200] [Batch 240/340] [D loss: 1.692253, acc: 65%] [G loss: 2.120357]\n",
      "[Epoch 133/200] [Batch 260/340] [D loss: 1.648561, acc: 65%] [G loss: 1.877589]\n",
      "[Epoch 133/200] [Batch 280/340] [D loss: 1.660184, acc: 65%] [G loss: 2.071361]\n",
      "[Epoch 133/200] [Batch 300/340] [D loss: 1.661912, acc: 68%] [G loss: 2.004791]\n",
      "[Epoch 133/200] [Batch 320/340] [D loss: 1.651929, acc: 68%] [G loss: 2.224191]\n",
      "[Epoch 134/200] [Batch 0/340] [D loss: 1.626332, acc: 66%] [G loss: 1.858845]\n",
      "[Epoch 134/200] [Batch 20/340] [D loss: 1.699772, acc: 67%] [G loss: 1.880407]\n",
      "[Epoch 134/200] [Batch 40/340] [D loss: 1.703116, acc: 65%] [G loss: 1.781035]\n",
      "[Epoch 134/200] [Batch 60/340] [D loss: 1.652904, acc: 65%] [G loss: 2.023680]\n",
      "[Epoch 134/200] [Batch 80/340] [D loss: 1.679579, acc: 63%] [G loss: 2.130051]\n",
      "[Epoch 134/200] [Batch 100/340] [D loss: 1.707382, acc: 66%] [G loss: 1.816943]\n",
      "[Epoch 134/200] [Batch 120/340] [D loss: 1.618150, acc: 66%] [G loss: 1.871357]\n",
      "[Epoch 134/200] [Batch 140/340] [D loss: 1.578078, acc: 65%] [G loss: 2.227368]\n",
      "[Epoch 134/200] [Batch 160/340] [D loss: 1.659954, acc: 67%] [G loss: 2.012503]\n",
      "[Epoch 134/200] [Batch 180/340] [D loss: 1.685629, acc: 66%] [G loss: 1.945999]\n",
      "[Epoch 134/200] [Batch 200/340] [D loss: 1.596560, acc: 64%] [G loss: 1.858182]\n",
      "[Epoch 134/200] [Batch 220/340] [D loss: 1.673500, acc: 67%] [G loss: 2.137049]\n",
      "[Epoch 134/200] [Batch 240/340] [D loss: 1.667963, acc: 67%] [G loss: 1.923861]\n",
      "[Epoch 134/200] [Batch 260/340] [D loss: 1.679595, acc: 66%] [G loss: 1.890727]\n",
      "[Epoch 134/200] [Batch 280/340] [D loss: 1.712091, acc: 66%] [G loss: 2.105307]\n",
      "[Epoch 134/200] [Batch 300/340] [D loss: 1.668254, acc: 67%] [G loss: 2.239367]\n",
      "[Epoch 134/200] [Batch 320/340] [D loss: 1.609795, acc: 67%] [G loss: 1.966337]\n",
      "[Epoch 135/200] [Batch 0/340] [D loss: 1.644511, acc: 68%] [G loss: 1.956815]\n",
      "[Epoch 135/200] [Batch 20/340] [D loss: 1.589973, acc: 68%] [G loss: 1.895924]\n",
      "[Epoch 135/200] [Batch 40/340] [D loss: 1.683494, acc: 68%] [G loss: 1.822504]\n",
      "[Epoch 135/200] [Batch 60/340] [D loss: 1.667778, acc: 67%] [G loss: 1.944873]\n",
      "[Epoch 135/200] [Batch 80/340] [D loss: 1.682046, acc: 68%] [G loss: 2.039296]\n",
      "[Epoch 135/200] [Batch 100/340] [D loss: 1.620265, acc: 70%] [G loss: 1.954404]\n",
      "[Epoch 135/200] [Batch 120/340] [D loss: 1.704775, acc: 68%] [G loss: 1.940249]\n",
      "[Epoch 135/200] [Batch 140/340] [D loss: 1.632639, acc: 66%] [G loss: 2.115395]\n",
      "[Epoch 135/200] [Batch 160/340] [D loss: 1.683507, acc: 66%] [G loss: 1.893208]\n",
      "[Epoch 135/200] [Batch 180/340] [D loss: 1.743884, acc: 71%] [G loss: 1.704211]\n",
      "[Epoch 135/200] [Batch 200/340] [D loss: 1.671438, acc: 66%] [G loss: 1.959610]\n",
      "[Epoch 135/200] [Batch 220/340] [D loss: 1.619803, acc: 66%] [G loss: 1.753681]\n",
      "[Epoch 135/200] [Batch 240/340] [D loss: 1.668714, acc: 67%] [G loss: 2.281532]\n",
      "[Epoch 135/200] [Batch 260/340] [D loss: 1.668227, acc: 68%] [G loss: 1.828173]\n",
      "[Epoch 135/200] [Batch 280/340] [D loss: 1.680353, acc: 66%] [G loss: 2.207957]\n",
      "[Epoch 135/200] [Batch 300/340] [D loss: 1.682070, acc: 69%] [G loss: 2.265005]\n",
      "[Epoch 135/200] [Batch 320/340] [D loss: 1.687853, acc: 69%] [G loss: 2.175317]\n",
      "[Epoch 136/200] [Batch 0/340] [D loss: 1.639060, acc: 65%] [G loss: 1.880887]\n",
      "[Epoch 136/200] [Batch 20/340] [D loss: 1.620427, acc: 67%] [G loss: 1.918069]\n",
      "[Epoch 136/200] [Batch 40/340] [D loss: 1.644669, acc: 67%] [G loss: 2.006166]\n",
      "[Epoch 136/200] [Batch 60/340] [D loss: 1.671394, acc: 69%] [G loss: 1.973774]\n",
      "[Epoch 136/200] [Batch 80/340] [D loss: 1.623838, acc: 67%] [G loss: 2.135035]\n",
      "[Epoch 136/200] [Batch 100/340] [D loss: 1.606106, acc: 67%] [G loss: 1.999075]\n",
      "[Epoch 136/200] [Batch 120/340] [D loss: 1.618951, acc: 71%] [G loss: 2.046598]\n",
      "[Epoch 136/200] [Batch 140/340] [D loss: 1.670616, acc: 66%] [G loss: 1.851157]\n",
      "[Epoch 136/200] [Batch 160/340] [D loss: 1.631605, acc: 71%] [G loss: 2.070615]\n",
      "[Epoch 136/200] [Batch 180/340] [D loss: 1.662548, acc: 66%] [G loss: 1.994228]\n",
      "[Epoch 136/200] [Batch 200/340] [D loss: 1.661815, acc: 66%] [G loss: 2.238972]\n",
      "[Epoch 136/200] [Batch 220/340] [D loss: 1.687251, acc: 68%] [G loss: 1.936061]\n",
      "[Epoch 136/200] [Batch 240/340] [D loss: 1.612737, acc: 64%] [G loss: 2.014275]\n",
      "[Epoch 136/200] [Batch 260/340] [D loss: 1.664112, acc: 66%] [G loss: 1.869134]\n",
      "[Epoch 136/200] [Batch 280/340] [D loss: 1.630197, acc: 65%] [G loss: 1.922842]\n",
      "[Epoch 136/200] [Batch 300/340] [D loss: 1.681539, acc: 67%] [G loss: 1.766324]\n",
      "[Epoch 136/200] [Batch 320/340] [D loss: 1.630212, acc: 66%] [G loss: 2.289906]\n",
      "[Epoch 137/200] [Batch 0/340] [D loss: 1.594370, acc: 68%] [G loss: 1.869189]\n",
      "[Epoch 137/200] [Batch 20/340] [D loss: 1.663609, acc: 65%] [G loss: 2.071159]\n",
      "[Epoch 137/200] [Batch 40/340] [D loss: 1.675206, acc: 66%] [G loss: 2.208622]\n",
      "[Epoch 137/200] [Batch 60/340] [D loss: 1.599496, acc: 69%] [G loss: 2.113105]\n",
      "[Epoch 137/200] [Batch 80/340] [D loss: 1.768945, acc: 66%] [G loss: 2.205460]\n",
      "[Epoch 137/200] [Batch 100/340] [D loss: 1.631347, acc: 68%] [G loss: 1.926210]\n",
      "[Epoch 137/200] [Batch 120/340] [D loss: 1.679911, acc: 63%] [G loss: 2.122405]\n",
      "[Epoch 137/200] [Batch 140/340] [D loss: 1.651622, acc: 68%] [G loss: 1.951116]\n",
      "[Epoch 137/200] [Batch 160/340] [D loss: 1.644276, acc: 67%] [G loss: 2.153941]\n",
      "[Epoch 137/200] [Batch 180/340] [D loss: 1.651312, acc: 66%] [G loss: 1.886918]\n",
      "[Epoch 137/200] [Batch 200/340] [D loss: 1.648012, acc: 66%] [G loss: 2.079431]\n",
      "[Epoch 137/200] [Batch 220/340] [D loss: 1.650449, acc: 67%] [G loss: 2.391194]\n",
      "[Epoch 137/200] [Batch 240/340] [D loss: 1.640670, acc: 61%] [G loss: 1.847599]\n",
      "[Epoch 137/200] [Batch 260/340] [D loss: 1.660343, acc: 66%] [G loss: 1.957232]\n",
      "[Epoch 137/200] [Batch 280/340] [D loss: 1.691559, acc: 67%] [G loss: 1.802271]\n",
      "[Epoch 137/200] [Batch 300/340] [D loss: 1.609829, acc: 67%] [G loss: 2.175929]\n",
      "[Epoch 137/200] [Batch 320/340] [D loss: 1.624646, acc: 67%] [G loss: 1.895365]\n",
      "[Epoch 138/200] [Batch 0/340] [D loss: 1.616250, acc: 69%] [G loss: 2.007033]\n",
      "[Epoch 138/200] [Batch 20/340] [D loss: 1.676907, acc: 70%] [G loss: 1.849156]\n",
      "[Epoch 138/200] [Batch 40/340] [D loss: 1.654436, acc: 67%] [G loss: 2.025837]\n",
      "[Epoch 138/200] [Batch 60/340] [D loss: 1.663484, acc: 67%] [G loss: 2.107399]\n",
      "[Epoch 138/200] [Batch 80/340] [D loss: 1.670323, acc: 67%] [G loss: 1.868793]\n",
      "[Epoch 138/200] [Batch 100/340] [D loss: 1.659284, acc: 63%] [G loss: 2.014230]\n",
      "[Epoch 138/200] [Batch 120/340] [D loss: 1.690719, acc: 63%] [G loss: 2.102848]\n",
      "[Epoch 138/200] [Batch 140/340] [D loss: 1.656163, acc: 66%] [G loss: 1.800696]\n",
      "[Epoch 138/200] [Batch 160/340] [D loss: 1.685177, acc: 68%] [G loss: 2.106967]\n",
      "[Epoch 138/200] [Batch 180/340] [D loss: 1.651755, acc: 67%] [G loss: 1.917451]\n",
      "[Epoch 138/200] [Batch 200/340] [D loss: 1.691765, acc: 67%] [G loss: 1.813941]\n",
      "[Epoch 138/200] [Batch 220/340] [D loss: 1.648475, acc: 65%] [G loss: 2.008146]\n",
      "[Epoch 138/200] [Batch 240/340] [D loss: 1.689656, acc: 68%] [G loss: 1.999190]\n",
      "[Epoch 138/200] [Batch 260/340] [D loss: 1.695198, acc: 69%] [G loss: 1.902197]\n",
      "[Epoch 138/200] [Batch 280/340] [D loss: 1.620325, acc: 66%] [G loss: 1.756700]\n",
      "[Epoch 138/200] [Batch 300/340] [D loss: 1.619366, acc: 68%] [G loss: 1.977062]\n",
      "[Epoch 138/200] [Batch 320/340] [D loss: 1.641199, acc: 67%] [G loss: 2.100666]\n",
      "[Epoch 139/200] [Batch 0/340] [D loss: 1.704995, acc: 72%] [G loss: 1.996634]\n",
      "[Epoch 139/200] [Batch 20/340] [D loss: 1.679781, acc: 66%] [G loss: 1.899915]\n",
      "[Epoch 139/200] [Batch 40/340] [D loss: 1.658188, acc: 66%] [G loss: 1.962354]\n",
      "[Epoch 139/200] [Batch 60/340] [D loss: 1.675434, acc: 70%] [G loss: 1.848261]\n",
      "[Epoch 139/200] [Batch 80/340] [D loss: 1.630419, acc: 68%] [G loss: 2.161341]\n",
      "[Epoch 139/200] [Batch 100/340] [D loss: 1.750791, acc: 67%] [G loss: 1.920247]\n",
      "[Epoch 139/200] [Batch 120/340] [D loss: 1.669292, acc: 67%] [G loss: 1.899846]\n",
      "[Epoch 139/200] [Batch 140/340] [D loss: 1.651825, acc: 69%] [G loss: 2.067101]\n",
      "[Epoch 139/200] [Batch 160/340] [D loss: 1.693516, acc: 68%] [G loss: 1.777900]\n",
      "[Epoch 139/200] [Batch 180/340] [D loss: 1.655802, acc: 67%] [G loss: 2.091561]\n",
      "[Epoch 139/200] [Batch 200/340] [D loss: 1.659872, acc: 65%] [G loss: 2.124873]\n",
      "[Epoch 139/200] [Batch 220/340] [D loss: 1.648680, acc: 67%] [G loss: 1.888343]\n",
      "[Epoch 139/200] [Batch 240/340] [D loss: 1.695091, acc: 70%] [G loss: 2.281541]\n",
      "[Epoch 139/200] [Batch 260/340] [D loss: 1.640737, acc: 68%] [G loss: 2.232295]\n",
      "[Epoch 139/200] [Batch 280/340] [D loss: 1.644299, acc: 65%] [G loss: 2.112148]\n",
      "[Epoch 139/200] [Batch 300/340] [D loss: 1.719318, acc: 67%] [G loss: 2.130863]\n",
      "[Epoch 139/200] [Batch 320/340] [D loss: 1.615689, acc: 70%] [G loss: 1.909895]\n",
      "[Epoch 140/200] [Batch 0/340] [D loss: 1.705399, acc: 67%] [G loss: 1.779967]\n",
      "[Epoch 140/200] [Batch 20/340] [D loss: 1.643459, acc: 66%] [G loss: 1.895582]\n",
      "[Epoch 140/200] [Batch 40/340] [D loss: 1.626991, acc: 71%] [G loss: 1.836358]\n",
      "[Epoch 140/200] [Batch 60/340] [D loss: 1.679297, acc: 65%] [G loss: 2.002297]\n",
      "[Epoch 140/200] [Batch 80/340] [D loss: 1.614088, acc: 68%] [G loss: 1.919781]\n",
      "[Epoch 140/200] [Batch 100/340] [D loss: 1.664811, acc: 66%] [G loss: 2.191048]\n",
      "[Epoch 140/200] [Batch 120/340] [D loss: 1.665165, acc: 64%] [G loss: 1.974345]\n",
      "[Epoch 140/200] [Batch 140/340] [D loss: 1.732937, acc: 65%] [G loss: 1.811130]\n",
      "[Epoch 140/200] [Batch 160/340] [D loss: 1.630679, acc: 66%] [G loss: 1.980307]\n",
      "[Epoch 140/200] [Batch 180/340] [D loss: 1.652895, acc: 64%] [G loss: 1.972205]\n",
      "[Epoch 140/200] [Batch 200/340] [D loss: 1.686415, acc: 66%] [G loss: 1.952867]\n",
      "[Epoch 140/200] [Batch 220/340] [D loss: 1.570805, acc: 68%] [G loss: 1.937945]\n",
      "[Epoch 140/200] [Batch 240/340] [D loss: 1.725686, acc: 64%] [G loss: 1.935930]\n",
      "[Epoch 140/200] [Batch 260/340] [D loss: 1.658154, acc: 65%] [G loss: 1.881659]\n",
      "[Epoch 140/200] [Batch 280/340] [D loss: 1.617311, acc: 69%] [G loss: 2.090587]\n",
      "[Epoch 140/200] [Batch 300/340] [D loss: 1.632646, acc: 67%] [G loss: 1.997838]\n",
      "[Epoch 140/200] [Batch 320/340] [D loss: 1.682078, acc: 68%] [G loss: 1.958226]\n",
      "[Epoch 141/200] [Batch 0/340] [D loss: 1.659481, acc: 66%] [G loss: 1.728812]\n",
      "[Epoch 141/200] [Batch 20/340] [D loss: 1.635031, acc: 64%] [G loss: 1.923138]\n",
      "[Epoch 141/200] [Batch 40/340] [D loss: 1.600306, acc: 67%] [G loss: 2.151905]\n",
      "[Epoch 141/200] [Batch 60/340] [D loss: 1.692899, acc: 71%] [G loss: 1.735174]\n",
      "[Epoch 141/200] [Batch 80/340] [D loss: 1.639421, acc: 69%] [G loss: 2.100875]\n",
      "[Epoch 141/200] [Batch 100/340] [D loss: 1.626622, acc: 69%] [G loss: 1.913285]\n",
      "[Epoch 141/200] [Batch 120/340] [D loss: 1.715209, acc: 67%] [G loss: 2.079425]\n",
      "[Epoch 141/200] [Batch 140/340] [D loss: 1.708086, acc: 64%] [G loss: 2.083050]\n",
      "[Epoch 141/200] [Batch 160/340] [D loss: 1.607979, acc: 70%] [G loss: 1.992295]\n",
      "[Epoch 141/200] [Batch 180/340] [D loss: 1.688279, acc: 67%] [G loss: 2.115535]\n",
      "[Epoch 141/200] [Batch 200/340] [D loss: 1.639080, acc: 69%] [G loss: 1.920538]\n",
      "[Epoch 141/200] [Batch 220/340] [D loss: 1.609877, acc: 71%] [G loss: 1.824485]\n",
      "[Epoch 141/200] [Batch 240/340] [D loss: 1.623136, acc: 67%] [G loss: 2.131073]\n",
      "[Epoch 141/200] [Batch 260/340] [D loss: 1.741731, acc: 69%] [G loss: 1.840461]\n",
      "[Epoch 141/200] [Batch 280/340] [D loss: 1.730447, acc: 69%] [G loss: 1.785702]\n",
      "[Epoch 141/200] [Batch 300/340] [D loss: 1.626769, acc: 67%] [G loss: 1.975466]\n",
      "[Epoch 141/200] [Batch 320/340] [D loss: 1.649092, acc: 72%] [G loss: 2.015344]\n",
      "[Epoch 142/200] [Batch 0/340] [D loss: 1.658795, acc: 69%] [G loss: 1.889042]\n",
      "[Epoch 142/200] [Batch 20/340] [D loss: 1.640015, acc: 68%] [G loss: 1.831788]\n",
      "[Epoch 142/200] [Batch 40/340] [D loss: 1.629750, acc: 67%] [G loss: 2.086381]\n",
      "[Epoch 142/200] [Batch 60/340] [D loss: 1.644281, acc: 66%] [G loss: 1.929673]\n",
      "[Epoch 142/200] [Batch 80/340] [D loss: 1.695584, acc: 68%] [G loss: 2.236904]\n",
      "[Epoch 142/200] [Batch 100/340] [D loss: 1.629013, acc: 70%] [G loss: 1.874826]\n",
      "[Epoch 142/200] [Batch 120/340] [D loss: 1.670959, acc: 70%] [G loss: 1.816863]\n",
      "[Epoch 142/200] [Batch 140/340] [D loss: 1.751378, acc: 65%] [G loss: 1.864956]\n",
      "[Epoch 142/200] [Batch 160/340] [D loss: 1.640778, acc: 66%] [G loss: 1.930078]\n",
      "[Epoch 142/200] [Batch 180/340] [D loss: 1.713482, acc: 66%] [G loss: 1.685391]\n",
      "[Epoch 142/200] [Batch 200/340] [D loss: 1.672310, acc: 70%] [G loss: 1.924244]\n",
      "[Epoch 142/200] [Batch 220/340] [D loss: 1.625418, acc: 66%] [G loss: 1.984873]\n",
      "[Epoch 142/200] [Batch 240/340] [D loss: 1.705711, acc: 67%] [G loss: 1.626582]\n",
      "[Epoch 142/200] [Batch 260/340] [D loss: 1.703366, acc: 67%] [G loss: 2.057412]\n",
      "[Epoch 142/200] [Batch 280/340] [D loss: 1.645963, acc: 72%] [G loss: 2.148181]\n",
      "[Epoch 142/200] [Batch 300/340] [D loss: 1.622209, acc: 66%] [G loss: 1.994209]\n",
      "[Epoch 142/200] [Batch 320/340] [D loss: 1.626936, acc: 68%] [G loss: 2.089179]\n",
      "[Epoch 143/200] [Batch 0/340] [D loss: 1.626080, acc: 65%] [G loss: 1.891517]\n",
      "[Epoch 143/200] [Batch 20/340] [D loss: 1.654929, acc: 69%] [G loss: 1.755369]\n",
      "[Epoch 143/200] [Batch 40/340] [D loss: 1.693781, acc: 60%] [G loss: 1.962414]\n",
      "[Epoch 143/200] [Batch 60/340] [D loss: 1.683923, acc: 71%] [G loss: 1.751271]\n",
      "[Epoch 143/200] [Batch 80/340] [D loss: 1.671460, acc: 65%] [G loss: 1.793019]\n",
      "[Epoch 143/200] [Batch 100/340] [D loss: 1.670602, acc: 68%] [G loss: 2.102471]\n",
      "[Epoch 143/200] [Batch 120/340] [D loss: 1.677557, acc: 66%] [G loss: 1.870014]\n",
      "[Epoch 143/200] [Batch 140/340] [D loss: 1.630845, acc: 63%] [G loss: 1.877477]\n",
      "[Epoch 143/200] [Batch 160/340] [D loss: 1.645851, acc: 69%] [G loss: 1.890516]\n",
      "[Epoch 143/200] [Batch 180/340] [D loss: 1.647443, acc: 65%] [G loss: 2.135437]\n",
      "[Epoch 143/200] [Batch 200/340] [D loss: 1.621965, acc: 68%] [G loss: 2.003813]\n",
      "[Epoch 143/200] [Batch 220/340] [D loss: 1.721797, acc: 68%] [G loss: 2.194297]\n",
      "[Epoch 143/200] [Batch 240/340] [D loss: 1.662632, acc: 65%] [G loss: 1.778035]\n",
      "[Epoch 143/200] [Batch 260/340] [D loss: 1.683470, acc: 71%] [G loss: 1.903661]\n",
      "[Epoch 143/200] [Batch 280/340] [D loss: 1.609529, acc: 70%] [G loss: 2.125747]\n",
      "[Epoch 143/200] [Batch 300/340] [D loss: 1.692737, acc: 68%] [G loss: 2.157507]\n",
      "[Epoch 143/200] [Batch 320/340] [D loss: 1.666414, acc: 69%] [G loss: 1.909528]\n",
      "[Epoch 144/200] [Batch 0/340] [D loss: 1.616336, acc: 71%] [G loss: 1.941130]\n",
      "[Epoch 144/200] [Batch 20/340] [D loss: 1.660756, acc: 65%] [G loss: 2.109187]\n",
      "[Epoch 144/200] [Batch 40/340] [D loss: 1.683314, acc: 66%] [G loss: 2.030799]\n",
      "[Epoch 144/200] [Batch 60/340] [D loss: 1.747821, acc: 65%] [G loss: 2.214274]\n",
      "[Epoch 144/200] [Batch 80/340] [D loss: 1.626531, acc: 66%] [G loss: 1.983264]\n",
      "[Epoch 144/200] [Batch 100/340] [D loss: 1.693573, acc: 66%] [G loss: 2.021204]\n",
      "[Epoch 144/200] [Batch 120/340] [D loss: 1.614985, acc: 68%] [G loss: 2.091269]\n",
      "[Epoch 144/200] [Batch 140/340] [D loss: 1.698832, acc: 68%] [G loss: 2.088140]\n",
      "[Epoch 144/200] [Batch 160/340] [D loss: 1.678018, acc: 68%] [G loss: 1.945866]\n",
      "[Epoch 144/200] [Batch 180/340] [D loss: 1.664636, acc: 70%] [G loss: 2.194381]\n",
      "[Epoch 144/200] [Batch 200/340] [D loss: 1.577941, acc: 71%] [G loss: 1.956546]\n",
      "[Epoch 144/200] [Batch 220/340] [D loss: 1.681862, acc: 65%] [G loss: 2.015248]\n",
      "[Epoch 144/200] [Batch 240/340] [D loss: 1.692486, acc: 65%] [G loss: 1.973219]\n",
      "[Epoch 144/200] [Batch 260/340] [D loss: 1.691334, acc: 70%] [G loss: 1.888267]\n",
      "[Epoch 144/200] [Batch 280/340] [D loss: 1.694744, acc: 65%] [G loss: 2.332348]\n",
      "[Epoch 144/200] [Batch 300/340] [D loss: 1.658504, acc: 67%] [G loss: 1.841325]\n",
      "[Epoch 144/200] [Batch 320/340] [D loss: 1.692482, acc: 65%] [G loss: 2.148848]\n",
      "[Epoch 145/200] [Batch 0/340] [D loss: 1.701123, acc: 68%] [G loss: 1.851206]\n",
      "[Epoch 145/200] [Batch 20/340] [D loss: 1.627593, acc: 67%] [G loss: 1.910975]\n",
      "[Epoch 145/200] [Batch 40/340] [D loss: 1.711462, acc: 69%] [G loss: 1.922798]\n",
      "[Epoch 145/200] [Batch 60/340] [D loss: 1.676806, acc: 65%] [G loss: 1.978416]\n",
      "[Epoch 145/200] [Batch 80/340] [D loss: 1.657914, acc: 68%] [G loss: 1.885879]\n",
      "[Epoch 145/200] [Batch 100/340] [D loss: 1.705623, acc: 69%] [G loss: 2.037999]\n",
      "[Epoch 145/200] [Batch 120/340] [D loss: 1.656563, acc: 69%] [G loss: 1.664398]\n",
      "[Epoch 145/200] [Batch 140/340] [D loss: 1.681441, acc: 69%] [G loss: 1.761586]\n",
      "[Epoch 145/200] [Batch 160/340] [D loss: 1.699878, acc: 61%] [G loss: 1.875152]\n",
      "[Epoch 145/200] [Batch 180/340] [D loss: 1.638974, acc: 66%] [G loss: 1.928090]\n",
      "[Epoch 145/200] [Batch 200/340] [D loss: 1.695514, acc: 66%] [G loss: 1.878594]\n",
      "[Epoch 145/200] [Batch 220/340] [D loss: 1.675173, acc: 69%] [G loss: 2.270483]\n",
      "[Epoch 145/200] [Batch 240/340] [D loss: 1.600140, acc: 66%] [G loss: 2.044097]\n",
      "[Epoch 145/200] [Batch 260/340] [D loss: 1.689442, acc: 67%] [G loss: 1.740429]\n",
      "[Epoch 145/200] [Batch 280/340] [D loss: 1.661440, acc: 72%] [G loss: 2.178907]\n",
      "[Epoch 145/200] [Batch 300/340] [D loss: 1.621870, acc: 71%] [G loss: 2.120667]\n",
      "[Epoch 145/200] [Batch 320/340] [D loss: 1.638938, acc: 70%] [G loss: 2.007269]\n",
      "[Epoch 146/200] [Batch 0/340] [D loss: 1.673801, acc: 66%] [G loss: 1.918098]\n",
      "[Epoch 146/200] [Batch 20/340] [D loss: 1.699265, acc: 68%] [G loss: 1.798437]\n",
      "[Epoch 146/200] [Batch 40/340] [D loss: 1.646004, acc: 69%] [G loss: 2.169100]\n",
      "[Epoch 146/200] [Batch 60/340] [D loss: 1.654071, acc: 68%] [G loss: 1.815478]\n",
      "[Epoch 146/200] [Batch 80/340] [D loss: 1.624103, acc: 69%] [G loss: 1.982318]\n",
      "[Epoch 146/200] [Batch 100/340] [D loss: 1.676605, acc: 65%] [G loss: 1.894736]\n",
      "[Epoch 146/200] [Batch 120/340] [D loss: 1.735166, acc: 69%] [G loss: 1.895922]\n",
      "[Epoch 146/200] [Batch 140/340] [D loss: 1.614289, acc: 68%] [G loss: 1.847812]\n",
      "[Epoch 146/200] [Batch 160/340] [D loss: 1.604902, acc: 72%] [G loss: 1.986311]\n",
      "[Epoch 146/200] [Batch 180/340] [D loss: 1.672372, acc: 66%] [G loss: 1.984702]\n",
      "[Epoch 146/200] [Batch 200/340] [D loss: 1.685152, acc: 70%] [G loss: 2.311920]\n",
      "[Epoch 146/200] [Batch 220/340] [D loss: 1.820538, acc: 70%] [G loss: 2.374935]\n",
      "[Epoch 146/200] [Batch 240/340] [D loss: 1.640234, acc: 69%] [G loss: 1.967800]\n",
      "[Epoch 146/200] [Batch 260/340] [D loss: 1.702765, acc: 69%] [G loss: 2.099109]\n",
      "[Epoch 146/200] [Batch 280/340] [D loss: 1.620514, acc: 67%] [G loss: 2.183666]\n",
      "[Epoch 146/200] [Batch 300/340] [D loss: 1.699056, acc: 67%] [G loss: 2.043463]\n",
      "[Epoch 146/200] [Batch 320/340] [D loss: 1.653697, acc: 68%] [G loss: 2.028849]\n",
      "[Epoch 147/200] [Batch 0/340] [D loss: 1.662601, acc: 67%] [G loss: 2.072392]\n",
      "[Epoch 147/200] [Batch 20/340] [D loss: 1.631034, acc: 70%] [G loss: 1.761992]\n",
      "[Epoch 147/200] [Batch 40/340] [D loss: 1.672183, acc: 66%] [G loss: 2.066057]\n",
      "[Epoch 147/200] [Batch 60/340] [D loss: 1.679064, acc: 67%] [G loss: 1.812595]\n",
      "[Epoch 147/200] [Batch 80/340] [D loss: 1.650260, acc: 67%] [G loss: 1.855305]\n",
      "[Epoch 147/200] [Batch 100/340] [D loss: 1.720720, acc: 67%] [G loss: 2.251056]\n",
      "[Epoch 147/200] [Batch 120/340] [D loss: 1.702893, acc: 65%] [G loss: 2.048732]\n",
      "[Epoch 147/200] [Batch 140/340] [D loss: 1.637897, acc: 68%] [G loss: 2.296740]\n",
      "[Epoch 147/200] [Batch 160/340] [D loss: 1.590654, acc: 72%] [G loss: 2.134562]\n",
      "[Epoch 147/200] [Batch 180/340] [D loss: 1.684089, acc: 67%] [G loss: 2.188885]\n",
      "[Epoch 147/200] [Batch 200/340] [D loss: 1.689230, acc: 67%] [G loss: 2.093181]\n",
      "[Epoch 147/200] [Batch 220/340] [D loss: 1.730157, acc: 68%] [G loss: 2.149633]\n",
      "[Epoch 147/200] [Batch 240/340] [D loss: 1.651670, acc: 62%] [G loss: 2.130076]\n",
      "[Epoch 147/200] [Batch 260/340] [D loss: 1.601022, acc: 65%] [G loss: 1.896492]\n",
      "[Epoch 147/200] [Batch 280/340] [D loss: 1.691124, acc: 66%] [G loss: 1.862070]\n",
      "[Epoch 147/200] [Batch 300/340] [D loss: 1.635859, acc: 66%] [G loss: 1.974417]\n",
      "[Epoch 147/200] [Batch 320/340] [D loss: 1.598096, acc: 68%] [G loss: 2.114549]\n",
      "[Epoch 148/200] [Batch 0/340] [D loss: 1.601611, acc: 66%] [G loss: 1.726122]\n",
      "[Epoch 148/200] [Batch 20/340] [D loss: 1.608346, acc: 72%] [G loss: 2.062882]\n",
      "[Epoch 148/200] [Batch 40/340] [D loss: 1.616970, acc: 70%] [G loss: 1.916124]\n",
      "[Epoch 148/200] [Batch 60/340] [D loss: 1.614299, acc: 66%] [G loss: 2.209494]\n",
      "[Epoch 148/200] [Batch 80/340] [D loss: 1.608180, acc: 68%] [G loss: 2.033589]\n",
      "[Epoch 148/200] [Batch 100/340] [D loss: 1.626887, acc: 72%] [G loss: 1.880295]\n",
      "[Epoch 148/200] [Batch 120/340] [D loss: 1.648401, acc: 66%] [G loss: 2.034580]\n",
      "[Epoch 148/200] [Batch 140/340] [D loss: 1.656052, acc: 67%] [G loss: 1.790055]\n",
      "[Epoch 148/200] [Batch 160/340] [D loss: 1.601157, acc: 67%] [G loss: 1.912825]\n",
      "[Epoch 148/200] [Batch 180/340] [D loss: 1.689887, acc: 71%] [G loss: 1.941536]\n",
      "[Epoch 148/200] [Batch 200/340] [D loss: 1.616570, acc: 67%] [G loss: 1.813263]\n",
      "[Epoch 148/200] [Batch 220/340] [D loss: 1.606851, acc: 70%] [G loss: 2.134167]\n",
      "[Epoch 148/200] [Batch 240/340] [D loss: 1.640684, acc: 68%] [G loss: 1.899810]\n",
      "[Epoch 148/200] [Batch 260/340] [D loss: 1.602013, acc: 67%] [G loss: 2.034320]\n",
      "[Epoch 148/200] [Batch 280/340] [D loss: 1.639672, acc: 70%] [G loss: 2.043476]\n",
      "[Epoch 148/200] [Batch 300/340] [D loss: 1.714845, acc: 71%] [G loss: 2.159725]\n",
      "[Epoch 148/200] [Batch 320/340] [D loss: 1.734718, acc: 70%] [G loss: 2.195216]\n",
      "[Epoch 149/200] [Batch 0/340] [D loss: 1.686140, acc: 69%] [G loss: 1.828008]\n",
      "[Epoch 149/200] [Batch 20/340] [D loss: 1.685120, acc: 66%] [G loss: 1.979608]\n",
      "[Epoch 149/200] [Batch 40/340] [D loss: 1.631202, acc: 68%] [G loss: 1.811640]\n",
      "[Epoch 149/200] [Batch 60/340] [D loss: 1.763321, acc: 66%] [G loss: 1.872380]\n",
      "[Epoch 149/200] [Batch 80/340] [D loss: 1.630198, acc: 66%] [G loss: 1.863343]\n",
      "[Epoch 149/200] [Batch 100/340] [D loss: 1.674086, acc: 66%] [G loss: 2.218560]\n",
      "[Epoch 149/200] [Batch 120/340] [D loss: 1.574660, acc: 69%] [G loss: 2.153765]\n",
      "[Epoch 149/200] [Batch 140/340] [D loss: 1.619828, acc: 66%] [G loss: 2.189379]\n",
      "[Epoch 149/200] [Batch 160/340] [D loss: 1.674153, acc: 71%] [G loss: 1.895131]\n",
      "[Epoch 149/200] [Batch 180/340] [D loss: 1.685611, acc: 69%] [G loss: 1.900881]\n",
      "[Epoch 149/200] [Batch 200/340] [D loss: 1.611524, acc: 74%] [G loss: 2.060444]\n",
      "[Epoch 149/200] [Batch 220/340] [D loss: 1.659207, acc: 72%] [G loss: 1.848060]\n",
      "[Epoch 149/200] [Batch 240/340] [D loss: 1.675600, acc: 69%] [G loss: 2.002200]\n",
      "[Epoch 149/200] [Batch 260/340] [D loss: 1.730076, acc: 67%] [G loss: 1.806341]\n",
      "[Epoch 149/200] [Batch 280/340] [D loss: 1.696975, acc: 69%] [G loss: 2.197505]\n",
      "[Epoch 149/200] [Batch 300/340] [D loss: 1.607855, acc: 71%] [G loss: 1.974457]\n",
      "[Epoch 149/200] [Batch 320/340] [D loss: 1.669271, acc: 69%] [G loss: 2.142207]\n",
      "[Epoch 150/200] [Batch 0/340] [D loss: 1.613121, acc: 72%] [G loss: 1.886823]\n",
      "[Epoch 150/200] [Batch 20/340] [D loss: 1.594150, acc: 67%] [G loss: 1.967637]\n",
      "[Epoch 150/200] [Batch 40/340] [D loss: 1.737489, acc: 69%] [G loss: 2.181075]\n",
      "[Epoch 150/200] [Batch 60/340] [D loss: 1.753397, acc: 69%] [G loss: 2.218526]\n",
      "[Epoch 150/200] [Batch 80/340] [D loss: 1.770416, acc: 66%] [G loss: 1.686667]\n",
      "[Epoch 150/200] [Batch 100/340] [D loss: 1.617668, acc: 68%] [G loss: 1.847132]\n",
      "[Epoch 150/200] [Batch 120/340] [D loss: 1.692702, acc: 69%] [G loss: 1.855004]\n",
      "[Epoch 150/200] [Batch 140/340] [D loss: 1.657070, acc: 66%] [G loss: 1.831134]\n",
      "[Epoch 150/200] [Batch 160/340] [D loss: 1.614343, acc: 71%] [G loss: 1.948093]\n",
      "[Epoch 150/200] [Batch 180/340] [D loss: 1.662168, acc: 71%] [G loss: 1.856012]\n",
      "[Epoch 150/200] [Batch 200/340] [D loss: 1.669065, acc: 68%] [G loss: 1.766350]\n",
      "[Epoch 150/200] [Batch 220/340] [D loss: 1.650089, acc: 68%] [G loss: 1.855166]\n",
      "[Epoch 150/200] [Batch 240/340] [D loss: 1.609162, acc: 63%] [G loss: 2.098649]\n",
      "[Epoch 150/200] [Batch 260/340] [D loss: 1.678601, acc: 68%] [G loss: 2.182104]\n",
      "[Epoch 150/200] [Batch 280/340] [D loss: 1.698199, acc: 68%] [G loss: 1.952649]\n",
      "[Epoch 150/200] [Batch 300/340] [D loss: 1.699366, acc: 65%] [G loss: 1.942307]\n",
      "[Epoch 150/200] [Batch 320/340] [D loss: 1.715945, acc: 67%] [G loss: 1.750206]\n",
      "[Epoch 151/200] [Batch 0/340] [D loss: 1.648855, acc: 71%] [G loss: 1.920028]\n",
      "[Epoch 151/200] [Batch 20/340] [D loss: 1.684941, acc: 68%] [G loss: 1.737374]\n",
      "[Epoch 151/200] [Batch 40/340] [D loss: 1.674659, acc: 67%] [G loss: 1.761830]\n",
      "[Epoch 151/200] [Batch 60/340] [D loss: 1.612880, acc: 70%] [G loss: 2.087056]\n",
      "[Epoch 151/200] [Batch 80/340] [D loss: 1.711033, acc: 68%] [G loss: 1.854342]\n",
      "[Epoch 151/200] [Batch 100/340] [D loss: 1.706919, acc: 70%] [G loss: 1.906220]\n",
      "[Epoch 151/200] [Batch 120/340] [D loss: 1.644488, acc: 70%] [G loss: 1.900358]\n",
      "[Epoch 151/200] [Batch 140/340] [D loss: 1.713337, acc: 72%] [G loss: 2.494887]\n",
      "[Epoch 151/200] [Batch 160/340] [D loss: 1.581904, acc: 68%] [G loss: 1.905706]\n",
      "[Epoch 151/200] [Batch 180/340] [D loss: 1.716528, acc: 68%] [G loss: 2.007553]\n",
      "[Epoch 151/200] [Batch 200/340] [D loss: 1.681590, acc: 68%] [G loss: 1.827455]\n",
      "[Epoch 151/200] [Batch 220/340] [D loss: 1.660240, acc: 69%] [G loss: 2.074892]\n",
      "[Epoch 151/200] [Batch 240/340] [D loss: 1.634462, acc: 71%] [G loss: 1.986942]\n",
      "[Epoch 151/200] [Batch 260/340] [D loss: 1.622244, acc: 71%] [G loss: 2.024985]\n",
      "[Epoch 151/200] [Batch 280/340] [D loss: 1.617956, acc: 70%] [G loss: 1.875553]\n",
      "[Epoch 151/200] [Batch 300/340] [D loss: 1.648998, acc: 75%] [G loss: 1.932213]\n",
      "[Epoch 151/200] [Batch 320/340] [D loss: 1.730894, acc: 70%] [G loss: 1.747068]\n",
      "[Epoch 152/200] [Batch 0/340] [D loss: 1.655184, acc: 70%] [G loss: 2.065636]\n",
      "[Epoch 152/200] [Batch 20/340] [D loss: 1.598472, acc: 67%] [G loss: 2.034309]\n",
      "[Epoch 152/200] [Batch 40/340] [D loss: 1.654898, acc: 73%] [G loss: 1.877136]\n",
      "[Epoch 152/200] [Batch 60/340] [D loss: 1.604594, acc: 71%] [G loss: 2.062754]\n",
      "[Epoch 152/200] [Batch 80/340] [D loss: 1.683810, acc: 68%] [G loss: 1.983822]\n",
      "[Epoch 152/200] [Batch 100/340] [D loss: 1.690915, acc: 68%] [G loss: 1.986173]\n",
      "[Epoch 152/200] [Batch 120/340] [D loss: 1.614601, acc: 68%] [G loss: 1.903713]\n",
      "[Epoch 152/200] [Batch 140/340] [D loss: 1.716153, acc: 68%] [G loss: 1.828254]\n",
      "[Epoch 152/200] [Batch 160/340] [D loss: 1.686927, acc: 68%] [G loss: 2.230525]\n",
      "[Epoch 152/200] [Batch 180/340] [D loss: 1.678335, acc: 69%] [G loss: 1.783769]\n",
      "[Epoch 152/200] [Batch 200/340] [D loss: 1.641769, acc: 70%] [G loss: 2.107611]\n",
      "[Epoch 152/200] [Batch 220/340] [D loss: 1.656719, acc: 69%] [G loss: 1.844104]\n",
      "[Epoch 152/200] [Batch 240/340] [D loss: 1.669591, acc: 68%] [G loss: 1.877740]\n",
      "[Epoch 152/200] [Batch 260/340] [D loss: 1.699881, acc: 66%] [G loss: 1.923071]\n",
      "[Epoch 152/200] [Batch 280/340] [D loss: 1.632067, acc: 69%] [G loss: 1.891547]\n",
      "[Epoch 152/200] [Batch 300/340] [D loss: 1.720903, acc: 74%] [G loss: 1.705249]\n",
      "[Epoch 152/200] [Batch 320/340] [D loss: 1.711362, acc: 71%] [G loss: 1.896612]\n",
      "[Epoch 153/200] [Batch 0/340] [D loss: 1.615408, acc: 69%] [G loss: 2.018886]\n",
      "[Epoch 153/200] [Batch 20/340] [D loss: 1.671948, acc: 68%] [G loss: 1.901892]\n",
      "[Epoch 153/200] [Batch 40/340] [D loss: 1.682141, acc: 67%] [G loss: 1.833961]\n",
      "[Epoch 153/200] [Batch 60/340] [D loss: 1.576121, acc: 69%] [G loss: 2.019742]\n",
      "[Epoch 153/200] [Batch 80/340] [D loss: 1.595414, acc: 71%] [G loss: 1.737855]\n",
      "[Epoch 153/200] [Batch 100/340] [D loss: 1.665437, acc: 71%] [G loss: 1.812798]\n",
      "[Epoch 153/200] [Batch 120/340] [D loss: 1.676457, acc: 66%] [G loss: 1.851004]\n",
      "[Epoch 153/200] [Batch 140/340] [D loss: 1.619269, acc: 69%] [G loss: 2.027999]\n",
      "[Epoch 153/200] [Batch 160/340] [D loss: 1.751251, acc: 71%] [G loss: 2.057551]\n",
      "[Epoch 153/200] [Batch 180/340] [D loss: 1.607064, acc: 67%] [G loss: 2.006469]\n",
      "[Epoch 153/200] [Batch 200/340] [D loss: 1.636184, acc: 69%] [G loss: 2.047561]\n",
      "[Epoch 153/200] [Batch 220/340] [D loss: 1.626526, acc: 71%] [G loss: 1.977009]\n",
      "[Epoch 153/200] [Batch 240/340] [D loss: 1.612606, acc: 69%] [G loss: 1.687127]\n",
      "[Epoch 153/200] [Batch 260/340] [D loss: 1.663086, acc: 70%] [G loss: 1.804384]\n",
      "[Epoch 153/200] [Batch 280/340] [D loss: 1.631684, acc: 69%] [G loss: 2.002746]\n",
      "[Epoch 153/200] [Batch 300/340] [D loss: 1.769407, acc: 70%] [G loss: 2.311450]\n",
      "[Epoch 153/200] [Batch 320/340] [D loss: 1.623803, acc: 67%] [G loss: 1.963560]\n",
      "[Epoch 154/200] [Batch 0/340] [D loss: 1.671071, acc: 70%] [G loss: 1.841909]\n",
      "[Epoch 154/200] [Batch 20/340] [D loss: 1.609944, acc: 71%] [G loss: 1.955331]\n",
      "[Epoch 154/200] [Batch 40/340] [D loss: 1.655458, acc: 71%] [G loss: 2.174103]\n",
      "[Epoch 154/200] [Batch 60/340] [D loss: 1.644600, acc: 68%] [G loss: 1.749330]\n",
      "[Epoch 154/200] [Batch 80/340] [D loss: 1.593807, acc: 71%] [G loss: 2.064708]\n",
      "[Epoch 154/200] [Batch 100/340] [D loss: 1.621835, acc: 70%] [G loss: 1.930616]\n",
      "[Epoch 154/200] [Batch 120/340] [D loss: 1.635608, acc: 68%] [G loss: 1.988613]\n",
      "[Epoch 154/200] [Batch 140/340] [D loss: 1.679274, acc: 70%] [G loss: 1.877181]\n",
      "[Epoch 154/200] [Batch 160/340] [D loss: 1.671602, acc: 70%] [G loss: 1.907866]\n",
      "[Epoch 154/200] [Batch 180/340] [D loss: 1.669911, acc: 75%] [G loss: 2.106169]\n",
      "[Epoch 154/200] [Batch 200/340] [D loss: 1.734423, acc: 66%] [G loss: 1.730911]\n",
      "[Epoch 154/200] [Batch 220/340] [D loss: 1.588985, acc: 69%] [G loss: 2.050001]\n",
      "[Epoch 154/200] [Batch 240/340] [D loss: 1.650692, acc: 70%] [G loss: 1.810425]\n",
      "[Epoch 154/200] [Batch 260/340] [D loss: 1.600675, acc: 71%] [G loss: 2.122570]\n",
      "[Epoch 154/200] [Batch 280/340] [D loss: 1.629679, acc: 71%] [G loss: 1.830548]\n",
      "[Epoch 154/200] [Batch 300/340] [D loss: 1.634567, acc: 70%] [G loss: 1.866357]\n",
      "[Epoch 154/200] [Batch 320/340] [D loss: 1.621163, acc: 70%] [G loss: 1.831984]\n",
      "[Epoch 155/200] [Batch 0/340] [D loss: 1.700446, acc: 66%] [G loss: 2.303034]\n",
      "[Epoch 155/200] [Batch 20/340] [D loss: 1.718145, acc: 68%] [G loss: 2.202899]\n",
      "[Epoch 155/200] [Batch 40/340] [D loss: 1.643072, acc: 67%] [G loss: 2.174090]\n",
      "[Epoch 155/200] [Batch 60/340] [D loss: 1.709000, acc: 73%] [G loss: 2.345089]\n",
      "[Epoch 155/200] [Batch 80/340] [D loss: 1.699121, acc: 68%] [G loss: 1.845508]\n",
      "[Epoch 155/200] [Batch 100/340] [D loss: 1.659763, acc: 67%] [G loss: 2.040394]\n",
      "[Epoch 155/200] [Batch 120/340] [D loss: 1.654039, acc: 68%] [G loss: 1.682234]\n",
      "[Epoch 155/200] [Batch 140/340] [D loss: 1.593659, acc: 70%] [G loss: 1.913008]\n",
      "[Epoch 155/200] [Batch 160/340] [D loss: 1.645657, acc: 71%] [G loss: 1.711912]\n",
      "[Epoch 155/200] [Batch 180/340] [D loss: 1.645027, acc: 67%] [G loss: 1.977036]\n",
      "[Epoch 155/200] [Batch 200/340] [D loss: 1.622787, acc: 68%] [G loss: 1.854850]\n",
      "[Epoch 155/200] [Batch 220/340] [D loss: 1.598089, acc: 70%] [G loss: 1.658033]\n",
      "[Epoch 155/200] [Batch 240/340] [D loss: 1.696185, acc: 67%] [G loss: 1.867805]\n",
      "[Epoch 155/200] [Batch 260/340] [D loss: 1.708906, acc: 68%] [G loss: 1.996225]\n",
      "[Epoch 155/200] [Batch 280/340] [D loss: 1.729780, acc: 69%] [G loss: 2.274840]\n",
      "[Epoch 155/200] [Batch 300/340] [D loss: 1.652460, acc: 72%] [G loss: 1.966349]\n",
      "[Epoch 155/200] [Batch 320/340] [D loss: 1.754215, acc: 68%] [G loss: 1.877695]\n",
      "[Epoch 156/200] [Batch 0/340] [D loss: 1.598389, acc: 73%] [G loss: 1.894506]\n",
      "[Epoch 156/200] [Batch 20/340] [D loss: 1.697927, acc: 69%] [G loss: 2.042043]\n",
      "[Epoch 156/200] [Batch 40/340] [D loss: 1.654163, acc: 71%] [G loss: 1.893760]\n",
      "[Epoch 156/200] [Batch 60/340] [D loss: 1.662646, acc: 65%] [G loss: 1.815724]\n",
      "[Epoch 156/200] [Batch 80/340] [D loss: 1.717681, acc: 68%] [G loss: 1.833031]\n",
      "[Epoch 156/200] [Batch 100/340] [D loss: 1.588042, acc: 71%] [G loss: 1.896068]\n",
      "[Epoch 156/200] [Batch 120/340] [D loss: 1.632624, acc: 70%] [G loss: 2.077920]\n",
      "[Epoch 156/200] [Batch 140/340] [D loss: 1.636546, acc: 69%] [G loss: 1.911043]\n",
      "[Epoch 156/200] [Batch 160/340] [D loss: 1.644889, acc: 69%] [G loss: 2.209447]\n",
      "[Epoch 156/200] [Batch 180/340] [D loss: 1.675752, acc: 66%] [G loss: 2.114892]\n",
      "[Epoch 156/200] [Batch 200/340] [D loss: 1.636017, acc: 70%] [G loss: 1.800365]\n",
      "[Epoch 156/200] [Batch 220/340] [D loss: 1.638053, acc: 70%] [G loss: 1.771746]\n",
      "[Epoch 156/200] [Batch 240/340] [D loss: 1.598292, acc: 68%] [G loss: 1.818182]\n",
      "[Epoch 156/200] [Batch 260/340] [D loss: 1.575530, acc: 71%] [G loss: 1.987926]\n",
      "[Epoch 156/200] [Batch 280/340] [D loss: 1.667269, acc: 70%] [G loss: 1.815921]\n",
      "[Epoch 156/200] [Batch 300/340] [D loss: 1.663525, acc: 72%] [G loss: 1.826703]\n",
      "[Epoch 156/200] [Batch 320/340] [D loss: 1.618181, acc: 71%] [G loss: 2.007576]\n",
      "[Epoch 157/200] [Batch 0/340] [D loss: 1.654721, acc: 73%] [G loss: 1.852636]\n",
      "[Epoch 157/200] [Batch 20/340] [D loss: 1.642830, acc: 70%] [G loss: 2.209273]\n",
      "[Epoch 157/200] [Batch 40/340] [D loss: 1.707912, acc: 69%] [G loss: 2.065968]\n",
      "[Epoch 157/200] [Batch 60/340] [D loss: 1.622705, acc: 71%] [G loss: 1.777702]\n",
      "[Epoch 157/200] [Batch 80/340] [D loss: 1.666150, acc: 68%] [G loss: 2.016109]\n",
      "[Epoch 157/200] [Batch 100/340] [D loss: 1.663465, acc: 70%] [G loss: 1.730234]\n",
      "[Epoch 157/200] [Batch 120/340] [D loss: 1.708032, acc: 69%] [G loss: 1.830217]\n",
      "[Epoch 157/200] [Batch 140/340] [D loss: 1.675669, acc: 70%] [G loss: 1.776731]\n",
      "[Epoch 157/200] [Batch 160/340] [D loss: 1.690020, acc: 69%] [G loss: 1.736586]\n",
      "[Epoch 157/200] [Batch 180/340] [D loss: 1.659635, acc: 70%] [G loss: 2.107960]\n",
      "[Epoch 157/200] [Batch 200/340] [D loss: 1.603343, acc: 68%] [G loss: 2.097144]\n",
      "[Epoch 157/200] [Batch 220/340] [D loss: 1.675237, acc: 72%] [G loss: 1.878385]\n",
      "[Epoch 157/200] [Batch 240/340] [D loss: 1.670157, acc: 67%] [G loss: 1.784067]\n",
      "[Epoch 157/200] [Batch 260/340] [D loss: 1.657890, acc: 69%] [G loss: 1.935951]\n",
      "[Epoch 157/200] [Batch 280/340] [D loss: 1.692946, acc: 68%] [G loss: 1.789293]\n",
      "[Epoch 157/200] [Batch 300/340] [D loss: 1.673998, acc: 68%] [G loss: 1.904975]\n",
      "[Epoch 157/200] [Batch 320/340] [D loss: 1.686823, acc: 72%] [G loss: 1.756908]\n",
      "[Epoch 158/200] [Batch 0/340] [D loss: 1.676242, acc: 67%] [G loss: 1.726580]\n",
      "[Epoch 158/200] [Batch 20/340] [D loss: 1.612394, acc: 73%] [G loss: 1.757934]\n",
      "[Epoch 158/200] [Batch 40/340] [D loss: 1.616126, acc: 68%] [G loss: 2.097307]\n",
      "[Epoch 158/200] [Batch 60/340] [D loss: 1.659989, acc: 72%] [G loss: 2.264256]\n",
      "[Epoch 158/200] [Batch 80/340] [D loss: 1.659785, acc: 72%] [G loss: 2.071727]\n",
      "[Epoch 158/200] [Batch 100/340] [D loss: 1.670461, acc: 70%] [G loss: 2.056889]\n",
      "[Epoch 158/200] [Batch 120/340] [D loss: 1.618964, acc: 69%] [G loss: 1.892900]\n",
      "[Epoch 158/200] [Batch 140/340] [D loss: 1.694841, acc: 73%] [G loss: 1.603109]\n",
      "[Epoch 158/200] [Batch 160/340] [D loss: 1.642641, acc: 65%] [G loss: 2.038330]\n",
      "[Epoch 158/200] [Batch 180/340] [D loss: 1.650396, acc: 68%] [G loss: 1.956718]\n",
      "[Epoch 158/200] [Batch 200/340] [D loss: 1.618106, acc: 69%] [G loss: 1.981718]\n",
      "[Epoch 158/200] [Batch 220/340] [D loss: 1.656749, acc: 74%] [G loss: 1.871984]\n",
      "[Epoch 158/200] [Batch 240/340] [D loss: 1.629193, acc: 71%] [G loss: 1.885660]\n",
      "[Epoch 158/200] [Batch 260/340] [D loss: 1.678868, acc: 69%] [G loss: 1.714844]\n",
      "[Epoch 158/200] [Batch 280/340] [D loss: 1.682963, acc: 71%] [G loss: 1.655303]\n",
      "[Epoch 158/200] [Batch 300/340] [D loss: 1.644193, acc: 70%] [G loss: 1.664098]\n",
      "[Epoch 158/200] [Batch 320/340] [D loss: 1.648289, acc: 66%] [G loss: 2.158066]\n",
      "[Epoch 159/200] [Batch 0/340] [D loss: 1.619785, acc: 68%] [G loss: 1.956348]\n",
      "[Epoch 159/200] [Batch 20/340] [D loss: 1.630006, acc: 73%] [G loss: 1.787358]\n",
      "[Epoch 159/200] [Batch 40/340] [D loss: 1.658120, acc: 70%] [G loss: 1.991286]\n",
      "[Epoch 159/200] [Batch 60/340] [D loss: 1.668357, acc: 71%] [G loss: 1.777278]\n",
      "[Epoch 159/200] [Batch 80/340] [D loss: 1.698110, acc: 72%] [G loss: 1.767242]\n",
      "[Epoch 159/200] [Batch 100/340] [D loss: 1.605923, acc: 72%] [G loss: 1.867450]\n",
      "[Epoch 159/200] [Batch 120/340] [D loss: 1.626360, acc: 72%] [G loss: 1.828993]\n",
      "[Epoch 159/200] [Batch 140/340] [D loss: 1.749177, acc: 68%] [G loss: 1.793636]\n",
      "[Epoch 159/200] [Batch 160/340] [D loss: 1.698059, acc: 72%] [G loss: 1.693921]\n",
      "[Epoch 159/200] [Batch 180/340] [D loss: 1.651314, acc: 72%] [G loss: 1.887871]\n",
      "[Epoch 159/200] [Batch 200/340] [D loss: 1.601414, acc: 70%] [G loss: 1.767805]\n",
      "[Epoch 159/200] [Batch 220/340] [D loss: 1.642353, acc: 70%] [G loss: 1.788100]\n",
      "[Epoch 159/200] [Batch 240/340] [D loss: 1.658011, acc: 69%] [G loss: 1.883198]\n",
      "[Epoch 159/200] [Batch 260/340] [D loss: 1.622676, acc: 70%] [G loss: 1.736570]\n",
      "[Epoch 159/200] [Batch 280/340] [D loss: 1.628186, acc: 70%] [G loss: 1.985851]\n",
      "[Epoch 159/200] [Batch 300/340] [D loss: 1.643370, acc: 66%] [G loss: 2.073460]\n",
      "[Epoch 159/200] [Batch 320/340] [D loss: 1.656388, acc: 70%] [G loss: 2.086941]\n",
      "[Epoch 160/200] [Batch 0/340] [D loss: 1.678141, acc: 70%] [G loss: 2.005726]\n",
      "[Epoch 160/200] [Batch 20/340] [D loss: 1.658308, acc: 72%] [G loss: 2.023230]\n",
      "[Epoch 160/200] [Batch 40/340] [D loss: 1.678205, acc: 68%] [G loss: 1.794447]\n",
      "[Epoch 160/200] [Batch 60/340] [D loss: 1.688691, acc: 68%] [G loss: 1.726661]\n",
      "[Epoch 160/200] [Batch 80/340] [D loss: 1.763006, acc: 69%] [G loss: 2.125506]\n",
      "[Epoch 160/200] [Batch 100/340] [D loss: 1.724115, acc: 67%] [G loss: 2.043243]\n",
      "[Epoch 160/200] [Batch 120/340] [D loss: 1.685761, acc: 71%] [G loss: 1.725237]\n",
      "[Epoch 160/200] [Batch 140/340] [D loss: 1.665676, acc: 74%] [G loss: 1.955588]\n",
      "[Epoch 160/200] [Batch 160/340] [D loss: 1.663093, acc: 69%] [G loss: 1.957103]\n",
      "[Epoch 160/200] [Batch 180/340] [D loss: 1.576493, acc: 73%] [G loss: 1.905754]\n",
      "[Epoch 160/200] [Batch 200/340] [D loss: 1.682308, acc: 70%] [G loss: 1.687544]\n",
      "[Epoch 160/200] [Batch 220/340] [D loss: 1.657499, acc: 70%] [G loss: 1.707050]\n",
      "[Epoch 160/200] [Batch 240/340] [D loss: 1.659552, acc: 68%] [G loss: 1.880520]\n",
      "[Epoch 160/200] [Batch 260/340] [D loss: 1.712988, acc: 66%] [G loss: 1.924368]\n",
      "[Epoch 160/200] [Batch 280/340] [D loss: 1.642815, acc: 69%] [G loss: 1.908107]\n",
      "[Epoch 160/200] [Batch 300/340] [D loss: 1.633092, acc: 69%] [G loss: 1.907828]\n",
      "[Epoch 160/200] [Batch 320/340] [D loss: 1.695141, acc: 65%] [G loss: 2.043077]\n",
      "[Epoch 161/200] [Batch 0/340] [D loss: 1.666465, acc: 69%] [G loss: 1.906230]\n",
      "[Epoch 161/200] [Batch 20/340] [D loss: 1.642783, acc: 70%] [G loss: 1.987624]\n",
      "[Epoch 161/200] [Batch 40/340] [D loss: 1.619244, acc: 69%] [G loss: 2.040873]\n",
      "[Epoch 161/200] [Batch 60/340] [D loss: 1.648666, acc: 67%] [G loss: 2.032914]\n",
      "[Epoch 161/200] [Batch 80/340] [D loss: 1.622959, acc: 67%] [G loss: 1.913890]\n",
      "[Epoch 161/200] [Batch 100/340] [D loss: 1.695170, acc: 68%] [G loss: 2.361005]\n",
      "[Epoch 161/200] [Batch 120/340] [D loss: 1.640224, acc: 69%] [G loss: 2.085461]\n",
      "[Epoch 161/200] [Batch 140/340] [D loss: 1.599828, acc: 71%] [G loss: 1.883735]\n",
      "[Epoch 161/200] [Batch 160/340] [D loss: 1.563613, acc: 72%] [G loss: 2.014388]\n",
      "[Epoch 161/200] [Batch 180/340] [D loss: 1.572310, acc: 72%] [G loss: 2.098809]\n",
      "[Epoch 161/200] [Batch 200/340] [D loss: 1.650459, acc: 69%] [G loss: 1.930814]\n",
      "[Epoch 161/200] [Batch 220/340] [D loss: 1.628651, acc: 67%] [G loss: 1.825393]\n",
      "[Epoch 161/200] [Batch 240/340] [D loss: 1.605548, acc: 71%] [G loss: 1.852932]\n",
      "[Epoch 161/200] [Batch 260/340] [D loss: 1.678759, acc: 71%] [G loss: 1.843153]\n",
      "[Epoch 161/200] [Batch 280/340] [D loss: 1.622251, acc: 70%] [G loss: 1.852611]\n",
      "[Epoch 161/200] [Batch 300/340] [D loss: 1.601408, acc: 75%] [G loss: 1.994912]\n",
      "[Epoch 161/200] [Batch 320/340] [D loss: 1.655951, acc: 70%] [G loss: 1.878621]\n",
      "[Epoch 162/200] [Batch 0/340] [D loss: 1.635736, acc: 74%] [G loss: 2.062282]\n",
      "[Epoch 162/200] [Batch 20/340] [D loss: 1.754688, acc: 73%] [G loss: 1.970279]\n",
      "[Epoch 162/200] [Batch 40/340] [D loss: 1.611008, acc: 68%] [G loss: 2.006275]\n",
      "[Epoch 162/200] [Batch 60/340] [D loss: 1.632739, acc: 71%] [G loss: 1.848961]\n",
      "[Epoch 162/200] [Batch 80/340] [D loss: 1.597615, acc: 68%] [G loss: 1.894139]\n",
      "[Epoch 162/200] [Batch 100/340] [D loss: 1.677438, acc: 71%] [G loss: 1.741474]\n",
      "[Epoch 162/200] [Batch 120/340] [D loss: 1.622168, acc: 71%] [G loss: 1.792325]\n",
      "[Epoch 162/200] [Batch 140/340] [D loss: 1.630892, acc: 71%] [G loss: 1.916957]\n",
      "[Epoch 162/200] [Batch 160/340] [D loss: 1.651673, acc: 71%] [G loss: 2.045875]\n",
      "[Epoch 162/200] [Batch 180/340] [D loss: 1.696491, acc: 67%] [G loss: 2.024035]\n",
      "[Epoch 162/200] [Batch 200/340] [D loss: 1.683274, acc: 71%] [G loss: 1.801207]\n",
      "[Epoch 162/200] [Batch 220/340] [D loss: 1.616780, acc: 71%] [G loss: 2.016557]\n",
      "[Epoch 162/200] [Batch 240/340] [D loss: 1.608786, acc: 68%] [G loss: 2.116193]\n",
      "[Epoch 162/200] [Batch 260/340] [D loss: 1.623122, acc: 69%] [G loss: 2.011205]\n",
      "[Epoch 162/200] [Batch 280/340] [D loss: 1.668030, acc: 68%] [G loss: 1.871539]\n",
      "[Epoch 162/200] [Batch 300/340] [D loss: 1.674414, acc: 68%] [G loss: 1.959376]\n",
      "[Epoch 162/200] [Batch 320/340] [D loss: 1.679787, acc: 69%] [G loss: 2.019457]\n",
      "[Epoch 163/200] [Batch 0/340] [D loss: 1.590303, acc: 68%] [G loss: 2.215576]\n",
      "[Epoch 163/200] [Batch 20/340] [D loss: 1.616660, acc: 67%] [G loss: 2.071773]\n",
      "[Epoch 163/200] [Batch 40/340] [D loss: 1.671337, acc: 69%] [G loss: 2.295449]\n",
      "[Epoch 163/200] [Batch 60/340] [D loss: 1.649485, acc: 69%] [G loss: 2.124534]\n",
      "[Epoch 163/200] [Batch 80/340] [D loss: 1.577242, acc: 74%] [G loss: 1.918497]\n",
      "[Epoch 163/200] [Batch 100/340] [D loss: 1.658778, acc: 70%] [G loss: 1.963076]\n",
      "[Epoch 163/200] [Batch 120/340] [D loss: 1.659811, acc: 75%] [G loss: 2.059038]\n",
      "[Epoch 163/200] [Batch 140/340] [D loss: 1.693758, acc: 68%] [G loss: 2.074619]\n",
      "[Epoch 163/200] [Batch 160/340] [D loss: 1.619109, acc: 73%] [G loss: 2.114994]\n",
      "[Epoch 163/200] [Batch 180/340] [D loss: 1.671730, acc: 69%] [G loss: 1.797025]\n",
      "[Epoch 163/200] [Batch 200/340] [D loss: 1.637468, acc: 67%] [G loss: 2.002086]\n",
      "[Epoch 163/200] [Batch 220/340] [D loss: 1.590775, acc: 70%] [G loss: 1.803476]\n",
      "[Epoch 163/200] [Batch 240/340] [D loss: 1.644303, acc: 73%] [G loss: 2.030827]\n",
      "[Epoch 163/200] [Batch 260/340] [D loss: 1.624394, acc: 74%] [G loss: 2.238519]\n",
      "[Epoch 163/200] [Batch 280/340] [D loss: 1.656371, acc: 72%] [G loss: 2.332726]\n",
      "[Epoch 163/200] [Batch 300/340] [D loss: 1.701997, acc: 68%] [G loss: 2.239843]\n",
      "[Epoch 163/200] [Batch 320/340] [D loss: 1.616187, acc: 69%] [G loss: 1.758102]\n",
      "[Epoch 164/200] [Batch 0/340] [D loss: 1.640326, acc: 70%] [G loss: 1.938262]\n",
      "[Epoch 164/200] [Batch 20/340] [D loss: 1.638535, acc: 68%] [G loss: 1.739025]\n",
      "[Epoch 164/200] [Batch 40/340] [D loss: 1.643610, acc: 72%] [G loss: 2.270872]\n",
      "[Epoch 164/200] [Batch 60/340] [D loss: 1.629784, acc: 71%] [G loss: 2.009513]\n",
      "[Epoch 164/200] [Batch 80/340] [D loss: 1.704306, acc: 69%] [G loss: 2.049548]\n",
      "[Epoch 164/200] [Batch 100/340] [D loss: 1.667286, acc: 66%] [G loss: 1.831693]\n",
      "[Epoch 164/200] [Batch 120/340] [D loss: 1.729084, acc: 71%] [G loss: 1.760418]\n",
      "[Epoch 164/200] [Batch 140/340] [D loss: 1.774952, acc: 72%] [G loss: 1.604740]\n",
      "[Epoch 164/200] [Batch 160/340] [D loss: 1.636298, acc: 72%] [G loss: 2.059092]\n",
      "[Epoch 164/200] [Batch 180/340] [D loss: 1.647988, acc: 72%] [G loss: 1.807959]\n",
      "[Epoch 164/200] [Batch 200/340] [D loss: 1.631090, acc: 71%] [G loss: 1.971145]\n",
      "[Epoch 164/200] [Batch 220/340] [D loss: 1.690766, acc: 71%] [G loss: 2.026308]\n",
      "[Epoch 164/200] [Batch 240/340] [D loss: 1.569021, acc: 71%] [G loss: 1.976894]\n",
      "[Epoch 164/200] [Batch 260/340] [D loss: 1.653982, acc: 69%] [G loss: 1.873855]\n",
      "[Epoch 164/200] [Batch 280/340] [D loss: 1.610877, acc: 71%] [G loss: 2.152848]\n",
      "[Epoch 164/200] [Batch 300/340] [D loss: 1.637506, acc: 70%] [G loss: 1.869214]\n",
      "[Epoch 164/200] [Batch 320/340] [D loss: 1.708979, acc: 71%] [G loss: 1.800352]\n",
      "[Epoch 165/200] [Batch 0/340] [D loss: 1.568684, acc: 73%] [G loss: 1.677786]\n",
      "[Epoch 165/200] [Batch 20/340] [D loss: 1.658651, acc: 72%] [G loss: 1.749270]\n",
      "[Epoch 165/200] [Batch 40/340] [D loss: 1.626451, acc: 69%] [G loss: 1.771371]\n",
      "[Epoch 165/200] [Batch 60/340] [D loss: 1.630625, acc: 71%] [G loss: 1.965191]\n",
      "[Epoch 165/200] [Batch 80/340] [D loss: 1.688043, acc: 71%] [G loss: 1.896387]\n",
      "[Epoch 165/200] [Batch 100/340] [D loss: 1.693460, acc: 68%] [G loss: 1.760273]\n",
      "[Epoch 165/200] [Batch 120/340] [D loss: 1.567230, acc: 73%] [G loss: 2.044192]\n",
      "[Epoch 165/200] [Batch 140/340] [D loss: 1.668331, acc: 68%] [G loss: 1.924394]\n",
      "[Epoch 165/200] [Batch 160/340] [D loss: 1.601863, acc: 73%] [G loss: 2.040137]\n",
      "[Epoch 165/200] [Batch 180/340] [D loss: 1.673125, acc: 67%] [G loss: 2.142168]\n",
      "[Epoch 165/200] [Batch 200/340] [D loss: 1.613929, acc: 71%] [G loss: 1.849780]\n",
      "[Epoch 165/200] [Batch 220/340] [D loss: 1.683905, acc: 68%] [G loss: 1.868849]\n",
      "[Epoch 165/200] [Batch 240/340] [D loss: 1.687270, acc: 71%] [G loss: 1.753212]\n",
      "[Epoch 165/200] [Batch 260/340] [D loss: 1.659315, acc: 73%] [G loss: 1.836321]\n",
      "[Epoch 165/200] [Batch 280/340] [D loss: 1.640229, acc: 72%] [G loss: 1.866921]\n",
      "[Epoch 165/200] [Batch 300/340] [D loss: 1.680929, acc: 69%] [G loss: 1.895466]\n",
      "[Epoch 165/200] [Batch 320/340] [D loss: 1.585938, acc: 69%] [G loss: 1.863304]\n",
      "[Epoch 166/200] [Batch 0/340] [D loss: 1.635166, acc: 71%] [G loss: 1.909508]\n",
      "[Epoch 166/200] [Batch 20/340] [D loss: 1.602181, acc: 69%] [G loss: 1.856769]\n",
      "[Epoch 166/200] [Batch 40/340] [D loss: 1.652838, acc: 69%] [G loss: 2.003213]\n",
      "[Epoch 166/200] [Batch 60/340] [D loss: 1.587044, acc: 75%] [G loss: 1.907289]\n",
      "[Epoch 166/200] [Batch 80/340] [D loss: 1.644305, acc: 67%] [G loss: 2.051466]\n",
      "[Epoch 166/200] [Batch 100/340] [D loss: 1.614941, acc: 67%] [G loss: 2.172867]\n",
      "[Epoch 166/200] [Batch 120/340] [D loss: 1.641836, acc: 72%] [G loss: 1.943433]\n",
      "[Epoch 166/200] [Batch 140/340] [D loss: 1.642677, acc: 72%] [G loss: 2.032797]\n",
      "[Epoch 166/200] [Batch 160/340] [D loss: 1.651526, acc: 69%] [G loss: 2.014776]\n",
      "[Epoch 166/200] [Batch 180/340] [D loss: 1.686723, acc: 71%] [G loss: 1.849759]\n",
      "[Epoch 166/200] [Batch 200/340] [D loss: 1.737306, acc: 70%] [G loss: 1.983733]\n",
      "[Epoch 166/200] [Batch 220/340] [D loss: 1.647818, acc: 72%] [G loss: 1.985120]\n",
      "[Epoch 166/200] [Batch 240/340] [D loss: 1.643607, acc: 68%] [G loss: 1.741986]\n",
      "[Epoch 166/200] [Batch 260/340] [D loss: 1.678689, acc: 72%] [G loss: 1.738314]\n",
      "[Epoch 166/200] [Batch 280/340] [D loss: 1.649642, acc: 71%] [G loss: 1.733979]\n",
      "[Epoch 166/200] [Batch 300/340] [D loss: 1.611207, acc: 72%] [G loss: 1.732288]\n",
      "[Epoch 166/200] [Batch 320/340] [D loss: 1.687386, acc: 69%] [G loss: 1.636409]\n",
      "[Epoch 167/200] [Batch 0/340] [D loss: 1.677893, acc: 72%] [G loss: 1.973417]\n",
      "[Epoch 167/200] [Batch 20/340] [D loss: 1.633528, acc: 70%] [G loss: 1.915654]\n",
      "[Epoch 167/200] [Batch 40/340] [D loss: 1.668746, acc: 68%] [G loss: 2.021746]\n",
      "[Epoch 167/200] [Batch 60/340] [D loss: 1.602064, acc: 70%] [G loss: 1.894930]\n",
      "[Epoch 167/200] [Batch 80/340] [D loss: 1.627932, acc: 70%] [G loss: 1.958546]\n",
      "[Epoch 167/200] [Batch 100/340] [D loss: 1.638209, acc: 70%] [G loss: 1.757554]\n",
      "[Epoch 167/200] [Batch 120/340] [D loss: 1.675287, acc: 71%] [G loss: 2.028260]\n",
      "[Epoch 167/200] [Batch 140/340] [D loss: 1.637639, acc: 74%] [G loss: 2.031203]\n",
      "[Epoch 167/200] [Batch 160/340] [D loss: 1.581111, acc: 73%] [G loss: 1.765456]\n",
      "[Epoch 167/200] [Batch 180/340] [D loss: 1.610922, acc: 72%] [G loss: 2.189589]\n",
      "[Epoch 167/200] [Batch 200/340] [D loss: 1.587589, acc: 72%] [G loss: 2.098638]\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(train_data):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity, pred_label = discriminator(gen_imgs)\n",
    "        g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_pred, real_aux = discriminator(real_imgs)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        # Calculate discriminator accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        if i%20 == 0:\n",
    "            print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %d%%] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(train_data), d_loss.item(), 100 * d_acc, g_loss.item())\n",
    "        )\n",
    "        batches_done = epoch * len(train_data) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_image(n_row=10, batches_done=batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
