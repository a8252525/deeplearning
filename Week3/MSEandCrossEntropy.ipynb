{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在下載\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "from urllib.request import urlretrieve \n",
    "print('正在下載')\n",
    "dataset = 'mnist.pkl.gz'\n",
    "if not os.path.isfile(dataset):\n",
    "        origin = \"https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\"\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urlretrieve(origin, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Sep 25 12:45:46 2019\n",
    "\n",
    "@author: ideapad 320\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import math\n",
    "import gzip #用來處理壓縮檔\n",
    "import pickle #\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "def Drelu(x):\n",
    "    return (x>0).astype('float32')\n",
    "def softmax(x):\n",
    "    t=np.exp(x)\n",
    "    return t/t.sum()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "dataset = 'mnist.pkl.gz'\n",
    "with gzip.open(dataset, 'rb') as f:#gzip.open來打開壓縮檔\n",
    "    train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
    "#run -i q_see_mnist_data.py 這行是另外的.py，跑在console，可以看到裡面的[0]是圖片用浮點數形式，[1]是他的答案\n",
    "# 確認完資料\n",
    "train_X, train_y = train_set\n",
    "test_X, test_y = test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with loss function:cross entropy...\n",
      "Accuracy:0.4873, loss:5.195881124350194e-05\n",
      "Accuracy:0.8379, loss:-0.0\n",
      "Accuracy:0.8621, loss:1.6802673171889604e-08\n",
      "Accuracy:0.8912, loss:1.743050148661511e-14\n",
      "Accuracy:0.8901, loss:-0.0\n",
      "Accuracy:0.8954, loss:-0.0\n",
      "Accuracy:0.9004, loss:-0.0\n",
      "Accuracy:0.9006, loss:1.0431762508266076e-08\n",
      "Accuracy:0.8995, loss:2.382130392815029e-06\n",
      "Accuracy:0.8985, loss:1.3322676295501888e-15\n",
      "Accuracy:0.899, loss:-0.0\n",
      "Accuracy:0.8986, loss:1.0318198523145713e-09\n",
      "Accuracy:0.8986, loss:0.1834361546842938\n",
      "Accuracy:0.8982, loss:0.0021299737017661676\n",
      "Accuracy:0.8984, loss:7.593925488436359e-14\n",
      "Accuracy:0.8975, loss:-0.0\n",
      "Accuracy:0.8998, loss:-0.0\n",
      "Accuracy:0.8992, loss:-0.0\n",
      "Accuracy:0.8987, loss:9.703349235224339e-14\n",
      "Accuracy:0.8987, loss:2.113694647318292e-07\n",
      "Accuracy:0.8986, loss:-0.0\n",
      "Accuracy:0.8973, loss:-0.0\n",
      "Accuracy:0.8978, loss:-0.0\n",
      "Accuracy:0.8981, loss:0.18328788553528502\n",
      "Accuracy:0.8978, loss:3.2730234394498776e-07\n",
      "Accuracy:0.8975, loss:-0.0\n",
      "Accuracy:0.8973, loss:-0.0\n",
      "Accuracy:0.897, loss:0.18135897608911317\n",
      "Accuracy:0.897, loss:-0.0\n",
      "Accuracy:0.8971, loss:8.522777409051415e-09\n",
      "Accuracy:0.8974, loss:4.107825191113088e-15\n",
      "Accuracy:0.8973, loss:5.884182030513347e-15\n",
      "Accuracy:0.8971, loss:-0.0\n",
      "Accuracy:0.8972, loss:9.65246875717202e-08\n",
      "Accuracy:0.8972, loss:0.178438910409274\n",
      "Accuracy:0.8972, loss:0.0003678305746691549\n",
      "Accuracy:0.897, loss:5.0471481491102966e-08\n",
      "Accuracy:0.897, loss:-0.0\n",
      "Accuracy:0.8969, loss:7.164889795143683e-10\n",
      "Accuracy:0.8968, loss:7.131630311746768e-09\n",
      "Accuracy:0.8967, loss:0.17820091077904646\n",
      "Accuracy:0.8967, loss:0.1782372872759074\n",
      "Accuracy:0.8967, loss:1.1313812882631078e-07\n",
      "Accuracy:0.8968, loss:0.00027190240223775946\n",
      "Accuracy:0.8965, loss:0.1779391801824978\n",
      "Accuracy:0.8965, loss:2.4769386572510263e-09\n",
      "Accuracy:0.8965, loss:-0.0\n",
      "Accuracy:0.8965, loss:-0.0\n",
      "Accuracy:0.8965, loss:-0.0\n",
      "Accuracy:0.8965, loss:2.2204460492503136e-16\n"
     ]
    }
   ],
   "source": [
    "print('Training with loss function:cross entropy...')\n",
    "\n",
    "test_X=test_X[...,None]\n",
    "\n",
    "W1=np.random.normal(size=(50,784))\n",
    "b1=np.random.normal(size=(50,1))\n",
    "W2=np.random.normal(size=(10,50))\n",
    "b2=np.random.normal(size=(10,1))\n",
    "LR=0.08\n",
    "L_history=[]\n",
    "for i in range(500):\n",
    "    idx=np.random.choice(5000,5000,replace=False)\n",
    "    for j in idx:\n",
    "        x=train_X[j]\n",
    "        y=train_y[j]\n",
    "        x=x.reshape(784,1)\n",
    "        U=relu(W1@x+b1)\n",
    "        V=softmax(W2@U+b2)\n",
    "        L=-np.log(V[y])[0]\n",
    "        L_history.append(L)\n",
    "        p=np.eye(10)[y][:,None]\n",
    "        grad_b2=V-p\n",
    "        grad_W2=grad_b2@U.T\n",
    "        #(10,50)=(10,1)@(1,50)\n",
    "        grad_b1=(W2.T@grad_b2)*Drelu(W1@x+b1)\n",
    "        grad_W1=grad_b1@x.T\n",
    "        W1-=LR*grad_W1\n",
    "        b1-=LR*grad_b1\n",
    "        W2-=LR*grad_W2\n",
    "        b2-=LR*grad_b2\n",
    "    if i%10==0:\n",
    "        LR=LR*0.9\n",
    "        accuracy=((W2@relu(W1@test_X+b1)+b2).argmax(axis=1).ravel()==test_y).mean()\n",
    "        print('Accuracy:{}, loss:{}'.format(accuracy,L))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with loss function: MSE\n",
      "Accuarcy:0.1204,loss:0.19999999566038332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6da5eecb6df2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mV\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training with loss function: MSE')\n",
    "W1=np.random.normal(size=(50,784))\n",
    "b1=np.random.normal(size=(50,1))\n",
    "W2=np.random.normal(size=(10,50))\n",
    "b2=np.random.normal(size=(10,1))\n",
    "LR=0.1\n",
    "L_history=[]\n",
    "\n",
    "for i in range(500):\n",
    "    idx=np.random.choice(5000,5000,replace=False)\n",
    "    for j in idx:\n",
    "        x=train_X[j]\n",
    "        y=train_y[j]\n",
    "        x=x.reshape(784,1)\n",
    "        U=relu(W1@x+b1)\n",
    "        V=softmax(W2@U+b2)\n",
    "        p=np.eye(10)[y][:,None]\n",
    "        L=((V-p)*(V-p)).sum()/len(V)\n",
    "        L_history.append(L)\n",
    "        grad_loss=((V-p)*2/len(V)).mean()\n",
    "        grad_softmax=grad_loss*((V.T@p)*(p-V))\n",
    "        grad_b2=grad_softmax\n",
    "        grad_W2=grad_b2@U.T\n",
    "        grad_b1=(W2.T@grad_b2)*Drelu(W1@x+b1)\n",
    "        grad_W1=grad_b1@x.T\n",
    "        W1-=LR*grad_W1\n",
    "        b1-=LR*grad_b1\n",
    "        W2-=LR*grad_W2\n",
    "        b2-=LR*grad_b2\n",
    "    \n",
    "    if i%10==0:\n",
    "        LR*=0.9\n",
    "        accuracy=((W2@relu(W1@test_X+b1)+b2).argmax(axis=1).ravel()==test_y).mean()\n",
    "        print('Accuarcy:{},loss:{}'.format(accuracy,L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
